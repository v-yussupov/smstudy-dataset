% Encoding: UTF-8
@InProceedings{Abad:2018:PSF:3185768.3186294,
  author    = {Abad, Cristina L. and Boza, Edwin F. and van Eyk, Erwin},
  title     = {Package-Aware Scheduling of FaaS Functions},
  booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
  year      = {2018},
  series    = {ICPE '18},
  pages     = {101--106},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3186294},
  doi       = {10.1145/3185768.3186294},
  file      = {:files/Abad2018.pdf:PDF},
  isbn      = {978-1-4503-5629-9},
  keywords  = {cloud computing, functions-as-a-service, load balancing, scheduling, serverless computing},
  location  = {Berlin, Germany},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3185768.3186294},
}

@InProceedings{Akkus:2018:STH:3277355.3277444,
  author    = {Akkus, Istemi Ekin and Chen, Ruichuan and Rimac, Ivica and Stein, Manuel and Satzke, Klaus and Beck, Andre and Aditya, Paarijaat and Hilt, Volker},
  title     = {SAND: Towards High-performance Serverless Computing},
  booktitle = {Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference},
  year      = {2018},
  series    = {USENIX ATC '18},
  pages     = {923--935},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3277444},
  file      = {:files/Akkus2018.pdf:PDF},
  isbn      = {978-1-931971-44-7},
  location  = {Boston, MA, USA},
  numpages  = {13},
  url       = {http://dl.acm.org/citation.cfm?id=3277355.3277444},
}

@InProceedings{8457832,
  author    = {Z. {Al-Ali} and S. {Goodarzy} and E. {Hunter} and S. {Ha} and R. {Han} and E. {Keller} and E. {Rozner}},
  title     = {Making Serverless Computing More Serverless},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {456-459},
  month     = {July},
  abstract  = {In serverless computing, developers define a function to handle an event, and the serverless framework horizontally scales the application as needed. The downside of this function-based abstraction is it limits the type of application supported and places a bound on the function to be within the physical resource limitations of the server the function executes on. In this paper we propose a new abstraction for serverless computing: a developer supplies a process and the serverless framework seamlessly scales out the process's resource usage across the datacenter. This abstraction enables processing to not only be more general purpose, but also allows a process to break out of the limitations of a single server - making serverless computing more serverless. To realize this abstraction, we propose ServerlessOS, comprised of three key components: (i) a new disaggregation model, which leverages disaggregation for abstraction, but enables resources to move fluidly between servers for performance; (ii) a cloud orchestration layer which manages fine-grained resource allocation and placement throughout the application's lifetime via local and global decision making; and (iii) an isolation capability that enforces data and resource isolation across disaggregation, effectively extending Linux cgroup functionality to span servers.},
  doi       = {10.1109/CLOUD.2018.00064},
  file      = {:files/8457832.pdf:PDF},
  issn      = {2159-6190},
  keywords  = {cloud computing;computer centres;decision making;Linux;resource allocation;serverless computing;function-based abstraction;physical resource limitations;resource usage;datacenter;disaggregation model;ServerlessOS;cloud orchestration layer;fine-grained resource allocation;global decision making;local decision making;isolation capability;disaggregation;Linux cgroup functionality;Servers;Instruction sets;Cloud computing;Sockets;Couplings;Micromechanical devices;Memory management;serverless;cloud;virtualization;isolation;orchestration;resource disaggregation},
}

@Article{2018arXiv181006080A,
  author        = {{Alder}, Fritz and {Asokan}, N. and {Kurnikov}, Arseny and {Paverd}, Andrew and {Steiner}, Michael},
  title         = {{S-FaaS: Trustworthy and Accountable Function-as-a-Service using Intel SGX}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1810.06080},
  month         = {Oct},
  abstract      = {Function-as-a-Service (FaaS) is a recent and already very popular paradigm in cloud computing. The function provider need only specify the function to be run, usually in a high-level language like JavaScript, and the service provider orchestrates all the necessary infrastructure and software stacks. The function provider is only billed for the actual computational resources used by the function invocation. Compared to previous cloud paradigms, FaaS requires significantly more fine-grained resource measurement mechanisms, e.g. to measure compute time and memory usage of a single function invocation with sub-second accuracy. Thanks to the short duration and stateless nature of functions, and the availability of multiple open-source frameworks, FaaS enables non-traditional service providers e.g. individuals or data centers with spare capacity. However, this exacerbates the challenge of ensuring that resource consumption is measured accurately and reported reliably. It also raises the issues of ensuring computation is done correctly and minimizing the amount of information leaked to service providers. To address these challenges, we introduce S-FaaS, the first architecture and implementation of FaaS to provide strong security and accountability guarantees backed by Intel SGX. To match the dynamic event-driven nature of FaaS, our design introduces a new key distribution enclave and a novel transitive attestation protocol. A core contribution of S-FaaS is our set of resource measurement mechanisms that securely measure compute time inside an enclave, and actual memory allocations. We have integrated S-FaaS into the popular OpenWhisk FaaS framework. We evaluate the security of our architecture, the accuracy of our resource measurement mechanisms, and the performance of our implementation, showing that our resource measurement mechanisms add less than 6.3% latency on standardized benchmarks. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181006080A},
  archiveprefix = {arXiv},
  eid           = {arXiv:1810.06080},
  eprint        = {1810.06080},
  file          = {:files/2018arXiv181006080A.pdf:PDF},
  keywords      = {Computer Science - Cryptography and Security},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/pdf/1810.06080.pdf},
}

@Article{Alpernas:2018:SSC:3288538.3276488,
  author     = {Alpernas, Kalev and Flanagan, Cormac and Fouladi, Sadjad and Ryzhyk, Leonid and Sagiv, Mooly and Schmitz, Thomas and Winstein, Keith},
  title      = {Secure Serverless Computing Using Dynamic Information Flow Control},
  journal    = {Proc. ACM Program. Lang.},
  year       = {2018},
  volume     = {2},
  number     = {OOPSLA},
  pages      = {118:1--118:26},
  month      = oct,
  issn       = {2475-1421},
  acmid      = {3276488},
  address    = {New York, NY, USA},
  articleno  = {118},
  doi        = {10.1145/3276488},
  file       = {:files/Alpernas2018.pdf:PDF},
  issue_date = {November 2018},
  keywords   = {Cloud Computing, Information Flow Control, Serverless},
  numpages   = {26},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3276488},
}

@InProceedings{Boucher:2018:PMB:3277355.3277417,
  author    = {Boucher, Sol and Kalia, Anuj and Andersen, David G. and Kaminsky, Michael},
  title     = {Putting the "Micro" Back in Microservice},
  booktitle = {Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference},
  year      = {2018},
  series    = {USENIX ATC '18},
  pages     = {645--650},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3277417},
  file      = {:files/Boucher2018.pdf:PDF},
  isbn      = {978-1-931971-44-7},
  location  = {Boston, MA, USA},
  numpages  = {6},
  url       = {http://dl.acm.org/citation.cfm?id=3277355.3277417},
}

@InProceedings{Brenner:2019:TMS:3319647.3325825,
  author    = {Brenner, Stefan and Kapitza, R\"{u}diger},
  title     = {Trust More, Serverless},
  booktitle = {Proceedings of the 12th ACM International Conference on Systems and Storage},
  year      = {2019},
  series    = {SYSTOR '19},
  pages     = {33--43},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3325825},
  doi       = {10.1145/3319647.3325825},
  file      = {:files/Brenner2019TMS3319647.3325825.pdf:PDF},
  isbn      = {978-1-4503-6749-3},
  keywords  = {intel SGX, serverless cloud, trusted function-as-a-service},
  location  = {Haifa, Israel},
  numpages  = {11},
  url       = {http://doi.acm.org/10.1145/3319647.3325825},
}

@Article{2019arXiv190100302D,
  author        = {{Danayi}, Abolfazl and {Sharifian}, Saeed},
  title         = {{openCoT: The opensource Cloud of Things platform}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.00302},
  month         = {Jan},
  abstract      = {In order to address the complexity and extensiveness of technology, Cloud Computing is utilized with four main service models. The most recent service model, function-as-a-service, enables developers to develop their application in a function-based structure and then deploy it to the Cloud. Using an optimum elastic auto-scaling, the performance of executing an application over FaaS Cloud, overcomes the extra overhead and reduces the total cost. However, researchers need a simple and well-documented FaaS Cloud manager in order to implement their proposed Auto-scaling algorithms. In this paper, we represent the openCoT platform and explain its building blocks and details. Experimental results show that executing a function (invoking and passing arguments) and returning the result using openCoT takes 21 ms over a remote connection. The source code of openCoT is available in the GitHub repository of the project (\code{www.github.com/adanayi/opencot}) for public usage. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190100302D},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.00302},
  eprint        = {1901.00302},
  file          = {:files/2019arXiv190100302D.pdf:PDF},
  keywords      = {Computer Science - Networking and Internet Architecture, Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.NI},
  url           = {https://arxiv.org/pdf/1901.00302.pdf},
}

@InProceedings{8700543,
  author    = {A. {Danayi} and S. {Sharifian}},
  title     = {PESS-MinA: A Proactive Stochastic Task Allocation Algorithm for FaaS Edge-Cloud environments},
  booktitle = {2018 4th Iranian Conference on Signal Processing and Intelligent Systems (ICSPIS)},
  year      = {2018},
  pages     = {27-31},
  month     = {Dec},
  abstract  = {By the advent of FaaS Cloud services and the micro-services programming architecture, designing task allocation algorithms with higher performance has become a crucial task. Motivated by this high-interested challenge, we propose a new allocation algorithm called PESS-MinA based on our novel modular model for FaaS Edge-Cloud environments. In contradiction to widely-used Max-Min and Min-Min algorithms which are both reactive and deterministic, this algorithm is based on stochastic score, and thus provides proactivity considerations. Experiments with Google Cloud Trace dataset show that our algorithm exhibits better performance in both resource load balancing and QoS assurance of FaaS. According to simulations, PESS-MinA decreased the dropped tasks percentage from 2.9% to 0.01%, alongside with a triple balancing score.},
  doi       = {10.1109/ICSPIS.2018.8700543},
  file      = {:files/8700543.pdf:PDF},
  keywords  = {cloud computing;minimax techniques;quality of service;resource allocation;stochastic processes;PESS-MinA;FaaS Cloud services;max-min algorithms;min-min algorithms;FaaS edge-cloud environments;stochastic task allocation algorithm;microservices programming architecture;Google Cloud Trace dataset;load balancing;QoS assurance;Task analysis;Signal processing algorithms;Cloud computing;FAA;Resource management;Stochastic processes;Containers;Proactive;Task Allocation;FaaS;Edge-Cloud},
}

@InProceedings{Hall:2019:EMS:3302505.3310084,
  author    = {Hall, Adam and Ramachandran, Umakishore},
  title     = {An Execution Model for Serverless Functions at the Edge},
  booktitle = {Proceedings of the International Conference on Internet of Things Design and Implementation},
  year      = {2019},
  series    = {IoTDI '19},
  pages     = {225--236},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3310084},
  doi       = {10.1145/3302505.3310084},
  file      = {:files/Hall2019.pdf:PDF},
  isbn      = {978-1-4503-6283-2},
  keywords  = {FaaS, edge computing, fog computing, function-as-a-service, serverless, webassembly},
  location  = {Montreal, Quebec, Canada},
  numpages  = {12},
  url       = {http://doi.acm.org/10.1145/3302505.3310084},
}

@InProceedings{Hendrickson:2016:SCO:3027041.3027047,
  author    = {Hendrickson, Scott and Sturdevant, Stephen and Harter, Tyler and Venkataramani, Venkateshwaran and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
  title     = {Serverless Computation with openLambda},
  booktitle = {Proceedings of the 8th USENIX Conference on Hot Topics in Cloud Computing},
  year      = {2016},
  series    = {HotCloud'16},
  pages     = {33--39},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3027047},
  file      = {:files/Hendrickson2016.pdf:PDF},
  location  = {Denver, CO},
  numpages  = {7},
  url       = {http://dl.acm.org/citation.cfm?id=3027041.3027047},
}

@InProceedings{10.1007/978-3-319-69035-3_17,
  author    = {HoseinyFarahabady, MohammadReza and Lee, Young Choon and Zomaya, Albert Y. and Tari, Zahir},
  title     = {A QoS-Aware Resource Allocation Controller for Function as a Service (FaaS) Platform},
  booktitle = {Service-Oriented Computing},
  year      = {2017},
  editor    = {Maximilien, Michael and Vallecillo, Antonio and Wang, Jianmin and Oriol, Marc},
  pages     = {241--255},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {Function as a Service (FaaS) is a recent event-driven serverless paradigm that allows enterprises to build their applications in a fault tolerant distributed manner. Having been considered as an attractive replacement of traditional Service Oriented Architecture (SOA), the FaaS platform leverages the management of massive data sets or the handling of event streams. However, the realization of such leverage is largely dependent on the effective exploitation of FaaS elasticity/scalability.},
  file      = {:files/10.1007978-3-319-69035-3_17.pdf:PDF},
  isbn      = {978-3-319-69035-3},
}

@InProceedings{8025307,
  author    = {M. {HoseinyFarahabady} and J. {Taheri} and Z. {Tari} and A. Y. {Zomaya}},
  title     = {A Dynamic Resource Controller for a Lambda Architecture},
  booktitle = {2017 46th International Conference on Parallel Processing (ICPP)},
  year      = {2017},
  pages     = {332-341},
  month     = {Aug},
  abstract  = {Lambda architecture is a novel event-driven serverless paradigm that allows companies to build scalable and reliable enterprise applications. As an attractive alternative to traditional service oriented architecture (SOA), Lambda architecture can be used in many use cases including BI tools, in-memory graph databases, OLAP, and streaming data processing. In practice, an important aim of Lambda's service providers is devising an efficient way to co-locate multiple Lambda functions with different attributes into a set of available computing resources. However, previous studies showed that consolidated workloads can compete fiercely for shared resources, resulting in severe performance variability/degradation. This paper proposes a resource allocation mechanism for a Lambda platform based on the model predictive control framework. Performance evaluation is carried out by comparing the proposed solution with multiple resource allocation heuristics, namely enhanced versions of spread and binpack, and best-effort approaches. Results confirm that the proposed controller increases the overall resource utilization by 37% on average and achieves a significant improvement in preventing QoS violation incidents compared to others.},
  doi       = {10.1109/ICPP.2017.42},
  file      = {:files/8025307.pdf:PDF},
  issn      = {2332-5690},
  keywords  = {bin packing;business data processing;data handling;performance evaluation;predictive control;resource allocation;software architecture;performance variability;QoS violation prevention;resource utilization;spread approach;binpack approach;best-effort approach;performance evaluation;model predictive control;resource allocation;performance degradation;shared resources;computing resources;Lambda functions;Lambda service providers;enterprise applications;event-driven serverless paradigm;Lambda architecture;dynamic resource controller;Quality of service;Resource management;Sensitivity;Memory management;Interference;Servers;Dynamic Resource Allocation;Performance degradation;Lambda Platform Processing;Shared Resource Interference},
}

@Article{8126823,
  author   = {M. R. {HoseinyFarahabady} and A. Y. {Zomaya} and Z. {Tari}},
  title    = {A Model Predictive Controller for Managing QoS Enforcements and Microarchitecture-Level Interferences in a Lambda Platform},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  year     = {2018},
  volume   = {29},
  number   = {7},
  pages    = {1442-1455},
  month    = {July},
  issn     = {1045-9219},
  abstract = {Lambda paradigm, also known as Function as a Service (FaaS), is a novel event-driven concept that allows companies to build scalable and reliable enterprise applications in an off-premise computing data-center as a serverless solution. In practice, however, an important goal for the service provider of a Lambda platform is to devise an efficient way to consolidate multiple Lambda functions in a single host. While the majority of existing resource management solutions use only operating-system level metrics (e.g., average utilization of computing and I/O resources) to allocate the available resources among the submitted workloads in a balanced way, a resource allocation schema that is oblivious to the issue of shared-resource contention can result in a significant performance variability and degradation within the entire platform. This paper proposes a predictive controller scheme that dynamically allocates resources in a Lambda platform. This scheme uses a prediction tool to estimate the future rate of every event stream and takes into account the quality of service enforcements requested by the owner of each Lambda function. This is formulated as an optimization problem where a set of cost functions are introduced (i) to reduce the total QoS violation incidents; (ii) to keep the CPU utilization level within an accepted range; and (iii) to avoid the fierce contention among collocated applications for obtaining shared resources. Performance evaluation is carried out by comparing the proposed solution with an enhanced interference-aware version of three well-known heuristics, namely spread, binpack (the two native clustering solutions employed by Docker Swarm) and best-effort resource allocation schema. Experimental results show that the proposed controller improves the overall performance (in terms of reducing the end-to-end response time) by 14.9 percent on average compared to the best result of the other heuristics. The proposed solution also increases the overall CPU utilization by 18 percent on average (for lightweight workloads), while achieves an average 87 percent (maximum 146 percent) improvement in preventing QoS violation incidents.},
  doi      = {10.1109/TPDS.2017.2779502},
  file     = {:files/8126823.pdf:PDF},
  keywords = {application program interfaces;cloud computing;computer centres;optimal control;optimisation;predictive control;quality of service;resource allocation;software engineering;virtualisation;model predictive controller;QoS enforcements;microarchitecture-level interferences;Lambda platform;Lambda paradigm;reliable enterprise applications;serverless solution;multiple Lambda functions;resource management solutions;operating-system level metrics;resource allocation schema;shared-resource contention;function as a service;quality of service;total QoS violation incident reduction;CPU utilization;enhanced interference-awareness;Docker Swarm;Resource management;Quality of service;Interference;Bandwidth;Measurement;Servers;Cloud computing;Serverless lambda platform;function as a service (FaaS);model predictive control;dynamic resource allocation/scheduling;performance degradation},
}

@InProceedings{Karhula:2019:CMI:3301418.3313947,
  author    = {Karhula, Pekka and Janak, Jan and Schulzrinne, Henning},
  title     = {Checkpointing and Migration of IoT Edge Functions},
  booktitle = {Proceedings of the 2Nd International Workshop on Edge Systems, Analytics and Networking},
  year      = {2019},
  series    = {EdgeSys '19},
  pages     = {60--65},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3313947},
  doi       = {10.1145/3301418.3313947},
  file      = {:files/Karhula2019.pdf:PDF},
  isbn      = {978-1-4503-6275-7},
  keywords  = {Internet of Things, checkpointing, function as a service, light-weight virtualization, serverless},
  location  = {Dresden, Germany},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3301418.3313947},
}

@Article{2019arXiv190109842K,
  author        = {{Kesidis}, George},
  title         = {{Temporal Overbooking of Lambda Functions in the Cloud}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.09842},
  month         = {Jan},
  abstract      = {We consider the problem of scheduling "serverless computing" instances such as Amazon Lambda functions. Instead of a quota per tenant/customer, we assume demand for Lambda functions is modulated by token-bucket mechanisms per tenant. Based on an upper bound on the stationary number of active "Lambda servers" considering the execution-time distribution of Lambda functions, we describe an approach that the cloud could use to overbook Lambda functions for improved utilization of IT resources. An earlier bound for a single service tier is extended to the case of multiple service tiers.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190109842K},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.09842},
  eprint        = {1901.09842},
  file          = {:files/2019arXiv190109842K.pdf:PDF},
  keywords      = {Computer Science - Performance},
  primaryclass  = {cs.PF},
  url           = {https://arxiv.org/pdf/1901.09842.pdf},
}

@InProceedings{8567385,
  author    = {Y. {Kim} and G. {Cha}},
  title     = {Design of the Cost Effective Execution Worker Scheduling Algorithm for FaaS Platform Using Two-Step Allocation and Dynamic Scaling},
  booktitle = {2018 IEEE 8th International Symposium on Cloud and Service Computing (SC2)},
  year      = {2018},
  pages     = {131-134},
  month     = {Nov},
  abstract  = {Function as a Service(FaaS) has been widely prevalent in the cloud computing area with the evolution of the cloud computing paradigm and the growing demand for event-based computing models. We have analyzed the preparation load required for the actual execution of a function, from assignment of a function execution walker to loading a function on the FaaS platform, by testing the execution of a dummy function on a simple FaaS prototype. According to the analysis results, we found that the cost of first worker allocation requires 1,850ms even though the lightweight container is used, and then the worker re-allocation cost require 470ms at the same node. The result shows that the function service is not enough to be used as a high efficiency processing calculation platform. We propose a new worker scheduling algorithm to appropriately distribute the worker's preparation load related to execution of functions so that FaaS platform is suitable for high efficiency computing environment. Proposed algorithm is to distribute the worker 's allocation tasks in two steps before the request occurs, and predict the number of workers required to be allocated in advance. When applying the proposed worker scheduling algorithm in FaaS platform under development, we estimate that worker allocation request can be processed with an allocation cost of less than 3% compared to the FaaS prototype. Therefore, it is expected that the functional service will become a high efficiency computing platform through the significant improvement of the worker allocation cost.},
  doi       = {10.1109/SC2.2018.00027},
  file      = {:files/8567385.pdf:PDF},
  keywords  = {cloud computing;resource allocation;scheduling;cost effective execution worker scheduling algorithm;FaaS platform;two-step allocation;cloud computing area;event-based computing models;worker re-allocation cost;high efficiency processing calculation platform;high efficiency computing environment;worker allocation request;high efficiency computing platform;worker allocation cost;Function as a Service;dynamic scaling;Resource management;FAA;Cloud computing;Containers;Dynamic scheduling;Scheduling algorithms;Scheduling, Dynamic scaling, Function, Function as a Service},
}

@InProceedings{8374513,
  author    = {J. {Kim} and T. J. {Jun} and D. {Kang} and D. {Kim} and D. {Kim}},
  title     = {GPU Enabled Serverless Computing Framework},
  booktitle = {2018 26th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)},
  year      = {2018},
  pages     = {533-540},
  month     = {March},
  abstract  = {A new form of cloud computing, serverless computing, is drawing attention as a new way to design micro-services architectures. In a serverless computing environment, services are developed as service functional units. The function development environment of all serverless computing framework at present is CPU based. In this paper, we propose a GPU-supported serverless computing framework that can deploy services faster than existing serverless computing framework using CPU. Our core approach is to integrate the open source serverless computing framework with NVIDIA-Docker and deploy services based on the GPU support container. We have developed an API that connects the open source framework to the NVIDIA-Docker and commands that enable GPU programming. In our experiments, we measured the performance of the framework in various environments. As a result, developers who want to develop services through the framework can deploy high-performance micro services and developers who want to run deep learning programs without a GPU environment can run code on remote GPUs with little performance degradation.},
  doi       = {10.1109/PDP2018.2018.00090},
  file      = {:files/8374513.pdf:PDF},
  issn      = {2377-5750},
  keywords  = {application program interfaces;cloud computing;graphics processing units;learning (artificial intelligence);public domain software;service-oriented architecture;GPU enabled serverless computing framework;cloud computing;microservices architectures;serverless computing environment;service functional units;function development environment;open source serverless computing framework;deploy services;open source framework;high-performance microservices;NVIDIA-Docker;GPU support container;GPU programming;deep learning programs;performance degradation;Graphics processing units;Servers;Cloud computing;Containers;Computer architecture;Libraries;Standards;Cloud Computing;Serverless Computing;Serverless Architecture;FaaS;GPGPU},
}

@InProceedings{8514884,
  author    = {Y. K. {Kim} and M. R. {HoseinyFarahabady} and Y. C. {Lee} and A. Y. {Zomaya} and R. {Jurdak}},
  title     = {Dynamic Control of CPU Usage in a Lambda Platform},
  booktitle = {2018 IEEE International Conference on Cluster Computing (CLUSTER)},
  year      = {2018},
  pages     = {234-244},
  month     = {Sep.},
  abstract  = {Lambda platform is a new concept based on an event-driven server-less computation that empowers application developers to build scalable enterprise software in a virtualized environment without provisioning or managing any physical servers (a server-less solution). In reality, however, devising an effective consolidation method to host multiple Lambda functions into a single machine is challenging. The existing simple resource allocation algorithms, such as the round-robin policy used in many commercial server-less systems, suffer from lack of responsiveness to a sudden surge in the incoming workload. This will result in an unsatisfactory performance degradation that is directly experienced by the end-user of a Lambda application. In this paper, we address the problem of CPU cap management in a Lambda platform for ensuring different QoS enforcement levels in a platform with shared resources, in case of fluctuations and sudden surges in the incoming workload requests. To this end, we present a closed-loop (feedback-based) CPU cap controller, which fulfills the QoS levels enforced by the application owners. The controller adjusts the number of working threads per QoS class and dispatches the outstanding Lambda functions along with the associated events to the most appropriate working thread. The proposed solution reduces the QoS violations by an average of 6.36 times compared to the round-robin policy. It can also maintain the end-to-end response time of applications belonging to the highest priority QoS class close to the target set-point while decreasing the overall response time by up to 52%.},
  doi       = {10.1109/CLUSTER.2018.00041},
  file      = {:files/8514884.pdf:PDF},
  issn      = {2168-9253},
  keywords  = {closed loop systems;microprocessor chips;quality of service;resource allocation;software engineering;virtualisation;simple resource allocation algorithms;QoS enforcement levels;highest priority QoS class close;end-to-end response time;outstanding Lambda functions;closed-loop CPU cap controller;CPU cap management;Lambda application;round-robin policy;multiple Lambda functions;scalable enterprise software;event-driven server-less computation;Lambda platform;CPU usage;dynamic control;Quality of service;Instruction sets;Delays;Time factors;Surges;Resource management;Server-less computing, FaaS (Function as a Service), QoS-aware resource manager, Dynamic CPU usage control},
}

@InProceedings{Koller:2017:SED:3102980.3103008,
  author    = {Koller, Ricardo and Williams, Dan},
  title     = {Will Serverless End the Dominance of Linux in the Cloud?},
  booktitle = {Proceedings of the 16th Workshop on Hot Topics in Operating Systems},
  year      = {2017},
  series    = {HotOS '17},
  pages     = {169--173},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3103008},
  doi       = {10.1145/3102980.3103008},
  file      = {:files/Koller2017.pdf:PDF},
  isbn      = {978-1-4503-5068-6},
  location  = {Whistler, BC, Canada},
  numpages  = {5},
  url       = {http://doi.acm.org/10.1145/3102980.3103008},
}

@Article{2019arXiv190312221L,
  author        = {{Lin}, Ping-Min and {Glikson}, Alex},
  title         = {{Mitigating Cold Starts in Serverless Platforms: A Pool-Based Approach}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1903.12221},
  month         = {Mar},
  abstract      = {Rapid adoption of the serverless (or Function-as-a-Service, FaaS) paradigm, pioneered by Amazon with AWS Lambda and followed by numerous commercial offerings and open source projects, introduces new challenges in designing the cloud infrastructure, balancing between performance and cost. While instant per-request elasticity that FaaS platforms typically offer application developers makes it possible to achieve high performance of bursty workloads without over-provisioning, such elasticity often involves extra latency associated with on-demand provisioning of individual runtime containers that serve the functions. This phenomenon is often called cold starts, as opposed to the situation when a function is served by a pre-provisioned "warm" container, ready to serve requests with close to zero overhead. Providers are constantly working on techniques aimed at reducing cold starts. A common approach to reduce cold starts is to maintain a pool of warm containers, in anticipation of future requests. In this report, we address the cold start problem in serverless architectures, specifically under the Knative Serving FaaS platform. We describe our implementation leveraging a pool of function instances, and evaluate the latency compared to the original implementation, resulting in a 85% reduction of P99 response time for a single instance pool. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190312221L},
  archiveprefix = {arXiv},
  eid           = {arXiv:1903.12221},
  eprint        = {1903.12221},
  file          = {:files/2019arXiv190312221L.pdf:PDF},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://ui.adsabs.harvard.edu/link_gateway/2019arXiv190312221L/EPRINT_PDF},
}

@InProceedings{7979855,
  author    = {G. {McGrath} and P. R. {Brenner}},
  title     = {Serverless Computing: Design, Implementation, and Performance},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems Workshops (ICDCSW)},
  year      = {2017},
  pages     = {405-410},
  month     = {June},
  abstract  = {We present the design of a novel performance-oriented serverless computing platform implemented in. NET, deployed in Microsoft Azure, and utilizing Windows containers as function execution environments. Implementation challenges such as function scaling and container discovery, lifecycle, and reuse are discussed in detail. We propose metrics to evaluate the execution performance of serverless platforms and conduct tests on our prototype as well as AWS Lambda, Azure Functions, Google Cloud Functions, and IBM's deployment of Apache OpenWhisk. Our measurements show the prototype achieving greater throughput than other platforms at most concurrency levels, and we examine the scaling and instance expiration trends in the implementations. Additionally, we discuss the gaps and limitations in our current design, propose possible solutions, and highlight future research.},
  doi       = {10.1109/ICDCSW.2017.36},
  file      = {:files/7979855.pdf:PDF},
  issn      = {2332-5666},
  keywords  = {cloud computing;concurrency (computers);Microsoft Windows (operating systems);performance evaluation;performance-oriented serverless computing;.NET;Microsoft Azure;Windows containers;function execution environments;execution performance evaluation;AWS Lambda;Azure Functions;Google Cloud Functions;IBM's deployment;Apache OpenWhisk;concurrency levels;Containers;Web services;Metadata;Resource management;Prototypes;Runtime;Google;serverless computing;serverless performance;FaaS;Function-as-a-Service;AWS Lambda;Azure Functions;Google Cloud Functions;Apache OpenWhisk;IBM OpenWhisk},
}

@InProceedings{MeiBner:2018:REP:3210284.3210285,
  author    = {Mei\ssner, Dominik and Erb, Benjamin and Kargl, Frank and Tichy, Matthias},
  title     = {Retro-{\$\Lambda\$}: An Event-sourced Platform for Serverless Applications with Retroactive Computing Support},
  booktitle = {Proceedings of the 12th ACM International Conference on Distributed and Event-based Systems},
  year      = {2018},
  series    = {DEBS '18},
  pages     = {76--87},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3210285},
  doi       = {10.1145/3210284.3210285},
  file      = {:files/MeiBner2018.pdf:PDF},
  isbn      = {978-1-4503-5782-1},
  keywords  = {event processing, event sourcing, event-driven architecture, retroaction, retroactive computing, serverless computing},
  location  = {Hamilton, New Zealand},
  numpages  = {12},
  url       = {http://doi.acm.org/10.1145/3210284.3210285},
}

@InProceedings{7979853,
  author    = {E. {Oakes} and L. {Yang} and K. {Houck} and T. {Harter} and A. C. {Arpaci-Dusseau} and R. H. {Arpaci-Dusseau}},
  title     = {Pipsqueak: Lean Lambdas with Large Libraries},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems Workshops (ICDCSW)},
  year      = {2017},
  pages     = {395-400},
  month     = {June},
  abstract  = {Microservices are usually fast to deploy because each microservice is small, and thus each can be installed and started quickly. Unfortunately, lean microservices that depend on large libraries will start slowly and harm elasticity. In this paper, we explore the challenges of lean microservices that rely on large libraries in the context of Python packages and the OpenLambda serverless computing platform. We analyze the package types and compressibility of libraries distributed via the Python Package Index and propose PipBench, a new tool for evaluating package support. We also propose Pipsqueak, a package-aware compute platform based on OpenLambda.},
  doi       = {10.1109/ICDCSW.2017.32},
  file      = {:files/7979853.pdf:PDF},
  issn      = {2332-5666},
  keywords  = {programming languages;software libraries;large libraries;Python packages;OpenLambda serverless computing platform;Python package index;cloud computing;Libraries;Containers;Virtual machining;Tools;Cloud computing;Memory management;Linux;serverless computing;cloud computing;distributed computing;distributed systems;python;distributed cache;software repository},
}

@InProceedings{Oakes:2018:SRT:3277355.3277362,
  author    = {Oakes, Edward and Yang, Leon and Zhou, Dennis and Houck, Kevin and Harter, Tyler and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
  title     = {SOCK: Rapid Task Provisioning with Serverless-optimized Containers},
  booktitle = {Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference},
  year      = {2018},
  series    = {USENIX ATC '18},
  pages     = {57--69},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3277362},
  file      = {:files/Oakes2018.pdf:PDF},
  isbn      = {978-1-931971-44-7},
  location  = {Boston, MA, USA},
  numpages  = {13},
  url       = {http://dl.acm.org/citation.cfm?id=3277355.3277362},
}

@InProceedings{10.1007/978-3-030-01701-9_25,
  author    = {Qiang, Weizhong and Dong, Zezhao and Jin, Hai},
  title     = {Se-Lambda: Securing Privacy-Sensitive Serverless Applications Using SGX Enclave},
  booktitle = {Security and Privacy in Communication Networks},
  year      = {2018},
  editor    = {Beyah, Raheem and Chang, Bing and Li, Yingjiu and Zhu, Sencun},
  pages     = {451--470},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {Serverless computing is an emerging trend in the cloud, which represents a new paradigm for deploying applications and services. In the serverless computing framework, cloud users can deploy arbitrary code and process data on the service runtime. However, as neither cloud users nor cloud providers are trustworthy, serverless computing platform suffers from trust issues caused by both sides. In this paper, we propose a new serverless computing framework called Se-Lambda, which protects the API gateway by using SGX enclave and the service runtime by leveraging a two-way sandbox that combines SGX enclave and WebAssembly sandboxed environment. In the proposed service runtime, users' untrusted code is confined by WebAssembly sandboxed environment, while SGX enclave prevents malicious cloud providers from stealing users' privacy-sensitive data. In addition, we implement a privilege monitoring mechanism in SGX enclave to manage the access control of function modules from users. We implement the prototype of Se-Lambda based on the open source project OpenLambda. The experimental results show that the Se-Lambda imposes a low performance penalty, while buying a significantly increased level of security.},
  file      = {:files/10.1007978-3-030-01701-9_25.pdf:PDF},
  isbn      = {978-3-030-01701-9},
}

@InProceedings{8457882,
  author    = {A. {Saha} and S. {Jindal}},
  title     = {EMARS: Efficient Management and Allocation of Resources in Serverless},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {827-830},
  month     = {July},
  abstract  = {We introduce EMARS, an efficient resource management system for serverless cloud computing frameworks with the goal to enhance resource (focus on memory) allocation among containers. We have built our prototype on top of an open-source serverless platform, OpenLambda. It is based upon application workloads and serverless functions' memory needs. As a background motivation we analyzed the latencies and memory requirements of functions running on AWS lambda. The memory limits also lead to variations in number of containers spawned on OpenLambda. We use memory limit settings to propose a model of predictive efficient memory management.},
  doi       = {10.1109/CLOUD.2018.00113},
  file      = {:files/8457882.pdf:PDF},
  issn      = {2159-6190},
  keywords  = {cloud computing;resource allocation;storage management;serverless cloud;open-source serverless platform;OpenLambda;serverless functions;memory limit settings;predictive efficient memory management;resource management system;EMARS system;Containers;Memory management;Resource management;Cloud computing;Servers;Predictive models;Time factors;Serverless;cloud computing;memory limit;response time},
}

@Article{SOLTANI2018121,
  author   = {Boubaker Soltani and Afifa Ghenai and Nadia Zeghib},
  title    = {Towards Distributed Containerized Serverless Architecture in Multi Cloud Environment},
  journal  = {Procedia Computer Science},
  year     = {2018},
  volume   = {134},
  pages    = {121 - 128},
  issn     = {1877-0509},
  note     = {The 15th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2018) / The 13th International Conference on Future Networks and Communications (FNC-2018) / Affiliated Workshops},
  abstract = {Compared to the traditional Cloud solution which consists of hiring resources from a single Cloud provider, Multi Cloud is a rather more efficient solution; because it combines the diverse benefits from them, thus, the different platforms complement each other on behalf the client applications. Moreover, Serverless Function technology is a powerful Cloud tool that hides the unnecessary infrastructure management details, hence, allowing the developers to solely focus on their own functional code. Nevertheless, as far as we found, Serverless Functions are always limited to the provider offering them and they are not adopted in a Multi Cloud context. In this paper, we tackled this limit by suggesting a distributed architecture which extends the Serverless technology advantages to a wider scope, permitting the client to get at time the Multi Cloud and Serverless strengths.},
  doi      = {https://doi.org/10.1016/j.procs.2018.07.152},
  file     = {:files/SOLTANI2018121.pdf:PDF},
  keywords = {Multi Cloud, Serverless architecture, Peer to Peer, container, container cluster manager},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050918311153},
}

@Article{2017arXiv170307562S,
  author        = {{Spillner}, Josef},
  title         = {{Snafu: Function-as-a-Service (FaaS) Runtime Design and Implementation}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1703.07562},
  month         = {Mar},
  abstract      = {Snafu, or Snake Functions, is a modular system to host, execute and manage language-level functions offered as stateless (micro-)services to diverse external triggers. The system interfaces resemble those of commercial FaaS providers but its implementation provides distinct features which make it overall useful to research on FaaS and prototyping of FaaS-based applications. This paper argues about the system motivation in the presence of already existing alternatives, its design and architecture, the open source implementation and collected metrics which characterise the system. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170307562S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1703.07562},
  eprint        = {1703.07562},
  file          = {:files/2017arXiv170307562S.pdf:PDF},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, C.2.4, H.3.5, D.1.1},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1703.07562.pdf},
}

@Article{2018arXiv180906100S,
  author        = {{Stein}, Manuel},
  title         = {{The Serverless Scheduling Problem and NOAH}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1809.06100},
  month         = {Sep},
  abstract      = {The serverless scheduling problem poses a new challenge to Cloud service platform providers because it is rather a job scheduling problem than a traditional resource allocation or request load balancing problem. Traditionally, elastic cloud applications use managed virtual resource allocation and employ request load balancers to orchestrate the deployment. With serverless, the provider needs to solve both the load balancing and the allocation. This work reviews the current Apache OpenWhisk serverless event load balancing and a noncooperative game-theoretic load balancing approach for response time minimization in distributed systems. It is shown by simulation that neither performs well under high system utilization which inspired a noncooperative online allocation heuristic that allows tuning the trade-off between for response time and resource cost of each serverless function. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180906100S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1809.06100},
  eprint        = {1809.06100},
  file          = {:files/2018arXiv180906100S.pdf:PDF},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1809.06100.pdf},
}

@InProceedings{Trach:2019:CTS:3319647.3325835,
  author    = {Trach, Bohdan and Oleksenko, Oleksii and Gregor, Franz and Bhatotia, Pramod and Fetzer, Christof},
  title     = {Clemmys: Towards Secure Remote Execution in FaaS},
  booktitle = {Proceedings of the 12th ACM International Conference on Systems and Storage},
  year      = {2019},
  series    = {SYSTOR '19},
  pages     = {44--54},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3325835},
  doi       = {10.1145/3319647.3325835},
  file      = {:files/Trach2019CTS3319647.3325835.pdf:PDF},
  isbn      = {978-1-4503-6749-3},
  location  = {Haifa, Israel},
  numpages  = {11},
  url       = {http://doi.acm.org/10.1145/3319647.3325835},
}

@InProceedings{vanEyk:2018:SRC:3185768.3186308,
  author    = {van Eyk, Erwin and Iosup, Alexandru and Abad, Cristina L. and Grohmann, Johannes and Eismann, Simon},
  title     = {A SPEC RG Cloud Group's Vision on the Performance Challenges of FaaS Cloud Architectures},
  booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
  year      = {2018},
  series    = {ICPE '18},
  pages     = {21--24},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3186308},
  doi       = {10.1145/3185768.3186308},
  file      = {:files/vanEyk2018.pdf:PDF},
  isbn      = {978-1-4503-5629-9},
  keywords  = {FaaS, benchmarking, function-as-a-service, performance evaluation, reference architecture, serverless computing},
  location  = {Berlin, Germany},
  numpages  = {4},
  url       = {http://doi.acm.org/10.1145/3185768.3186308},
}

@InProceedings{Wang:2019:REO:3302424.3303978,
  author    = {Wang, Kai-Ting Amy and Ho, Rayson and Wu, Peng},
  title     = {Replayable Execution Optimized for Page Sharing for a Managed Runtime Environment},
  booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
  year      = {2019},
  series    = {EuroSys '19},
  pages     = {39:1--39:16},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3303978},
  articleno = {39},
  doi       = {10.1145/3302424.3303978},
  file      = {:files/Wang2019.pdf:PDF},
  isbn      = {978-1-4503-6281-8},
  keywords  = {Cloud Computing, Operating Systems, Programming Languages and Runtimes},
  location  = {Dresden, Germany},
  numpages  = {16},
  url       = {http://doi.acm.org/10.1145/3302424.3303978},
}

@InProceedings{Aske:2018:SMS:3229710.3229742,
  author    = {Aske, Austin and Zhao, Xinghui},
  title     = {Supporting Multi-Provider Serverless Computing on the Edge},
  booktitle = {Proceedings of the 47th International Conference on Parallel Processing Companion},
  year      = {2018},
  series    = {ICPP '18},
  pages     = {20:1--20:6},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3229742},
  articleno = {20},
  doi       = {10.1145/3229710.3229742},
  file      = {:files/Aske2018.pdf:PDF},
  isbn      = {978-1-4503-6523-9},
  keywords  = {Edge Computing, Function-as-a-Service, Performance, Scheduling, Serverless Computing},
  location  = {Eugene, OR, USA},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3229710.3229742},
}

@InProceedings{10.1007/978-3-319-99819-0_11,
  author    = {Back, Timon and Andrikopoulos, Vasilios},
  title     = {Using a Microbenchmark to Compare Function as a Service Solutions},
  booktitle = {Service-Oriented and Cloud Computing},
  year      = {2018},
  editor    = {Kritikos, Kyriakos and Plebani, Pierluigi and de Paoli, Flavio},
  pages     = {146--160},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {The Function as a Service (FaaS) subtype of serverless computing provides the means for abstracting away from servers on which developed software is meant to be executed. It essentially offers an event-driven and scalable environment in which billing is based on the invocation of functions and not on the provisioning of resources. This makes it very attractive for many classes of applications with bursty workload. However, the terms under which FaaS services are structured and offered to consumers uses mechanisms like GB--seconds (that is, X GigaBytes of memory used for Y seconds of execution) that differ from the usual models for compute resources in cloud computing. Aiming to clarify these terms, in this work we develop a microbenchmark that we use to evaluate the performance and cost model of popular FaaS solutions using well known algorithmic tasks. The results of this process show a field still very much under development, and justify the need for further extensive benchmarking of these services.},
  file      = {:files/10.1007978-3-319-99819-0_11.pdf:PDF},
  isbn      = {978-3-319-99819-0},
}

@InProceedings{8247460,
  author    = {E. F. {Boza} and C. L. {Abad} and M. {Villavicencio} and S. {Quimba} and J. A. {Plaza}},
  title     = {Reserved, on demand or serverless: Model-based simulations for cloud budget planning},
  booktitle = {2017 IEEE Second Ecuador Technical Chapters Meeting (ETCM)},
  year      = {2017},
  pages     = {1-6},
  month     = {Oct},
  abstract  = {Cloud computing providers offer a variety of pricing models, complicating the client decision, as no single model is the cheapest in all scenarios. In addition, small to medium-sized organizations frequently lack personnel that can navigate the intricacies of each pricing model, and as a result, end up opting for a sub-optimal strategy, leading to overpaying for computing resources or not being able to meet performance goals. In this paper, we: (1) present the results of a study that shows that, in Ecuador, a considerable percentage of companies choose conservative pricing strategies, (2) present a case study that shows that the conservative pricing strategy is suboptimal under certain workloads, and (3) propose a set of models, a tool and a process that can be used by tenants to properly plan and budget their cloud computing costs. Our tool is based on M (t)/M/* queuing theory models and is easy to configure and use. Note that, even though we are motivated by our study of adoption of cloud computing technologies in Ecuador, our tool and process are widely applicable and not restricted to the Ecuadorian context.},
  doi       = {10.1109/ETCM.2017.8247460},
  file      = {:files/8247460.pdf:PDF},
  keywords  = {cloud computing;decision making;optimisation;pricing;queueing theory;securities trading;cloud budget planning;cloud computing providers;pricing model;client decision;conservative pricing strategy;cloud computing costs;suboptimal strategy;model-based simulations;small to medium-sized organizations;M (t)/M/* queuing theory models;Cloud computing;Pricing;Computational modeling;Tools;Companies;Electronic mail;Cloud;reserved;on-demand;serverless;budget;simulation;queuing theory},
}

@InProceedings{8103476,
  author    = {K. S. {Chang} and S. J. {Fink}},
  title     = {Visualizing serverless cloud application logs for program understanding},
  booktitle = {2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)},
  year      = {2017},
  pages     = {261-265},
  month     = {Oct},
  abstract  = {A cloud platform records a wealth of information regarding program execution. Most cloud service providers offer dashboard monitoring tools that visualize resource usage and billing information, and support debugging. In this paper, we present a tool that visualizes cloud execution logs for a different goal - to facilitate program understanding and generate documentations for an application using runtime data. Our tool introduces a new timeline visualization, a new method and user interface to summarize multiple JSON objects and present the result, and interaction techniques that facilitate navigating among functions. Together, these features explain a serverless cloud application's composition, performance, dataflow and data schema. We report some initial user feedback from several expert developers that were involved in the tool's design and development process.},
  doi       = {10.1109/VLHCC.2017.8103476},
  file      = {:files/8103476.pdf:PDF},
  issn      = {1943-6106},
  keywords  = {cloud computing;data flow analysis;data visualisation;program debugging;resource allocation;system monitoring;user interfaces;program understanding;runtime data;timeline visualization;user interface;serverless cloud application;cloud service providers;resource usage;billing information;debugging;cloud platform;program execution;dashboard monitoring tools;cloud execution logs;JSON objects;Tools;Cloud computing;Data visualization;Unified modeling language;Bars;Computational modeling;Visualization;serverless computing;function as a service;program understanding;log visualization;cloud computing},
}

@InProceedings{8567674,
  author    = {T. {Elgamal}},
  title     = {Costless: Optimizing Cost of Serverless Computing through Function Fusion and Placement},
  booktitle = {2018 IEEE/ACM Symposium on Edge Computing (SEC)},
  year      = {2018},
  pages     = {300-312},
  month     = {Oct},
  abstract  = {Serverless computing has recently experienced significant adoption by several applications, especially Internet of Things (IoT) applications. In serverless computing, rather than deploying and managing dedicated virtual machines, users are able to deploy individual functions, and pay only for the time that their code is actually executing. However, since serverless platforms are relatively new, they have a completely different pricing model that depends on the memory, duration, and the number of executions of a sequence/workflow of functions. In this paper we present an algorithm that optimizes the price of serverless applications in AWS Lambda. We first describe the factors affecting price of serverless applications which include: (1) fusing a sequence of functions, (2) splitting functions across edge and cloud resources, and (3) allocating the memory for each function. We then present an efficient algorithm to explore different function fusion-placement solutions and find the solution that optimizes the application's price while keeping the latency under a certain threshold. Our results on image processing workflows show that the algorithm can find solutions optimizing the price by more than 35%-57% with only 5%-15% increase in latency. We also show that our algorithm can find non-trivial memory configurations that reduce both latency and price.},
  doi       = {10.1109/SEC.2018.00029},
  file      = {:files/8567674.pdf:PDF},
  keywords  = {cloud computing;image fusion;Internet of Things;virtual machines;serverless computing;serverless platforms;serverless applications;Internet of Things;virtual machines;splitting functions;function fusion-placement solutions;AWS Lambda;edge resources;cloud resources;image processing workflows;nontrivial memory configuration;Computational modeling;Fuses;Pricing;Cloud computing;Internet of Things;Memory management;Face;Serverless;Edge computing;AWS Lambda;Cloud Computing;Cost Optimization},
}

@Article{doi:10.1002/cpe.4792,
  author   = {Figiela, Kamil and Gajek, Adam and Zima, Adam and Obrok, Beata and Malawski, Maciej},
  title    = {Performance evaluation of heterogeneous cloud functions},
  journal  = {Concurrency and Computation: Practice and Experience},
  year     = {2018},
  volume   = {30},
  number   = {23},
  pages    = {e4792},
  note     = {e4792 cpe.4792},
  abstract = {Summary Cloud Functions, often called Function-as-a-Service (FaaS), pioneered by AWS Lambda, are an increasingly popular method of running distributed applications. As in other cloud offerings, cloud functions are heterogeneous due to variations in underlying hardware, runtime systems, as well as resource management and billing models. In this paper, we focus on performance evaluation of cloud functions, taking into account heterogeneity aspects. We developed a cloud function benchmarking framework, consisting of one suite based on Serverless Framework and one based on HyperFlow. We deployed the CPU-intensive benchmarks: Mersenne Twister and Linpack. We measured the data transfer times between cloud functions and storage, and we measured the lifetime of the runtime environment. We evaluated all the major cloud function providers: AWS Lambda, Azure Functions, Google Cloud Functions, and IBM Cloud Functions. We made our results available online and continuously updated. We report on the results of the performance evaluation, and we discuss the discovered insights into resource allocation policies.},
  doi      = {10.1002/cpe.4792},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.4792},
  file     = {:files/doi10.1002cpe.4792.pdf:PDF},
  keywords = {cloud computing, cloud functions, FaaS, performance evaluation, serverless},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4792},
}

@InProceedings{10.1007/978-3-030-13342-9_15,
  author    = {Horovitz, Shay and Amos, Roei and Baruch, Ohad and Cohen, Tomer and Oyar, Tal and Deri, Afik},
  title     = {FaaStest - Machine Learning Based Cost and Performance FaaS Optimization},
  booktitle = {Economics of Grids, Clouds, Systems, and Services},
  year      = {2019},
  editor    = {Coppola, Massimo and Carlini, Emanuele and D'Agostino, Daniele and Altmann, J{\"o}rn and Ba{\~{n}}ares, Jos{\'e} {\'A}ngel},
  pages     = {171--186},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {With the emergence of Function-as-a-Service (FaaS) in the cloud, pay-per-use pricing models became available along with the traditional fixed price model for VMs and increased the complexity of selecting the optimal platform for a given service. We present FaaStest - an autonomous solution for cost and performance optimization of FaaS services by taking a hybrid approach - learning the behavioral patterns of the service and dynamically selecting the optimal platform. Moreover, we combine a prediction based solution for reducing cold starts of FaaS services. Experiments present a reduction of over 50{\%} in cost and over 90{\%} in response time for FaaS calls.},
  file      = {:files/10.1007978-3-030-13342-9_15.pdf:PDF},
  isbn      = {978-3-030-13342-9},
}

@InProceedings{10.1007/978-3-030-03673-7_4,
  author    = {Ivanov, Vitalii and Smolander, Kari},
  title     = {Implementation of a DevOps Pipeline for Serverless Applications},
  booktitle = {Product-Focused Software Process Improvement},
  year      = {2018},
  editor    = {Kuhrmann, Marco and Schneider, Kurt and Pfahl, Dietmar and Amasaki, Sousuke and Ciolkowski, Marcus and Hebig, Regina and Tell, Paolo and Kl{\"u}nder, Jil and K{\"u}pper, Steffen},
  pages     = {48--64},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {Context: The term ``serverless'' defines applications that use elements of Function as a Service or Backend as a Service cloud models in their architectures. Serverless promises infrastructure and operations cost reduction, faster software development, and automatic application scalability. Although many practitioners agree that Serverless simplifies operations part of DevOps, it still requires a new approach to automation practices because of the differences in its design and development workflow. Goal: The goal of this paper is to explore how Serverless affects DevOps practices and demonstrate a DevOps pipeline implementation for a Serverless case project. Method: As the method, we use the design science research, where the resulting artefact is a release and monitoring pipeline designed and implemented according to the requirements of the case organization. Results: The result of the study is an automated DevOps pipeline with an implementation of Continuous Integration, Continuous Delivery and Monitoring practices as required by the Serverless approach of the case project. Conclusions: The outcome shows how strongly the Serverless approach affects some automation practices such as test execution, deployment and monitoring of the application. In total, 18 out of 27 implemented practices were influenced by the Serverless-specific features of the project.},
  file      = {:files/10.1007978-3-030-03673-7_4.pdf:PDF},
  isbn      = {978-3-030-03673-7},
}

@InProceedings{8605773,
  author    = {D. {Jackson} and G. {Clynch}},
  title     = {An Investigation of the Impact of Language Runtime on the Performance and Cost of Serverless Functions},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {154-160},
  month     = {Dec},
  abstract  = {Serverless, otherwise known as "Function-as-a-Service" (FaaS), is a compelling evolution of cloud computing that is highly scalable and event-driven. Serverless applications are composed of multiple independent functions, each of which can be implemented in a range of programming languages. This paper seeks to understand the impact of the choice of language runtime on the performance and subsequent cost of serverless function execution. It presents the design and implementation of a new serverless performance testing framework created to analyse performance and cost metrics for both AWS Lambda and Azure Functions. For optimum performance and cost management of serverless applications, Python is the clear choice on AWS Lambda. C# .NET is the top performer and most economical option for Azure Functions. NodeJS on Azure Functions and .NET Core 2 on AWS should be avoided or at the very least, used carefully in order to avoid their potentially slow and costly start-up times.},
  doi       = {10.1109/UCC-Companion.2018.00050},
  file      = {:files/8605773.pdf:PDF},
  keywords  = {cloud computing;program testing;programming languages;serverless functions;event-driven system;NodeJS;Azure Functions;AWS Lambda;serverless performance testing framework;serverless function execution;programming languages;cloud computing;compelling evolution;Function-as-a-Service;language runtime;Runtime;Measurement;Testing;Containers;Java;C# languages;Cloud computing;serverless;FaaS;Lambda;aws;azure;functions;performance;cloud},
}

@Article{2019arXiv190205870J,
  author        = {{Jangda}, Abhinav and {Pinckney}, Donald and {Baxter}, Samuel and {Devore-McDonald}, Breanna and {Spitzer}, Joseph and {Brun}, Yuriy and {Guha}, Arjun},
  title         = {{Formal Foundations of Serverless Computing}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1902.05870},
  month         = {Feb},
  abstract      = {A robust, large-scale web service can be difficult to engineer. When demand spikes, it must configure new machines and manage load-balancing; when demand falls, it must shut down idle machines to reduce costs; and when a machine crashes, it must quickly work around the failure without losing data. In recent years, serverless computing, a new cloud computing abstraction, has emerged to help address these challenges. In serverless computing, programmers write serverless functions, and the cloud platform transparently manages the operating system, resource allocation, load-balancing, and fault tolerance. In 2014, Amazon Web Services introduced the first serverless platform, AWS Lambda, and similar abstractions are now available on all major clouds. Unfortunately, the serverless computing abstraction exposes several low-level operational details that make it hard for programmers to write and reason about their code. This paper sheds light on this problem by presenting , an operational semantics of the essence of serverless computing. Despite being a small core calculus (less than one column),  models all the low-level details that serverless functions can observe. To show that  is useful, we present three applications. First, to make it easier for programmers to reason about their code, we present a simplified semantics of serverless execution and precisely characterize when the simplified semantics and  coincide. Second, we augment  with a key-value store, which allows us to reason about stateful serverless functions. Third, since a handful of serverless platforms support serverless function composition, we show how to extend  with a composition language. We have implemented this composition language and show that it outperforms prior work. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190205870J},
  archiveprefix = {arXiv},
  eid           = {arXiv:1902.05870},
  eprint        = {1902.05870},
  file          = {:files/2019arXiv190205870J.pdf:PDF},
  keywords      = {Computer Science - Programming Languages},
  primaryclass  = {cs.PL},
  url           = {https://arxiv.org/pdf/1902.05870.pdf},
}

@InProceedings{Klimovic:2018:PEE:3291168.3291200,
  author    = {Klimovic, Ana and Wang, Yawen and Stuedi, Patrick and Trivedi, Animesh and Pfefferle, Jonas and Kozyrakis, Christos},
  title     = {Pocket: Elastic Ephemeral Storage for Serverless Analytics},
  booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
  year      = {2018},
  series    = {OSDI'18},
  pages     = {427--444},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3291200},
  file      = {:files/Klimovic2018PEE3291168.3291200.pdf:PDF},
  isbn      = {978-1-931971-47-8},
  location  = {Carlsbad, CA, USA},
  numpages  = {18},
  url       = {http://dl.acm.org/citation.cfm?id=3291168.3291200},
}

@InProceedings{10.1007/978-3-319-69035-3_48,
  author    = {Kuhlenkamp, J{\"o}rn and Klems, Markus},
  title     = {Costradamus: A Cost-Tracing System for Cloud-Based Software Services},
  booktitle = {Service-Oriented Computing},
  year      = {2017},
  editor    = {Maximilien, Michael and Vallecillo, Antonio and Wang, Jianmin and Oriol, Marc},
  pages     = {657--672},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {Cloud providers offer a range of fully managed infrastructure services that enable a ``serverless'' architecture and development paradigm. Following this paradigm, software services can be built on compositions of cloud infrastructure services that offer fine-granular pay-per-use pricing models. While this development and deployment approach simplifies service development and management, it remains an open challenge to make use of fine-granular pricing models for improving cost transparency and reducing cost of service operations. As a solution, we present Costradamus, a cost-tracing system that implements a generic cost model and three different tracing approaches. With Costradamus, we can derive cost and performance information per API operation. We evaluate our approach and system in a smart grid context and discuss unexpected performance and deployment cost tradeoffs.},
  file      = {:files/10.1007978-3-319-69035-3_48.pdf:PDF},
  isbn      = {978-3-319-69035-3},
}

@InProceedings{8457807,
  author    = {W. {Lin} and C. {Krintz} and R. {Wolski}},
  title     = {Tracing Function Dependencies across Clouds},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {253-260},
  month     = {July},
  abstract  = {In this paper, we present Lowgo, a crosscloud tracing tool for capturing causal relationships in serverless applications. To do so, Lowgo records dependencies between functions, through cloud services, and across regions to facilitate debugging and reasoning about highly concurrent, multi-cloud applications. We empirically evaluate Lowgo using microbenchmarks and multi-function and multi-cloud applications. We find that Lowgo is able to capture causal dependencies with overhead that ranges from 2-12%, which is less than half that of the best-performing, cloud-specific approach.},
  doi       = {10.1109/CLOUD.2018.00039},
  file      = {:files/8457807.pdf:PDF},
  issn      = {2159-6190},
  keywords  = {causality;cloud computing;program debugging;function dependencies;clouds;crosscloud tracing tool;causal relationships;serverless applications;Lowgo records dependencies;cloud services;multicloud applications;microbenchmarks;causal dependencies;cloud-specific approach;Cloud computing;Tools;Pipelines;Servers;Google;Debugging;Computational modeling;cloud computing;serverless computing;function as a service;faas;AWS Lambda;Azure Functions},
}

@InProceedings{8360312,
  author    = {W. {Lin} and C. {Krintz} and R. {Wolski} and M. {Zhang} and X. {Cai} and T. {Li} and W. {Xu}},
  title     = {Tracking Causal Order in AWS Lambda Applications},
  booktitle = {2018 IEEE International Conference on Cloud Engineering (IC2E)},
  year      = {2018},
  pages     = {50-60},
  month     = {April},
  abstract  = {Serverless computing is a new cloud programming and deployment paradigm that is receiving wide-spread uptake. Serverless offerings such as Amazon Web Services (AWS) Lambda, Google Functions, and Azure Functions automatically execute simple functions uploaded by developers, in response to cloud-based event triggers. The serverless abstraction greatly simplifies integration of concurrency and parallelism into cloud applications, and enables deployment of scalable distributed systems and services at very low cost. Although a significant first step, the serverless abstraction requires tools that software engineers can use to reason about, debug, and optimize their increasingly complex, asynchronous applications. Toward this end, we investigate the design and implementation of GammaRay, a cloud service that extracts causal dependencies across functions and through cloud services, without programmer intervention. We implement GammaRay for AWS Lambda and evaluate the overheads that it introduces for serverless micro-benchmarks and applications written in Python.},
  doi       = {10.1109/IC2E.2018.00027},
  file      = {:files/8360312.pdf:PDF},
  keywords  = {cloud computing;Web services;cloud applications;scalable distributed systems;serverless abstraction;increasingly complex applications;asynchronous applications;cloud service;AWS Lambda applications;serverless computing;cloud programming;wide-spread uptake;serverless offerings;Amazon Web Services Lambda;Google Functions;Azure Functions;simple functions;cloud-based event triggers;concurrency;parallelism;causal dependencies extraction;causal order tracking;GammaRay;Cloud computing;Containers;X-ray imaging;Tools;Monitoring;Concurrent computing;serverless;faas;causal dependencies;AWS Lambda;profiling},
}

@InProceedings{10.1007/978-3-319-75178-8_34,
  author    = {Malawski, Maciej and Figiela, Kamil and Gajek, Adam and Zima, Adam},
  title     = {Benchmarking Heterogeneous Cloud Functions},
  booktitle = {Euro-Par 2017: Parallel Processing Workshops},
  year      = {2018},
  editor    = {Heras, Dora B. and Boug{\'e}, Luc and Mencagli, Gabriele and Jeannot, Emmanuel and Sakellariou, Rizos and Badia, Rosa M. and Barbosa, Jorge G. and Ricci, Laura and Scott, Stephen L. and Lankes, Stefan and Weidendorfer, Josef},
  pages     = {415--426},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {Cloud Functions, often called Function-as-a-Service (FaaS), pioneered by AWS Lambda, are an increasingly popular method of running distributed applications. As in other cloud offerings, cloud functions are heterogeneous, due to different underlying hardware, runtime systems, as well as resource management and billing models. In this paper, we focus on performance evaluation of cloud functions, taking into account heterogeneity aspects. We developed a cloud function benchmarking framework, consisting of one suite based on Serverless Framework, and one based on HyperFlow. We deployed the CPU-intensive benchmarks: Mersenne Twister and Linpack, and evaluated all the major cloud function providers: AWS Lambda, Azure Functions, Google Cloud Functions and IBM OpenWhisk. We make our results available online and continuously updated. We report on the initial results of the performance evaluation and we discuss the discovered insights on the resource allocation policies.},
  file      = {:files/10.1007978-3-319-75178-8_34.pdf:PDF},
  isbn      = {978-3-319-75178-8},
}

@InProceedings{8605777,
  author    = {J. {Manner} and M. {Endre} and T. {Heckel} and G. {Wirtz}},
  title     = {Cold Start Influencing Factors in Function as a Service},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {181-188},
  month     = {Dec},
  abstract  = {Function as a Service (FaaS) is a young and rapidly evolving cloud paradigm. Due to its hardware abstraction, inherent virtualization problems come into play and need an assessment from the FaaS point of view. Especially avoidance of idling and scaling on demand cause a lot of container starts and as a consequence a lot of cold starts for FaaS users. The aim of this paper is to address the cold start problem in a benchmark and investigate influential factors on the duration of the perceived cold start. We conducted a benchmark on AWS Lambda and Microsoft Azure Functions with 49500 cloud function executions. Formulated as hypotheses, the influence of the chosen programming language, platform, memory size for the cloud function, and size of the deployed artifact are the dimensions of our benchmark. Cold starts on the platform as well as the cold starts for users were measured and compared to each other. Our results show that there is an enormous difference for the overhead the user perceives compared to the billed duration. In our benchmark, the average cold start overheads on the user's side ranged from 300ms to 24s for the chosen configurations.},
  doi       = {10.1109/UCC-Companion.2018.00054},
  file      = {:files/8605777.pdf:PDF},
  keywords  = {cloud computing;virtualisation;container starts;cold start problem;Microsoft Azure Functions;virtualization problems;cloud function executions;Function as a Service;AWS Lambda;FAA;Containers;Benchmark testing;Cloud computing;Java;Pipelines;Serverless Computing, Function as a Service, FaaS, Cloud Functions, Cold Start, Benchmarking},
}

@Article{Manner2019,
  author   = {Manner, Johannes and Kolb, Stefan and Wirtz, Guido},
  title    = {Troubleshooting Serverless functions: a combined monitoring and debugging approach},
  journal  = {SICS Software-Intensive Cyber-Physical Systems},
  year     = {2019},
  month    = {Feb},
  issn     = {2524-8529},
  abstract = {Today, Serverless computing gathers pace and attention in the cloud computing area. The abstraction of operational tasks combined with the auto-scaling property are convincing reasons to adapt this new cloud paradigm. Building applications in a Serverless style via cloud functions is challenging due to the fine-grained architecture and the tighter coupling to back end services. Increased complexity, loss of control over software layers and the large number of participating functions and back end services complicate the task of finding the cause of a faulty execution. A tedious but widespread strategy is the manual analysis of log data. In this paper, we present a semi-automated troubleshooting process to improve fault detection and resolution for Serverless functions. Log data is the vehicle to enable a posteriori analysis. The process steps of our concept enhance the log quality, detect failed executions automatically, and generate test skeletons based on the information provided in the log data. Ultimately, this leads to an increased test coverage, a better regression testing and more robust functions. Developers can trigger this process asynchronously and work with their accustomed tools. We also present a prototype SeMoDe to validate our approach for Serverless functions implemented in Java and deployed to AWS Lambda.},
  day      = {06},
  doi      = {10.1007/s00450-019-00398-6},
  file     = {:files/Manner2019.pdf:PDF},
  url      = {https://doi.org/10.1007/s00450-019-00398-6},
}

@InProceedings{8605775,
  author    = {P. {Moczurad} and M. {Malawski}},
  title     = {Visual-Textual Framework for Serverless Computation: A Luna Language Approach},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {169-174},
  month     = {Dec},
  abstract  = {As serverless technologies are emerging as a breakthrough in the cloud computing industry, the lack of proper tooling is becoming apparent. The model of computation that the serverless is imposing is as flexible as it is hard to manage and grasp. We present a novel approach towards serverless computing that tightly integrates it with the visual-textual, functional programming language: Luna. This way we are hoping to achieve the clarity and cognitive ease of visual solutions while retaining the flexibility and expressive power of textual programming languages. We created a proof of concept of the Luna Serverless Framework in which we extend the Luna standard library and we leverage the language features to create an intuitive API for serverless function calls using AWS Lambda and to call external functions implemented in JavaScript.},
  doi       = {10.1109/UCC-Companion.2018.00052},
  file      = {:files/8605775.pdf:PDF},
  keywords  = {application program interfaces;cloud computing;data visualisation;functional programming;Java;visual programming;Luna language approach;serverless technologies;cloud computing industry;serverless computing;visual-textual programming language;functional programming language;textual programming languages;Luna standard library;serverless function;visual-textual framework;Luna serverless framework;AWS Lambda;JavaScript;Visualization;Cloud computing;Computer languages;Computational modeling;Market research;Programming;Semantics;serverless;functional programming;visual programming},
}

@InProceedings{Nadgowda:2017:LSA:3154847.3154850,
  author    = {Nadgowda, Shripad and Bila, Nilton and Isci, Canturk},
  title     = {The Less Server Architecture for Cloud Functions},
  booktitle = {Proceedings of the 2Nd International Workshop on Serverless Computing},
  year      = {2017},
  series    = {WoSC '17},
  pages     = {22--27},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3154850},
  doi       = {10.1145/3154847.3154850},
  file      = {:files/Nadgowda2017.pdf:PDF},
  isbn      = {978-1-4503-5434-9},
  keywords  = {cloud functions, data deduplication, serverless platform},
  location  = {Las Vegas, Nevada},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3154847.3154850},
}

@Article{2019arXiv190507228P,
  author        = {{Pellegrini}, Roland and {Ivkic}, Igor and {Tauber}, Markus},
  title         = {{Towards a Security-Aware Benchmarking Framework for Function-as-a-Service}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1905.07228},
  month         = {May},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190507228P},
  archiveprefix = {arXiv},
  eid           = {arXiv:1905.07228},
  eprint        = {1905.07228},
  file          = {:files/2019arXiv190507228P.pdf:PDF},
  keywords      = {Computer Science - Software Engineering, Computer Science - Cryptography and Security},
  primaryclass  = {cs.SE},
}

@Article{2019arXiv190511707P,
  author        = {{Pellegrini}, Roland and {Ivkic}, Igor and {Tauber}, Markus},
  title         = {{Function-as-a-Service Benchmarking Framework}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1905.11707},
  month         = {May},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190511707P},
  archiveprefix = {arXiv},
  eid           = {arXiv:1905.11707},
  eprint        = {1905.11707},
  file          = {:files/2019arXiv190511707P.pdf:PDF},
  keywords      = {Computer Science - Performance, Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.PF},
}

@Article{PEREZ201850,
  author   = {Alfonso Prez and Germn Molt and Miguel Caballer and Amanda Calatrava},
  title    = {Serverless computing for container-based architectures},
  journal  = {Future Generation Computer Systems},
  year     = {2018},
  volume   = {83},
  pages    = {50 - 59},
  issn     = {0167-739X},
  abstract = {New architectural patterns (e.g. microservices), the massive adoption of Linux containers (e.g. Docker containers), and improvements in key features of Cloud computing such as auto-scaling, have helped developers to decouple complex and monolithic systems into smaller stateless services. In turn, Cloud providers have introduced serverless computing, where applications can be defined as a workflow of event-triggered functions. However, serverless services, such as AWS Lambda, impose serious restrictions for these applications (e.g. using a predefined set of programming languages or difficulting the installation and deployment of external libraries). This paper addresses such issues by introducing a framework and a methodology to create Serverless Container-aware ARchitectures (SCAR). The SCAR framework can be used to create highly-parallel event-driven serverless applications that run on customized runtime environments defined as Docker images on top of AWS Lambda. This paper describes the architecture of SCAR together with the cache-based optimizations applied to minimize cost, exemplified on a massive image processing use case. The results show that, by means of SCAR, AWS Lambda becomes a convenient platform for High Throughput Computing, specially for highly-parallel bursty workloads of short stateless jobs.},
  doi      = {https://doi.org/10.1016/j.future.2018.01.022},
  file     = {:files/PEREZ201850.pdf:PDF},
  keywords = {Cloud computing, Serverless, Docker, Elasticity, AWS lambda},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X17316485},
}

@InProceedings{8588841,
  author    = {D. {Pinto} and J. P. {Dias} and H. {Sereno Ferreira}},
  title     = {Dynamic Allocation of Serverless Functions in IoT Environments},
  booktitle = {2018 IEEE 16th International Conference on Embedded and Ubiquitous Computing (EUC)},
  year      = {2018},
  pages     = {1-8},
  month     = {Oct},
  abstract  = {The IoT area has grown significantly in the last few years and is expected to reach a gigantic amount of 50 billion devices by 2020. The appearance of serverless architectures, specifically highlighting FaaS, raises the question of the suitability of using them in IoT environments. Combining IoT with a serverless architectural design can effective when trying to make use of local processing power that exists in a local network of IoT devices and creating a fog layer that leverages computational capabilities that are closer to the end-user. In this approach, which is placed between the device and the serverless function, when a device requests for the execution of a serverless function will decide based on previous metrics of execution if the serverless function should be executed locally, in the fog layer of a local network of IoT devices, or if it should be executed remotely, in one of the available cloud servers. Therefore, this approach allows dynamically allocating functions to the most suitable layer.},
  doi       = {10.1109/EUC.2018.00008},
  file      = {:files/8588841.pdf:PDF},
  keywords  = {cloud computing;computer networks;Internet of Things;serverless function;device requests;fog layer;local network;IoT devices;IoT environments;IoT area;serverless architectural design;local processing power;cloud servers;Cloud computing;Servers;Edge computing;Runtime environment;Internet of Things;Market research;Estimation;Fog Computing;Internet of Things;Multi Armed Bandit;Ubiquitous Computing;Serverless},
}

@Article{2017arXiv170508169S,
  author        = {{Spillner}, Josef},
  title         = {{Transformation of Python Applications into Function-as-a-Service Deployments}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1705.08169},
  month         = {May},
  abstract      = {New cloud programming and deployment models pose challenges to software application engineers who are looking, often in vain, for tools to automate any necessary code adaptation and transformation. Function-as-a-Service interfaces are particular non-trivial targets when considering that most cloud applications are implemented in non-functional languages. Among the most widely used of these languages is Python. This starting position calls for an automated approach to transform monolithic Python code into modular FaaS units by partially automated decomposition. Hence, this paper introduces and evaluates Lambada, a Python module to dynamically decompose, convert and deploy unmodified Python code into AWS Lambda functions. Beyond the tooling in the form of a measured open source prototype implementation, the paper contributes a description of the algorithms and code rewriting rules as blueprints for transformations of other scripting languages. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170508169S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1705.08169},
  eprint        = {1705.08169},
  file          = {:files/2017arXiv170508169S.pdf:PDF},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Software Engineering, D.2.1, I.2.2, C.2.4},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1705.08169.pdf},
}

@Article{2017arXiv170205510S,
  author        = {{Spillner}, Josef and {Dorodko}, Serhii},
  title         = {{Java Code Analysis and Transformation into AWS Lambda Functions}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1702.05510},
  month         = {Feb},
  abstract      = {Software developers are faced with the issue of either adapting their programming model to the execution model (e.g. cloud platforms) or finding appropriate tools to adapt the model and code automatically. A recent execution model which would benefit from automated enablement is Function-as-a-Service. Automating this process requires a pipeline which includes steps for code analysis, transformation and deployment. In this paper, we outline the design and runtime characteristics of Podilizer, a tool which implements the pipeline specifically for Java source code as input and AWS Lambda as output. We contribute technical and economic metrics about this concrete 'FaaSification' process by observing the behaviour of Podilizer with two representative Java software projects.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170205510S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1702.05510},
  eprint        = {1702.05510},
  file          = {:files/2017arXiv170205510S.pdf:PDF},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Software Engineering, D.2.1, I.2.2, C.2.4},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1702.05510.pdf},
}

@InProceedings{8599581,
  author    = {M. {Wurster} and U. {Breitenbcher} and K. {Kpes} and F. {Leymann} and V. {Yussupov}},
  title     = {Modeling and Automated Deployment of Serverless Applications Using TOSCA},
  booktitle = {2018 IEEE 11th Conference on Service-Oriented Computing and Applications (SOCA)},
  year      = {2018},
  pages     = {73-80},
  month     = {Nov},
  abstract  = {The serverless computing paradigm brings multiple benefits to application developers who are interested in consuming computing resources as services without the need to manage physical capacities or limits. There are several deployment technologies and languages available suitable for deploying applications to a single cloud provider. However, for multi-cloud application deployments, multiple technologies have to be used and orchestrated. In addition, the event-driven nature of serverless computing imposes further requirements on modeling such application structures in order to automate their deployment. In this paper, we tackle these issues by introducing an event-driven deployment modeling approach using the standard Topology and Orchestration Specification for Cloud Applications (TOSCA) that fully employs the suggested standard lifecycle to provision and manage multi-cloud serverless applications. To show the feasibility of our approach, we extended the existing TOSCA-based ecosystem OpenTOSCA.},
  doi       = {10.1109/SOCA.2018.00017},
  file      = {:files/8599581.pdf:PDF},
  issn      = {2163-2871},
  keywords  = {cloud computing;computing resources;physical capacities;deployment technologies;single cloud provider;multicloud application deployments;multiple technologies;event-driven nature;application structures;event-driven deployment modeling approach;multicloud serverless applications;serverless computing paradigm;application developers;cloud applications;TOSCA-based ecosystem;OpenTOSCA;Cloud computing;Computational modeling;Topology;Standards;Computer architecture;FAA;Serverless;Multi-Cloud;Modeling;Automated Deployment;TOSCA},
}

@Comment{jabref-meta: databaseType:bibtex;}
