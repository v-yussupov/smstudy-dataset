% Encoding: UTF-8

@INPROCEEDINGS{8725610,
author={U. {Acar} and R. F. {Ustok} and S. {Keskin} and D. {Breitgand} and A. {Weit}},
booktitle={2018 IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN)},
title={Programming Tools for Rapid NFV-Based Media Application Development in 5G Networks},
year={2018},
volume={},
number={},
pages={1-5},
abstract={The emergence of virtualisation and Infrastructure-as-a-Service (IaaS) have dramatically transformed the telecom industry through network function virtualisation (NFV). A recently introduced cloud-native concept, Platform as a Service (PaaS), ensures to further boost the performance, portability and cost efficiency of the NFV. The 5G-MEDIA project proposes the application of a serverless paradigm known as Function-as-a- Service (FaaS) to NFV for the media applications exploiting the 5G technologies. In addition to integration of FaaS, the 5G-MEDIA application/service development kit (SDK) supports microservice-based application development for both hypervisor-based and containerized approaches, specifically supporting Docker, unikernel and LXC. In this paper, we provide an overview of the 5G-MEDIA SDK which is built to support NFV-based next generation media applications and to achieve a development time in the order of minutes. Furthermore, implementations of FaaS Emulation and FaaS command line interface (CLI) tools are also presented.},
keywords={Tools;Media;FAA;5G mobile communication;Emulation;Containers;Computer architecture;5G;5G MEDIA;NFV;SDK},
doi={10.1109/NFV-SDN.2018.8725610},
ISSN={},
month={Nov}
}

@Article{8653379,
  author   = {P. {Aditya} and I. E. {Akkus} and A. {Beck} and R. {Chen} and V. {Hilt} and I. {Rimac} and K. {Satzke} and M. {Stein}},
  title    = {Will Serverless Computing Revolutionize NFV?},
  journal  = {Proceedings of the IEEE},
  year     = {2019},
  volume   = {107},
  number   = {4},
  pages    = {667-678},
  month    = {April},
  issn     = {0018-9219},
  abstract = {Communication networks need to be both adaptive and scalable. The last few years have seen an explosive growth of software-defined networking (SDN) and network function virtualization (NFV) to address this need. Both technologies help enable networking software to be decoupled from the hardware so that software functionality is no longer constrained by the underlying hardware and can evolve independently. Both SDN and NFV aim to advance a software-based approach to networking, where networking functionality is implemented in software modules and executed on a suitable cloud computing platform. Achieving this goal requires the virtualization paradigm used in these services that play an important role in the transition to software-based networks. Consequently, the corresponding computing platforms accompanying the virtualization technologies need to provide the required agility, robustness, and scalability for the services executed. Serverless computing has recently emerged as a new paradigm in virtualization and has already significantly changed the economics of offloading computations to the cloud. It is considered as a low-latency, resource-efficient, and rapidly deployable alternative to traditional virtualization approaches, such as those based on virtual machines and containers. Serverless computing provides scalability and cost reduction, without requiring any additional configuration overhead on the part of the developer. In this paper, we explore and survey how serverless computing technology can help building adaptive and scalable networks and show the potential pitfalls of doing so.},
  doi      = {10.1109/JPROC.2019.2898101},
  keywords = {Communication netwowrks;Cloud computing;Virtualization;Hardware;Servers;Edge computing;Network function virtualization;Scalability;Application virtualization;cloud computing;edge computing;network function virtualization (NFV);serverless computing;software-defined networking (SDN)},
}

@InProceedings{8457832,
  author    = {Z. {Al-Ali} and S. {Goodarzy} and E. {Hunter} and S. {Ha} and R. {Han} and E. {Keller} and E. {Rozner}},
  title     = {Making Serverless Computing More Serverless},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {456-459},
  month     = {July},
  abstract  = {In serverless computing, developers define a function to handle an event, and the serverless framework horizontally scales the application as needed. The downside of this function-based abstraction is it limits the type of application supported and places a bound on the function to be within the physical resource limitations of the server the function executes on. In this paper we propose a new abstraction for serverless computing: a developer supplies a process and the serverless framework seamlessly scales out the process's resource usage across the datacenter. This abstraction enables processing to not only be more general purpose, but also allows a process to break out of the limitations of a single server - making serverless computing more serverless. To realize this abstraction, we propose ServerlessOS, comprised of three key components: (i) a new disaggregation model, which leverages disaggregation for abstraction, but enables resources to move fluidly between servers for performance; (ii) a cloud orchestration layer which manages fine-grained resource allocation and placement throughout the application's lifetime via local and global decision making; and (iii) an isolation capability that enforces data and resource isolation across disaggregation, effectively extending Linux cgroup functionality to span servers.},
  doi       = {10.1109/CLOUD.2018.00064},
  issn      = {2159-6190},
  keywords  = {cloud computing;computer centres;decision making;Linux;resource allocation;serverless computing;function-based abstraction;physical resource limitations;resource usage;datacenter;disaggregation model;ServerlessOS;cloud orchestration layer;fine-grained resource allocation;global decision making;local decision making;isolation capability;disaggregation;Linux cgroup functionality;Servers;Instruction sets;Cloud computing;Sockets;Couplings;Micromechanical devices;Memory management;serverless;cloud;virtualization;isolation;orchestration;resource disaggregation},
}

@InProceedings{8622117,
  author    = {E. {Al-Masri} and I. {Diabate} and R. {Jain} and M. H. {Lam} and S. {Reddy Nathala}},
  title     = {Recycle.io: An IoT-Enabled Framework for Urban Waste Management},
  booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
  year      = {2018},
  pages     = {5285-5287},
  month     = {Dec},
  abstract  = {Addressing environmentally safe management of waste is becoming increasingly a challenging task. The predicament of the rate at which waste is generated due to increasing populations is also contributing to this challenge. One possible approach for effectively handling waste can be achieved by source reduction and recycling. The problem, however, improving the collection of waste can be costly particularly during the source separation process after waste is collected. It would be desirable if there exists a mechanism that can help municipalities, local governments or waste management companies to monitor in real-time sources of violations prior to the waste collection process. In this paper, we introduce recycle.io, an Internet of Things (IoT)-enabled waste management system that is based on a serverless architecture that can identify these sources of violations. Using recycle.io, it is then possible to track the violations geographically which can help local governments, for example, to improve or enforce tighter regulations for waste disposal. Our recycle.io system uses Microsoft Azure IoT Hub for device management. Throughout the paper, we demonstrate usefulness of using our approach for urban waste management in smart cities.},
  doi       = {10.1109/BigData.2018.8622117},
  keywords  = {environmental science computing;Internet of Things;recycling;waste disposal;Recycle.io;Internet of Things;waste disposal;waste collection process;recycling;urban waste management;IoT-enabled framework;Image edge detection;Waste management;Real-time systems;Cloud computing;Sensors;Edge computing;Hardware;IoT devices;IoT gateways;waste management;industrial internet of things;IIoT;garbage collection;smart bin;smart garbage;smart city},
}

@InProceedings{8539123,
  author    = {E. {Al-Masri} and I. {Diabate} and R. {Jain} and M. H. L. {Lam} and S. R. {Nathala}},
  title     = {A Serverless IoT Architecture for Smart Waste Management Systems},
  booktitle = {2018 IEEE International Conference on Industrial Internet (ICII)},
  year      = {2018},
  pages     = {179-180},
  month     = {Oct},
  abstract  = {In recent years, the waste management and recycling industry has been facing multiple challenges such as protecting the public from harmful hazardous waste, promoting recycling and material source separation into common material streams. For instance, separating waste of hazardous wastes such as paint or batteries can be costly during the source separation process after waste is collected. What is therefore needed is a smart waste management system that is capable of identifying waste materials prior to the separation process. To overcome these challenges and more, we introduce recycle.io, a serverless Internet of Things (IoT) architecture for smart waste management systems. Using recycle.io, it is then possible to determine in real-time the types of source material violations prior to the waste collection. In this manner, waste management systems can identify sources of violations and rectify this by bringing awareness to the public or issuing fines to prevent violations from occurring. We demonstrate usefulness of our approach throughout the paper.},
  doi       = {10.1109/ICII.2018.00034},
  keywords  = {distributed processing;hazardous materials;Internet of Things;pollution control;real-time systems;recycling;separation;source separation process;smart waste management system;recycle.io;source material violations;waste collection;serverless IoT architecture;recycling industry;harmful hazardous waste;material source separation;public protection;real-time system;Waste management;Recycling;Source separation;Real-time systems;Internet of Things;Image edge detection;Cloud computing;IIoT;IoT devices;Waste Management;Industrial Internet;Garbage Collection;Smart Bin;Smart Garbage;Smart City},
}

@Article{8667014,
  author   = {F. {Alvarez} and D. {Breitgand} and D. {Griffin} and P. {Andriani} and S. {Rizou} and N. {Zioulis} and F. {Moscatelli} and J. {Serrano} and M. {Keltsch} and P. {Trakadas} and T. K. {Phan} and A. {Weit} and U. {Acar} and O. {Prieto} and F. {Iadanza} and G. {Carrozzo} and H. {Koumaras} and D. {Zarpalas} and D. {Jimenez}},
  title    = {An Edge-to-Cloud Virtualized Multimedia Service Platform for 5G Networks},
  journal  = {IEEE Transactions on Broadcasting},
  year     = {2019},
  pages    = {1-12},
  issn     = {0018-9316},
  abstract = {The focus of research into 5G networks to date has been largely on the required advances in network architectures, technologies, and infrastructures. Less effort has been put on the applications and services that will make use of and exploit the flexibility of 5G networks built upon the concept of software-defined networking (SDN) and network function virtualization (NFV). Media-based applications are amongst the most demanding services, requiring large bandwidths for high audio-visual quality, low-latency for interactivity, and sufficient infrastructure resources to deliver the computational power for running the media applications in the networked cloud. This paper presents a novel service virtualization platform (SVP), called 5G-MEDIA SVP, which leverages the principles of NFV and SDN to facilitate the development, deployment, and operation of media services on 5G networks. The platform offers an advanced cognitive management environment for the provisioning of network services (NSs) and media-related applications, which directly link their lifecycle management with user experience as well as optimization of infrastructure resource utilization. Another innovation of 5G-MEDIA SVP is the integration of serverless computing with media intensive applications in 5G networks, increasing cost effectiveness of operation and simplifying development and deployment time. The proposed SVP is being validated against three media use cases: 1) immersive virtual reality 3-D gaming application; 2) remote production of broadcast content incorporating user generated contents; and 3) dynamically adaptive content distribution networks for the intelligent distribution of ultrahigh definition content. The preliminary results of the 5G-MEDIA SVP platform evaluation are compared against current practice and show that the proposed platform provides enhanced functionality for the operators and infrastructure owners, while ensuring better NS performance to service providers and end users.},
  doi      = {10.1109/TBC.2019.2901400},
  keywords = {5G mobile communication;Media;Computer architecture;Cloud computing;Streaming media;Tools;Bandwidth;5G networks;network functions virtualization;serverless computing;immersive media;remote production;content delivery networks},
}

@InProceedings{8622913,
  author    = {T. {Asghar} and S. {Rasool} and M. {Iqbal} and Z. u. {Qayyum} and A. N. {Mian} and G. {Ubakanma}},
  title     = {Feasibility of Serverless Cloud Services for Disaster Management Information Systems},
  booktitle = {2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
  year      = {2018},
  pages     = {1054-1057},
  month     = {June},
  abstract  = {Serverless is the new generation of cloud services that supports the pay-per-use policy in true spirit by charging only for the execution time of the hosted code. Amazon introduced serverless service of Lambda in 2014 and it is consider as the most popular serverless cloud service till date. This paper focuses on the serverless cloud services of Lambda and elaborates the importance of Lambda based serverless cloud services for hosting the disaster management information systems (DMIS). We have identified two repeatedly occurring phases of the life cycle of a DMIS. These phases are high activity phase and low activity phase. Our findings state that serverless cloud services are well-suited for both of these phases of a DMIS. Serverless reduce the operational cost during the low activity phase by detaching the code from running containers and it improves the scalability during the high activity phase by quickly assigning the already available containers from the container pool. However, this all comes with the price of reduced QoS for initial requests and our experimental results reflect the extent of this quality degradation.},
  doi       = {10.1109/HPCC/SmartCity/DSS.2018.00175},
  keywords  = {cloud computing;emergency management;information systems;quality of service;pay-per-use policy;Amazon;DMIS;operational cost reduction;scalability;QoS;quality degradation;Lambda based serverless cloud services;disaster management information systems;Containers;Quality of service;Cloud computing;Virtualization;Scalability;Color;Degradation;FaaS;Serverless;Lambda;QoS;DMIS;Disaster},
}

@InProceedings{7833001,
  author    = {I. {Baldini} and P. {Castro} and P. {Cheng} and S. {Fink} and V. {Ishakian} and N. {Mitchell} and V. {Muthusamy} and R. {Rabbah} and P. {Suter}},
  title     = {Cloud-Native, Event-Based Programming for Mobile Applications},
  booktitle = {2016 IEEE/ACM International Conference on Mobile Software Engineering and Systems (MOBILESoft)},
  year      = {2016},
  pages     = {287-288},
  month     = {May},
  abstract  = {Creating mobile applications often requires both client and server- side code development, each requiring vastly differentskills. Recently, cloud providers like Amazon and Google introduced "server-less" programming models that abstract away many infrastructure concerns and allow developers to focus on their application logic. In this demonstration, we introduce OpenWhisk, our system for constructing cloud native actions, within the context of mobile application development process. We demonstrate how OpenWhisk is used in mobile application development, allows cloud API customizations for mobile, and simplifies mobile application architectures.},
  doi       = {10.1109/MobileSoft.2016.063},
  keywords  = {application program interfaces;cloud computing;mobile computing;software engineering;cloud-native programming;event-based programming;mobile applications;client-side code development;server-side code development;cloud providers;Amazon;Google;serverless programming models;OpenWhisk;cloud API customizations;Mobile communication;Programming;Speech;Mobile applications;Application programming interfaces;Cloud computing;Mobile development;server-less programming models},
}

@InProceedings{8513710,
  author    = {D. {Bardsley} and L. {Ryan} and J. {Howard}},
  title     = {Serverless Performance and Optimization Strategies},
  booktitle = {2018 IEEE International Conference on Smart Cloud (SmartCloud)},
  year      = {2018},
  pages     = {19-26},
  month     = {Sep.},
  abstract  = {In the constantly changing technological landscape the concept of serverless computing in a public Cloud is a relatively new development. Over recent years the serverless abstraction has gained significant traction within the IT industry. Google, Microsoft and AWS all now provide feature equivalent serverless implementations as part of their Cloud-based offerings and solution architects throughout the industry are utilizing serverless as part of mission critical enterprise systems. Throughout this paper we examine the performance profile of the serverless ecosystem in a low latency, high availability context, present results on the integral performance of such systems and outline some practical mitigation strategies to optimize serverless architectures. We confine our investigation specifically to one aspect of the AWS implementation of serverless; known as AWS Lambda. Our results show there are opportunities to tune the performance characteristics of Lambda-based architectures and we outline considerations such as cold starts and potential latency characteristics created by a combination of factors including external systems and events. We propose a diverse set of strategies, approaches and techniques which, when successfully implemented and deployed, simultaneously play to its strengths with the ultimate goal of providing a set of design patterns aimed at increasing the capability of serverless computing to a wider set of problem domains.},
  doi       = {10.1109/SmartCloud.2018.00012},
  keywords  = {cloud computing;object-oriented programming;open systems;software architecture;software performance evaluation;Web services;Lambda-based architectures;serverless computing;public Cloud;IT industry;mission critical enterprise systems;serverless ecosystem;serverless architectures;AWS Lambda;optimization strategies;Computer architecture;Logic gates;Servers;Time factors;Ecosystems;Monitoring;Computational modeling;serverless;aws;lambdas;architecture;performance},
}

@Article{8405632,
  author   = {K. {Bhatia}},
  title    = {Nate Taggart on Serverless},
  journal  = {IEEE Software},
  year     = {2018},
  volume   = {35},
  number   = {4},
  pages    = {101-104},
  month    = {July},
  issn     = {0740-7459},
  abstract = {In this excerpt from Software Engineering Radio, Nate Taggart, cofounder and CEO of Stackery, discusses serverless—the ability to purchase function as a service in which the cloud provider assumes responsibility for providing a server and an execution environment on demand to run a piece of code. To hear the full interview, visit www.se-radio.net or access our archives via RSS at feeds.feedburner.com/se-radio.},
  doi      = {10.1109/MS.2018.2801544},
  keywords = {Software engineering;Computational modeling;FAA;Nate Taggart;serverless;Software Engineering Radio;software engineering;software development},
}

@InProceedings{7979854,
  author    = {N. {Bila} and P. {Dettori} and A. {Kanso} and Y. {Watanabe} and A. {Youssef}},
  title     = {Leveraging the Serverless Architecture for Securing Linux Containers},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems Workshops (ICDCSW)},
  year      = {2017},
  pages     = {401-404},
  month     = {June},
  abstract  = {Linux containers present a lightweight solution to package applications into images and instantiate them in isolated environments. Such images may include vulnerabilities that can be exploited at runtime. A vulnerability scanning service can detect these vulnerabilities by periodically scanning the containers and their images for potential threats. When a threat is detected, an event may be generated to (1) quarantine or terminate the compromised container(s) and optionally (2) remedy the vulnerability by rebuilding a secure image. We believe that such event-driven process is a great fit to be implemented in a serverless architecture. In this paper we explore the design of an automated threat mitigation architecture based on OpenWhisk and Kubernetes.},
  doi       = {10.1109/ICDCSW.2017.66},
  issn      = {2332-5666},
  keywords  = {Linux;serverless architecture;Linux containers;vulnerability scanning service;secure image;automated threat mitigation architecture;Kubernetes;OpenWhisk;Containers;Security;Computer architecture;Engines;Linux;Tools;Runtime;Linux containers;serverless architecture;Kubernetes;OpenWhisk;Docker;security analysis},
}

@InProceedings{8247460,
  author    = {E. F. {Boza} and C. L. {Abad} and M. {Villavicencio} and S. {Quimba} and J. A. {Plaza}},
  title     = {Reserved, on demand or serverless: Model-based simulations for cloud budget planning},
  booktitle = {2017 IEEE Second Ecuador Technical Chapters Meeting (ETCM)},
  year      = {2017},
  pages     = {1-6},
  month     = {Oct},
  abstract  = {Cloud computing providers offer a variety of pricing models, complicating the client decision, as no single model is the cheapest in all scenarios. In addition, small to medium-sized organizations frequently lack personnel that can navigate the intricacies of each pricing model, and as a result, end up opting for a sub-optimal strategy, leading to overpaying for computing resources or not being able to meet performance goals. In this paper, we: (1) present the results of a study that shows that, in Ecuador, a considerable percentage of companies choose conservative pricing strategies, (2) present a case study that shows that the conservative pricing strategy is suboptimal under certain workloads, and (3) propose a set of models, a tool and a process that can be used by tenants to properly plan and budget their cloud computing costs. Our tool is based on M (t)/M/* queuing theory models and is easy to configure and use. Note that, even though we are motivated by our study of adoption of cloud computing technologies in Ecuador, our tool and process are widely applicable and not restricted to the Ecuadorian context.},
  doi       = {10.1109/ETCM.2017.8247460},
  keywords  = {cloud computing;decision making;optimisation;pricing;queueing theory;securities trading;cloud budget planning;cloud computing providers;pricing model;client decision;conservative pricing strategy;cloud computing costs;suboptimal strategy;model-based simulations;small to medium-sized organizations;M (t)/M/* queuing theory models;Cloud computing;Pricing;Computational modeling;Tools;Companies;Electronic mail;Cloud;reserved;on-demand;serverless;budget;simulation;queuing theory},
}

@InProceedings{8588721,
  author    = {M. {Branowski} and A. {Belloum}},
  title     = {Cookery: A Framework for Creating Data Processing Pipeline Using Online Services},
  booktitle = {2018 IEEE 14th International Conference on e-Science (e-Science)},
  year      = {2018},
  pages     = {368-369},
  month     = {Oct},
  abstract  = {With the increasing amount of data the importance of data analysis has grown. A large amount of this data has shifted to cloud-based storage. The cloud offers storage and computation power. The Cookery framework is a tool developed to build application in the cloud for scientists without a complete understanding of programming. In this paper with present the cookery systems and how it can be used to authenticate and use standard online 3rd party services to easily create data analytics pipeline. Cookery framework is not limited to work with standard web services, it can also integrate and work with the emerging AWS Lambda. The combination of AWS Lambda and Cookery, which makes it possible for people, who do not have any program experience, to create data processing pipeline using cloud services in short time.},
  doi       = {10.1109/eScience.2018.00102},
  keywords  = {cloud computing;data analysis;Web services;cookery systems;data analytics pipeline;cloud services;AWS Lambda;data processing pipeline;online 3rd party services;standard Web services;Electronic mail;Pipelines;Data analysis;Google;Programming;Market research;Function-as-a-Service (FaaS);AWS Lambda;OAuth;IFTTT;Cloud},
}

@InProceedings{7980271,
  author    = {P. {Castro} and V. {Ishakian} and V. {Muthusamy} and A. {Slominski}},
  title     = {Serverless Programming (Function as a Service)},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)},
  year      = {2017},
  pages     = {2658-2659},
  month     = {June},
  abstract  = {In this tutorial, we will present serverless computing, survey existing serverless platforms from industry, academia, and open source projects, identify key characteristics and use cases, and describe technical challenges and open problems. Our tutorial will involve a hands-on experience of using the serverless technologies available from different cloud providers (e.g. IBM, Amazon, Google and Microsoft). We expect our users to have basic knowledge of programming and basic knowledge of cloud computing.},
  doi       = {10.1109/ICDCS.2017.305},
  issn      = {1063-6927},
  keywords  = {cloud computing;serverless programming;function as a service;serverless computing;serverless technologies;cloud providers;cloud computing;Programming;Cloud computing;Computer architecture;Mobile communication;Conferences;Tutorials;Quality of service},
}

@InProceedings{8103476,
  author    = {K. S. {Chang} and S. J. {Fink}},
  title     = {Visualizing serverless cloud application logs for program understanding},
  booktitle = {2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)},
  year      = {2017},
  pages     = {261-265},
  month     = {Oct},
  abstract  = {A cloud platform records a wealth of information regarding program execution. Most cloud service providers offer dashboard monitoring tools that visualize resource usage and billing information, and support debugging. In this paper, we present a tool that visualizes cloud execution logs for a different goal - to facilitate program understanding and generate documentations for an application using runtime data. Our tool introduces a new timeline visualization, a new method and user interface to summarize multiple JSON objects and present the result, and interaction techniques that facilitate navigating among functions. Together, these features explain a serverless cloud application's composition, performance, dataflow and data schema. We report some initial user feedback from several expert developers that were involved in the tool's design and development process.},
  doi       = {10.1109/VLHCC.2017.8103476},
  issn      = {1943-6106},
  keywords  = {cloud computing;data flow analysis;data visualisation;program debugging;resource allocation;system monitoring;user interfaces;program understanding;runtime data;timeline visualization;user interface;serverless cloud application;cloud service providers;resource usage;billing information;debugging;cloud platform;program execution;dashboard monitoring tools;cloud execution logs;JSON objects;Tools;Cloud computing;Data visualization;Unified modeling language;Bars;Computational modeling;Visualization;serverless computing;function as a service;program understanding;log visualization;cloud computing},
}

@InProceedings{7979852,
  author    = {R. {Chard} and K. {Chard} and J. {Alt} and D. Y. {Parkinson} and S. {Tuecke} and I. {Foster}},
  title     = {Ripple: Home Automation for Research Data Management},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems Workshops (ICDCSW)},
  year      = {2017},
  pages     = {389-394},
  month     = {June},
  abstract  = {Exploding data volumes and acquisition rates, plus ever more complex research processes, place significant strain on research data management processes. It is increasingly common for data to flow through pipelines comprised of dozens of different management, organization, and analysis steps distributed across multiple institutions and storage systems. To alleviate the resulting complexity, we propose a home automation approach to managing data throughout its lifecycle, in which users specify via high-level rules the actions that should be performed on data at different times and locations. To this end, we have developed Ripple, a responsive storage architecture that allows users to express data management tasks via a rules notation. Ripple monitors storage systems for events, evaluates rules, and uses serverless computing techniques to execute actions in response to these events. We evaluate our solution by applying Ripple to the data lifecycles of two real-world projects, in astronomy and light source science, and show that it can automate many mundane and cumbersome data management processes.},
  doi       = {10.1109/ICDCSW.2017.30},
  issn      = {2332-5666},
  keywords  = {astronomy computing;data acquisition;home automation;light sources;storage management;data volumes;data acquisition rates;complex research processes;research data management;home automation;high-level rules;RIPPLE;responsive storage architecture;storage systems;serverless computing;data lifecycles;astronomy;light source science;Monitoring;Telescopes;Observers;Home automation;Light sources;Reliability;Distributed databases;Responsive storage;Serverless;Software defined cyberinfrastructure},
}

@InProceedings{8560183,
  author    = {Y. {Cheng} and Z. {Zhou}},
  title     = {Autonomous Resource Scheduling for Real-Time and Stream Processing},
  booktitle = {2018 IEEE SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computing, Scalable Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
  year      = {2018},
  pages     = {1181-1184},
  month     = {Oct},
  abstract  = {This work proposes the ARS(FaaS) framework, scheduling and provisioning resources for streaming applications autonomously. It ensures real-time response on unpredictable and fluctuating streaming data. We use a HPC Cloud platform as the de facto platform, and explore FaaS for stream processing on it. The major contribution of this work is effective and efficient autonomous resource scheduling for real-time streaming analytic.},
  doi       = {10.1109/SmartWorld.2018.00205},
  keywords  = {cloud computing;parallel processing;resource allocation;scheduling;autonomous resource scheduling;stream processing;provisioning resources;real-time response;unpredictable streaming data;fluctuating streaming data;facto platform;FaaS;HPC cloud platform;autonomous scheduling;steam processing;cloud computing;FaaS},
}

@InProceedings{8389457,
  author    = {S. {Chinchole} and A. {Kulkarni} and L. {Matai} and C. {Kotadiya}},
  title     = {A real-time cloud-based messaging system for delivering medication to the rural areas},
  booktitle = {2017 International Conference on Intelligent Sustainable Systems (ICISS)},
  year      = {2017},
  pages     = {475-479},
  month     = {Dec},
  abstract  = {The rural areas of most of the developing nations suffer from lack of proper schools, poor road infrastructure, scarcity of pharmacies and hospitals and reliable public transport service amongst many other things. These conditions have a major impact on the people residing in such areas in a negative way. The illiterate, visually impaired, handicapped and old aged people suffer the most because they do not have access to quality healthcare in such areas. The system presented in the paper utilizes the mobile internet, serverless technology and cloud computing to serve these people. The system acts as a picture centered real-time messaging system to enable the people to order medication online and get it delivered to their residences.},
  doi       = {10.1109/ISS1.2017.8389457},
  keywords  = {cloud computing;geriatrics;handicapped aids;health care;Internet;medical information systems;mobile computing;telemedicine;real-time cloud;rural areas;developing nations;pharmacies;hospitals;handicapped people;old aged people;serverless technology;real-time messaging system;schools;road infrastructure;public transport service;online medication;mobile Internet;Cloud computing;Real-time systems;Portals;Task analysis;Conferences;Cameras;Medical services;Cloud Computing;Healthcare;Real-Time System;Serverless Architecture},
}

@InProceedings{8590993,
  author    = {C. {Cicconetti} and M. {Conti} and A. {Passarella}},
  title     = {An Architectural Framework for Serverless Edge Computing: Design and Emulation Tools},
  booktitle = {2018 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)},
  year      = {2018},
  pages     = {48-55},
  month     = {Dec},
  abstract  = {We consider a Software Defined Networking (SDN)-enabled edge computing domain, where networking devices also have processing capabilities. In particular, we investigate the problem of dynamic allocation of stateless computations, that we call lambda functions, and propose an architectural framework through which requests for execution of lambda functions originated by mobile nodes can be appropriately routed to specific edge devices following a serverless model. In addition, we propose a detailed emulation environment to test the architecture. Our framework supports many possible distributed algorithms to dynamically adapt the choice where requests should be executed, in order to optimize a given performance target. In the paper we consider a few such policies, to test the flexibility of the architecture. We thus present extensive performance results of the considered policies.},
  doi       = {10.1109/CloudCom2018.2018.00024},
  issn      = {2330-2186},
  keywords  = {distributed algorithms;mobile computing;software defined networking;telecommunication network routing;software defined networking-enabled edge computing domain;distributed algorithms;serverless model;specific edge devices;mobile nodes;lambda functions;stateless computations;dynamic allocation;processing capabilities;networking devices;SDN;serverless edge computing;architectural framework;Edge computing;Computer architecture;Performance evaluation;Emulation;Servers;Optimization;Face;edge computing;software defined networking;serverless computing;computation delegation},
}

@INPROCEEDINGS{8700543,
author={A. {Danayi} and S. {Sharifian}},
booktitle={2018 4th Iranian Conference on Signal Processing and Intelligent Systems (ICSPIS)},
title={PESS-MinA: A Proactive Stochastic Task Allocation Algorithm for FaaS Edge-Cloud environments},
year={2018},
volume={},
number={},
pages={27-31},
abstract={By the advent of FaaS Cloud services and the micro-services programming architecture, designing task allocation algorithms with higher performance has become a crucial task. Motivated by this high-interested challenge, we propose a new allocation algorithm called PESS-MinA based on our novel modular model for FaaS Edge-Cloud environments. In contradiction to widely-used Max-Min and Min-Min algorithms which are both reactive and deterministic, this algorithm is based on stochastic score, and thus provides proactivity considerations. Experiments with Google Cloud Trace dataset show that our algorithm exhibits better performance in both resource load balancing and QoS assurance of FaaS. According to simulations, PESS-MinA decreased the dropped tasks percentage from 2.9% to 0.01%, alongside with a triple balancing score.},
keywords={cloud computing;minimax techniques;quality of service;resource allocation;stochastic processes;PESS-MinA;FaaS Cloud services;max-min algorithms;min-min algorithms;FaaS edge-cloud environments;stochastic task allocation algorithm;microservices programming architecture;Google Cloud Trace dataset;load balancing;QoS assurance;Task analysis;Signal processing algorithms;Cloud computing;FAA;Resource management;Stochastic processes;Containers;Proactive;Task Allocation;FaaS;Edge-Cloud},
doi={10.1109/ICSPIS.2018.8700543},
ISSN={},
month={Dec}
}

@InProceedings{8605776,
  author    = {A. {Das} and S. {Patterson} and M. {Wittie}},
  title     = {EdgeBench: Benchmarking Edge Computing Platforms},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {175-180},
  month     = {Dec},
  abstract  = {The emerging trend of edge computing has led several cloud providers to release their own platforms for performing computation at the 'edge' of the network. We compare two such platforms, Amazon AWS Greengrass and Microsoft Azure IoT Edge, using a new benchmark comprising a suite of performance metrics. We also compare the performance of the edge frameworks to cloud-only implementations available in their respective cloud ecosystems. Amazon AWS Greengrass and Azure IoT Edge use different underlying technologies, edge Lambda functions vs. containers, and so we also elaborate on platform features available to developers. Our study shows that both of these edge platforms provide comparable performance, which nevertheless differs in important ways for key types of workloads used in edge applications. Finally, we discuss several current issues and challenges we faced in deploying these platforms.},
  doi       = {10.1109/UCC-Companion.2018.00053},
  keywords  = {benchmark testing;cloud computing;Internet of Things;performance metrics;edge frameworks;cloud-only implementations;Amazon AWS Greengrass;edge platforms;edge applications;cloud providers;Microsoft Azure IoT Edge;cloud ecosystems;edge computing platform benchmarking;edge Lambda functions;EdgeBench;Cloud computing;Image edge detection;Benchmark testing;Edge computing;Performance evaluation;Pipelines;Internet of Things;Serverless Computing;Cloud Functions},
}

@InProceedings{8077240,
  author    = {S. {Dash} and D. K. {Dash}},
  title     = {Serverless cloud computing framework for smart grid architecture},
  booktitle = {2016 IEEE 7th Power India International Conference (PIICON)},
  year      = {2016},
  pages     = {1-6},
  month     = {Nov},
  abstract  = {Additional digital layer is an important aspect in the Smart grid architecture. We gather the data using this layer and the whole grid is controlled accordingly. Energy cost can be estimated and the demand can be predicted. Estimation of health issues of electrical equipment can be done from the control room by gathering the data from remote locations. In the case of computer-aided digital relay model if we choose the pilot relay action by remote data then that would be the good application of this digital layer. In this paper, we have suggested an alternative method- to gather and analyze the remote data using the serverless cloud computing framework. The prime objective is to design a cheaper and technically easier simulation strategy with this new framework. At the end, we have tried to create a mathematical model to estimate the cost and quality of service. The system is simulated using MATLAB, Amazon Web Service (AWS) products and a batch program.},
  doi       = {10.1109/POWERI.2016.8077240},
  keywords  = {cloud computing;data analysis;demand forecasting;digital simulation;estimation theory;load forecasting;power engineering computing;power system protection;power system relaying;relay protection;smart power grids;Web services;serverless cloud computing framework;smart grid architecture;additional digital layer;control room;remote locations;digital relay model;pilot relay action;energy cost estimation;simulation strategy;demand prediction;electrical equipment health estimation;remote data analysis;MATLAB;Amazon Web service;AWS;Smart grid;Cloud computing;Serverless Cloud computing architecture},
}

@InProceedings{8411080,
  author    = {A. {Deese}},
  title     = {Implementation of Unsupervised k-Means Clustering Algorithm Within Amazon Web Services Lambda},
  booktitle = {2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)},
  year      = {2018},
  pages     = {626-632},
  month     = {May},
  abstract  = {This work demonstrates how an unsupervised learning algorithm based on k-Means Clustering with Kaufman Initialization may be implemented effectively as an Amazon Web Services Lambda Function, within their serverless cloud computing service. It emphasizes the need to employ a lean and modular design philosophy, transfer data efficiently between Lambda and DynamoDB, as well as employ Lambda Functions within mobile applications seamlessly and with negligible latency. This work presents a novel application of serverless cloud computing and provides specific examples that will allow readers to develop similar algorithms. The author provides compares the computation speed and cost of machine learning implementations on traditional PC and mobile hardware (running locally) as well as implementations that employ Lambda.},
  doi       = {10.1109/CCGRID.2018.00093},
  keywords  = {cloud computing;learning (artificial intelligence);mobile computing;pattern clustering;unsupervised learning;Web services;unsupervised k-Means Clustering algorithm;unsupervised learning algorithm;Kaufman Initialization;Amazon Web Services Lambda Function;serverless cloud computing service;lean design philosophy;modular design philosophy;mobile applications;DynamoDB;data transfer;machine learning;Cloud computing;Machine learning;Libraries;Training data;Servers;Task analysis;Pipelines;machine learning;k-Means Clustering;Kaufman;serverless computing;Amazon Web Services Lambda;cloud computing},
}

@Article{7912239,
  author   = {A. {Eivy}},
  title    = {Be Wary of the Economics of "Serverless" Cloud Computing},
  journal  = {IEEE Cloud Computing},
  year     = {2017},
  volume   = {4},
  number   = {2},
  pages    = {6-12},
  month    = {March},
  issn     = {2325-6095},
  abstract = {One of the latest developments in cloud computing is usually called "serverless" computing, despite the fact that servers are still where processing takes place. The economic benefits of serverless computing heavily depend on the execution behavior and volumes of the application workload. Serverless has the potential to be a great abstraction offering economic advantages for simple workflows, however it's important to model the economic impact of your architecture and operation choices.},
  doi      = {10.1109/MCC.2017.32},
  keywords = {cloud computing;socio-economic effects;serverless cloud computing;economic benefits;economic advantages;economic impact;Cloud computing;Economics;Pricing;Servers;Google;Scalability;cloud computing;serverless;cloud economics;pap-per-use resources;frameworks},
}

@InProceedings{8567674,
  author    = {T. {Elgamal}},
  title     = {Costless: Optimizing Cost of Serverless Computing through Function Fusion and Placement},
  booktitle = {2018 IEEE/ACM Symposium on Edge Computing (SEC)},
  year      = {2018},
  pages     = {300-312},
  month     = {Oct},
  abstract  = {Serverless computing has recently experienced significant adoption by several applications, especially Internet of Things (IoT) applications. In serverless computing, rather than deploying and managing dedicated virtual machines, users are able to deploy individual functions, and pay only for the time that their code is actually executing. However, since serverless platforms are relatively new, they have a completely different pricing model that depends on the memory, duration, and the number of executions of a sequence/workflow of functions. In this paper we present an algorithm that optimizes the price of serverless applications in AWS Lambda. We first describe the factors affecting price of serverless applications which include: (1) fusing a sequence of functions, (2) splitting functions across edge and cloud resources, and (3) allocating the memory for each function. We then present an efficient algorithm to explore different function fusion-placement solutions and find the solution that optimizes the application's price while keeping the latency under a certain threshold. Our results on image processing workflows show that the algorithm can find solutions optimizing the price by more than 35%-57% with only 5%-15% increase in latency. We also show that our algorithm can find non-trivial memory configurations that reduce both latency and price.},
  doi       = {10.1109/SEC.2018.00029},
  keywords  = {cloud computing;image fusion;Internet of Things;virtual machines;serverless computing;serverless platforms;serverless applications;Internet of Things;virtual machines;splitting functions;function fusion-placement solutions;AWS Lambda;edge resources;cloud resources;image processing workflows;nontrivial memory configuration;Computational modeling;Fuses;Pricing;Cloud computing;Internet of Things;Memory management;Face;Serverless;Edge computing;AWS Lambda;Cloud Computing;Cost Optimization},
}

@Article{8481652,
  author   = {E. {van Eyk} and L. {Toader} and S. {Talluri} and L. {Versluis} and A. {Uță} and A. {Iosup}},
  title    = {Serverless is More: From PaaS to Present Cloud Computing},
  journal  = {IEEE Internet Computing},
  year     = {2018},
  volume   = {22},
  number   = {5},
  pages    = {8-17},
  month    = {Sep.},
  issn     = {1089-7801},
  abstract = {In the late-1950s, leasing time on an IBM 704 cost hundreds of dollars per minute. Today, cloud computing, that is, using IT as a service, on-demand and pay-per-use, is a widely used computing paradigm that offers large economies of scale. Born from a need to make platform as a service (PaaS) more accessible, fine-grained, and affordable, serverless computing has garnered interest from both industry and academia. This article aims to give an understanding of these early days of serverless computing: what it is, where it comes from, what is the current status of serverless technology, and what are its main obstacles and opportunities.},
  doi      = {10.1109/MIC.2018.053681358},
  keywords = {cloud computing;serverless computing;serverless technology;PaaS;affordable computing;cloud computing;computing paradigm;IT service;Cloud computing;Internet;Economics;Servers;serverless;function-as-a-service;cloud computing;workflows;internet;internet computing},
}

@InProceedings{8549544,
  author    = {M. {Ezzeddine} and R. {Morcel} and H. {Artail} and M. A. R. {Saghir} and H. {Akkary} and H. {Hajj}},
  title     = {RESTful Hardware Microservices Using Reconfigurable Networked Accelerators in Cloud and Edge Datacenters},
  booktitle = {2018 IEEE 7th International Conference on Cloud Networking (CloudNet)},
  year      = {2018},
  pages     = {1-4},
  month     = {Oct},
  abstract  = {We propose enabling cloud datacenters with Reconfigurable Networked Accelerators RNAs. RNAs are FPGA and memory compute nodes connected to the main network of the datacenter. To enable seamless integration of RNAs, we propose RESTful hardware microservices in cloud datacenters. We show how a front-end model view controller (MVC) web application can issue a call to remote RNA-accelerated RESTful microservices to decrease the latency of a single client query and increase the throughput of clients served. As a use case, we investigate just in time classification of client uploaded media (e.g., images, videos, etc.) against adult or hateful content. The system architecture is implemented using Spring MVC (Spring Boot) and AlexNet convolutional neural network CNN for image classification. Observed results show up to more than 10x improvements in throughput and energy efficiency depending on the target RNA (FPGA) device and the level of optimization of the employed hardware classifier.},
  doi       = {10.1109/CloudNet.2018.8549544},
  keywords  = {cloud computing;computer centres;field programmable gate arrays;image classification;neural nets;power aware computing;software architecture;RESTful hardware microservices;cloud datacenters;memory compute nodes;seamless integration;remote RNA-accelerated RESTful microservices;single client query;client uploaded media;target RNA device;employed hardware classifier;front-end model view controller Web application;image classification;reconfigurable networked accelerators;RNA;Acceleration;Cloud computing;Field programmable gate arrays;Servers;Hardware;Ethernet;FPGA;HTTP;REST;model view controller;MVC;AlexNet;convolutional neural network;CNN;Spark;data center;cloud computing;representational state transfer;edge computing;fog computing;serverless;cloud functions;Spring framework},
}

@InProceedings{8457817,
  author    = {L. {Feng} and P. {Kudva} and D. {Da Silva} and J. {Hu}},
  title     = {Exploring Serverless Computing for Neural Network Training},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {334-341},
  month     = {July},
  abstract  = {Serverless or functions as a service runtimes have shown significant benefits to efficiency and cost for event-driven cloud applications. Although serverless runtimes are limited to applications requiring lightweight computation and memory, such as machine learning prediction and inference, they have shown improvements on these applications beyond other cloud runtimes. Training deep learning can be both compute and memory intensive. We investigate the use of serverless runtimes while leveraging data parallelism for large models, show the challenges and limitations due to the tightly coupled nature of such models, and propose modifications to the underlying runtime implementations that would mitigate them. For hyperparameter optimization of smaller deep learning models, we show that serverless runtimes can provide significant benefit.},
  doi       = {10.1109/CLOUD.2018.00049},
  issn      = {2159-6190},
  keywords  = {cloud computing;learning (artificial intelligence);neural nets;serverless computing;neural network training;serverless functions;service runtimes;event-driven cloud applications;serverless runtimes;lightweight computation;machine learning prediction;cloud runtimes;lightweight memory;machine learning inference;deep learning models;hyperparameter optimization;data parallelism;Servers;Training;Merging;Runtime;Neural networks;Data transfer;Machine learning;serverless computing;cloud computing;deep learning;cloud scaling;cloud cost and performance},
}

@INPROCEEDINGS{8730873,
author={H. {Flores} and P. {Nurmi} and P. {Hui}},
booktitle={2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)},
title={AI on the Move: From On-Device to On-Multi-Device},
year={2019},
volume={},
number={},
pages={310-315},
abstract={On-Device AI is an emerging paradigm that aims to make devices more intelligent, autonomous and proactive by equipping them with machine and deep learning routines for robust decision making and optimal execution in devices' operations. On-Device intelligence promises the possibility of computing huge amounts of data close to its source, e.g., sensor and multimedia data. By doing so, devices can complement their counterpart cloud services with more sophisticated functionality to provide better applications and services. However, increased computational capabilities of smart devices, wearables and IoT devices along with the emergence of services at the Edge of the network are driving the trend of migrating and distributing computation between devices. Indeed, devices can reduce the burden of executing resource intensive tasks via collaborations in the wild. While several work has shown the benefits of an opportunistic collaboration of a device with others, not much is known regarding how devices can be organized as a group as they move together. In this paper, we contribute by analyzing how dynamic group organization of devices can be utilized to distribute intelligence on the moving Edge. The key insight is that instead of On-Device solutions complementing with cloud, dynamic groups can be formed to complement each other in an On-Multi-Device manner. Thus, we highlight the challenges and opportunities from extending the scope of On-Device AI from an egocentric view to a collaborative, multi-device view.},
keywords={Artificial intelligence;Cloud computing;Collaboration;Task analysis;Device-to-device communication;Smart devices;Performance evaluation;Cloud;Edge;Cloudlet;Artificial Intelligence;Device-to-Device;Data Analytics;Serverless},
doi={10.1109/PERCOMW.2019.8730873},
ISSN={},
month={March}
}

@InProceedings{8475098,
  author    = {J. {Franz} and T. {Nagasuri} and A. {Wartman} and A. V. {Ventrella} and F. {Esposito}},
  title     = {Reunifying Families after a Disaster via Serverless Computing and Raspberry Pis},
  booktitle = {2018 IEEE International Symposium on Local and Metropolitan Area Networks (LANMAN)},
  year      = {2018},
  pages     = {131-132},
  month     = {June},
  abstract  = {Children constitute a vulnerable population and special considerations are necessary in order to provide proper care for them during disasters. After disasters such as Hurricane Katrina, the rapid identification and protection of separated children and their reunification with legal guardians is necessary to minimize secondary injuries (i:e, physical and sexual abuse, neglect and abduction). At Camp Gruber, an Oklahoma shelter for Louisianan's displaced by Hurricane Katrina, of the 254 children at the camp, 36 ((i.e, 14.2%) were separated from their legal guardians. It took 6 months to reunify the last children; 70% of the children were with their legal guardian after 2 weeks. Imagine not knowing for 2 weeks (or 6 months) if your children are dead or alive. To exacerbate these natural challenges, during a disaster Internet connectivity is scarse or unreliable.},
  doi       = {10.1109/LANMAN.2018.8475098},
  issn      = {1944-0375},
  keywords  = {disasters;emergency management;injuries;Internet;minimisation;paediatrics;storms;disaster Internet connectivity;serverless computing;vulnerable population;Hurricane Katrina;separated children;legal guardian;Camp Gruber;reunifying families;Raspberry Pis;secondary injuries minimization;physical abuse;sexual abuse;Oklahoma shelter;Databases;Law;Face recognition;Training;Hurricanes;Internet},
}

@Article{8125550,
  author   = {D. {Gannon} and R. {Barga} and N. {Sundaresan}},
  title    = {Cloud-Native Applications},
  journal  = {IEEE Cloud Computing},
  year     = {2017},
  volume   = {4},
  number   = {5},
  pages    = {16-21},
  month    = {Sep.},
  issn     = {2325-6095},
  abstract = {Cloud-native is a term that is invoked often but seldom defined beyond saying “we built it in the cloud” as opposed to “on-prem”. However, there is now an emerging consensus around key ideas and informal applications design patterns that have been adopted and used in many successful cloud applications. In this introduction, we will describe these cloud-native concepts and illustrate them with examples. We will also look at the technical trends that may give us an idea about the future of cloud applications. We begin by discussing the basic properties that many cloud-native apps have in common. Once we have characterized them, we can then describe how these properties emerge from the technical design patterns.},
  doi      = {10.1109/MCC.2017.4250939},
  keywords = {cloud computing;cloud-native applications;technical design patterns;cloud-native;distributed computing;microservices;serverless;cloud computing},
}

@InProceedings{8582355,
  author    = {X. {Geng} and O. {Ma} and Y. {Pei} and Z. {Xu} and W. {Zeng} and J. {Zou}},
  title     = {Research on Early Warning System of Power Network Overloading Under Serverless Architecture},
  booktitle = {2018 2nd IEEE Conference on Energy Internet and Energy System Integration (EI2)},
  year      = {2018},
  pages     = {1-6},
  month     = {Oct},
  abstract  = {Under the background of massive computing resources in power grid, smart grid has become the direction and trend of power industry development. In order to ensure the safe and stable operation of smart grid, the monitoring of the grid load should be strengthened and a reasonable solution should be arranged in time when the grid load exceeded the standard. So early warning system of power network overloading under serverless architecture was proposed. The main advantages of the combination of smart grid and serverless architecture were analyzed in detail, and the technical feasibility and economic feasibility of the early warning system for grid overload under the serverless mode were analyzed by examples.},
  doi       = {10.1109/EI2.2018.8582355},
  keywords  = {alarm systems;electricity supply industry;power system measurement;power system stability;smart power grids;economic feasibility;grid load monitoring;power industry development;smart grid;power grid;massive computing resources;power network overloading;serverless mode;grid overload;early warning system;serverless architecture;Cloud computing;Smart grids;FAA;Computer architecture;Monitoring;Alarm systems;serverless architecture;overload warning;smart grid;cloud computing},
}

@InProceedings{8622139,
  author    = {V. {Gupta} and S. {Wang} and T. {Courtade} and K. {Ramchandran}},
  title     = {OverSketch: Approximate Matrix Multiplication for the Cloud},
  booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
  year      = {2018},
  pages     = {298-304},
  month     = {Dec},
  abstract  = {We propose OverSketch, an approximate algorithm for distributed matrix multiplication in serverless computing. OverSketch leverages ideas from matrix sketching and high-performance computing to enable cost-efficient multiplication that is resilient to faults and straggling nodes pervasive in low-cost serverless architectures. We establish statistical guarantees on the accuracy of OverSketch and empirically validate our results by solving a large-scale linear program using interior-point methods and demonstrate a 34% reduction in compute time on AWS Lambda.},
  doi       = {10.1109/BigData.2018.8622139},
  keywords  = {approximation theory;cloud computing;computational complexity;linear programming;mathematics computing;matrix multiplication;matrix sketching;high-performance computing;cost-efficient multiplication;straggling nodes;low-cost serverless architectures;approximate matrix multiplication;approximate algorithm;distributed matrix multiplication;serverless computing;OverSketch;Cloud computing;Bandwidth;Distributed algorithms;Memory management;Partitioning algorithms;serverless computing;straggler mitigation;sketched matrix multiplication},
}

@InProceedings{8576921,
  author    = {E. {Handoyo} and M. {Arfan} and Y. A. A. {Soetrisno} and M. {Somantri} and A. {Sofwan} and E. W. {Sinuraya}},
  title     = {Ticketing Chatbot Service using Serverless NLP Technology},
  booktitle = {2018 5th International Conference on Information Technology, Computer, and Electrical Engineering (ICITACEE)},
  year      = {2018},
  pages     = {325-330},
  month     = {Sep.},
  abstract  = {Personal assistant using a human operator need some time to process single request such as ticket booking, ordering something, and get services. One request can contain many queries for some information provided on the internet. Business performance values time efficiency so must be considered an alternative way to take request. Chatbot can give 24 hours service which can become an advantage besides using a human personal assistant. Chatbot acts like routing agent that can classify user context in conversation. Chatbot helped with natural language processing (NLP) to analyze the request and extract some keyword information. One important process in NLP is morphological analysis and part of speech (POS) tagging. POS help to parse the meaning of chat text based on a set of rules. The rule base is specific to some language and designed to capture all the keyword relies on chat text. Keyword in booking conversation term is like departure and destination city and also the date of flight. There is a variation from a user determining city and date. NLP in booking confirmation has a task to analyze various pattern describing ordering requests like city and date. Messenger bot would be an example of assistance that can help user connected to many services some like ticketing service through conversation interaction. The contribution of this research is to conduct some scenario that happening in ordering tickets. This research conduct that chatbot can help acts as customer service, based on the conducted scenario and show an F-measure score of 89.65%.},
  doi       = {10.1109/ICITACEE.2018.8576921},
  keywords  = {customer services;Internet;natural language processing;query processing;reservation computer systems;text analysis;ticketing service;customer service;ticketing chatbot service;serverless NLP technology;human operator;ticket booking;human personal assistant;part of speech tagging;POS tagging;Internet;NLP;keyword information extraction;chat text;natural language processing;Facebook;Electrical engineering;Urban areas;Natural language processing;Programming;Task analysis;Testing;chatbot;routing agent;conversation;NLP;interaction;intent},
}

@Article{8344774,
  author   = {L. F. {Herrera-Quintero} and J. C. {Vega-Alfonso} and K. B. A. {Banse} and E. {Carrillo Zambrano}},
  title    = {Smart ITS Sensor for the Transportation Planning Based on IoT Approaches Using Serverless and Microservices Architecture},
  journal  = {IEEE Intelligent Transportation Systems Magazine},
  year     = {2018},
  volume   = {10},
  number   = {2},
  pages    = {17-27},
  month    = {Summer},
  issn     = {1939-1390},
  abstract = {Currently, there are many challenges in the transportation scope that researchers are attempting to resolve, and one of them is transportation planning. The main contribution of this paper is the design and implementation of an ITS (Intelligent Transportation Systems) smart sensor prototype that incorporates and combines the Internet of Things (IoT) approaches using the Serverless and Microservice Architecture, to help the transportation planning for Bus Rapid Transit (BRT) systems. The ITS smart sensor prototype can detect several Bluetooth signals of several devices (e.g., from mobile phones) that people use while travelling by the BRT system (e.g., in Bogota city). From that information, the ITS smart-sensor prototype can create an O/D (origin/destiny) matrix for several BRT routes, and this information can be used by the Administrator Authorities (AA) to produce a suitable transportation planning for the BRT systems. In addition, this information can be used by the center of traffic management and the AA from ITS cloud services using the Serverless and Microservice architecture.},
  doi      = {10.1109/MITS.2018.2806620},
  keywords = {Bluetooth;intelligent sensors;intelligent transportation systems;Internet of Things;mobile radio;planning;rapid transit systems;road vehicles;traffic engineering computing;transportation;transportation planning;ITS smart sensor prototype;Intelligent Transportation Systems;BRT system;smart-sensor prototype;smart ITS sensor;IoT approaches;microservices architecture;bus rapid transit systems;Internet of Things;serverless architecture;Bluetooth signals;origin-destiny matrix;traffic management;Transportation;Planning;Cloud computing;Bluetooth;Urban areas;Computer architecture;Internet of Things;Intelligent sensors;Intelligent transportation systems},
}

@InProceedings{8025307,
  author    = {M. {HoseinyFarahabady} and J. {Taheri} and Z. {Tari} and A. Y. {Zomaya}},
  title     = {A Dynamic Resource Controller for a Lambda Architecture},
  booktitle = {2017 46th International Conference on Parallel Processing (ICPP)},
  year      = {2017},
  pages     = {332-341},
  month     = {Aug},
  abstract  = {Lambda architecture is a novel event-driven serverless paradigm that allows companies to build scalable and reliable enterprise applications. As an attractive alternative to traditional service oriented architecture (SOA), Lambda architecture can be used in many use cases including BI tools, in-memory graph databases, OLAP, and streaming data processing. In practice, an important aim of Lambda's service providers is devising an efficient way to co-locate multiple Lambda functions with different attributes into a set of available computing resources. However, previous studies showed that consolidated workloads can compete fiercely for shared resources, resulting in severe performance variability/degradation. This paper proposes a resource allocation mechanism for a Lambda platform based on the model predictive control framework. Performance evaluation is carried out by comparing the proposed solution with multiple resource allocation heuristics, namely enhanced versions of spread and binpack, and best-effort approaches. Results confirm that the proposed controller increases the overall resource utilization by 37% on average and achieves a significant improvement in preventing QoS violation incidents compared to others.},
  doi       = {10.1109/ICPP.2017.42},
  issn      = {2332-5690},
  keywords  = {bin packing;business data processing;data handling;performance evaluation;predictive control;resource allocation;software architecture;performance variability;QoS violation prevention;resource utilization;spread approach;binpack approach;best-effort approach;performance evaluation;model predictive control;resource allocation;performance degradation;shared resources;computing resources;Lambda functions;Lambda service providers;enterprise applications;event-driven serverless paradigm;Lambda architecture;dynamic resource controller;Quality of service;Resource management;Sensitivity;Memory management;Interference;Servers;Dynamic Resource Allocation;Performance degradation;Lambda Platform Processing;Shared Resource Interference},
}

@Article{8126823,
  author   = {M. R. {HoseinyFarahabady} and A. Y. {Zomaya} and Z. {Tari}},
  title    = {A Model Predictive Controller for Managing QoS Enforcements and Microarchitecture-Level Interferences in a Lambda Platform},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  year     = {2018},
  volume   = {29},
  number   = {7},
  pages    = {1442-1455},
  month    = {July},
  issn     = {1045-9219},
  abstract = {Lambda paradigm, also known as Function as a Service (FaaS), is a novel event-driven concept that allows companies to build scalable and reliable enterprise applications in an off-premise computing data-center as a serverless solution. In practice, however, an important goal for the service provider of a Lambda platform is to devise an efficient way to consolidate multiple Lambda functions in a single host. While the majority of existing resource management solutions use only operating-system level metrics (e.g., average utilization of computing and I/O resources) to allocate the available resources among the submitted workloads in a balanced way, a resource allocation schema that is oblivious to the issue of shared-resource contention can result in a significant performance variability and degradation within the entire platform. This paper proposes a predictive controller scheme that dynamically allocates resources in a Lambda platform. This scheme uses a prediction tool to estimate the future rate of every event stream and takes into account the quality of service enforcements requested by the owner of each Lambda function. This is formulated as an optimization problem where a set of cost functions are introduced (i) to reduce the total QoS violation incidents; (ii) to keep the CPU utilization level within an accepted range; and (iii) to avoid the fierce contention among collocated applications for obtaining shared resources. Performance evaluation is carried out by comparing the proposed solution with an enhanced interference-aware version of three well-known heuristics, namely spread, binpack (the two native clustering solutions employed by Docker Swarm) and best-effort resource allocation schema. Experimental results show that the proposed controller improves the overall performance (in terms of reducing the end-to-end response time) by 14.9 percent on average compared to the best result of the other heuristics. The proposed solution also increases the overall CPU utilization by 18 percent on average (for lightweight workloads), while achieves an average 87 percent (maximum 146 percent) improvement in preventing QoS violation incidents.},
  doi      = {10.1109/TPDS.2017.2779502},
  keywords = {application program interfaces;cloud computing;computer centres;optimal control;optimisation;predictive control;quality of service;resource allocation;software engineering;virtualisation;model predictive controller;QoS enforcements;microarchitecture-level interferences;Lambda platform;Lambda paradigm;reliable enterprise applications;serverless solution;multiple Lambda functions;resource management solutions;operating-system level metrics;resource allocation schema;shared-resource contention;function as a service;quality of service;total QoS violation incident reduction;CPU utilization;enhanced interference-awareness;Docker Swarm;Resource management;Quality of service;Interference;Bandwidth;Measurement;Servers;Cloud computing;Serverless lambda platform;function as a service (FaaS);model predictive control;dynamic resource allocation/scheduling;performance degradation},
}

@InProceedings{8360337,
  author    = {V. {Ishakian} and V. {Muthusamy} and A. {Slominski}},
  title     = {Serving Deep Learning Models in a Serverless Platform},
  booktitle = {2018 IEEE International Conference on Cloud Engineering (IC2E)},
  year      = {2018},
  pages     = {257-262},
  month     = {April},
  abstract  = {Serverless computing has emerged as a compelling paradigm for the development and deployment of a wide range of event based cloud applications. At the same time, cloud providers and enterprise companies are heavily adopting machine learning and Artificial Intelligence to either differentiate themselves, or provide their customers with value added services. In this work we evaluate the suitability of a serverless computing environment for the inferencing of large neural network models. Our experimental evaluations are executed on the AWS Lambda environment using the MxNet deep learning framework. Our experimental results show that while the inferencing latency can be within an acceptable range, longer delays due to cold starts can skew the latency distribution and hence risk violating more stringent SLAs.},
  doi       = {10.1109/IC2E.2018.00052},
  keywords  = {cloud computing;learning (artificial intelligence);neural nets;serverless platform;event based cloud applications;cloud providers;enterprise companies;machine learning;value added services;serverless computing environment;neural network models;AWS Lambda environment;MxNet deep learning framework;artificial intelligence;latency distribution;SLA;Machine learning;Cloud computing;Neural networks;Containers;Computational modeling;Delays;Training;AWS Lambda;Cloud Computing;Serverless Computing;Deep Learningm},
}

@InProceedings{8605773,
  author    = {D. {Jackson} and G. {Clynch}},
  title     = {An Investigation of the Impact of Language Runtime on the Performance and Cost of Serverless Functions},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {154-160},
  month     = {Dec},
  abstract  = {Serverless, otherwise known as "Function-as-a-Service" (FaaS), is a compelling evolution of cloud computing that is highly scalable and event-driven. Serverless applications are composed of multiple independent functions, each of which can be implemented in a range of programming languages. This paper seeks to understand the impact of the choice of language runtime on the performance and subsequent cost of serverless function execution. It presents the design and implementation of a new serverless performance testing framework created to analyse performance and cost metrics for both AWS Lambda and Azure Functions. For optimum performance and cost management of serverless applications, Python is the clear choice on AWS Lambda. C# .NET is the top performer and most economical option for Azure Functions. NodeJS on Azure Functions and .NET Core 2 on AWS should be avoided or at the very least, used carefully in order to avoid their potentially slow and costly start-up times.},
  doi       = {10.1109/UCC-Companion.2018.00050},
  keywords  = {cloud computing;program testing;programming languages;serverless functions;event-driven system;NodeJS;Azure Functions;AWS Lambda;serverless performance testing framework;serverless function execution;programming languages;cloud computing;compelling evolution;Function-as-a-Service;language runtime;Runtime;Measurement;Testing;Containers;Java;C# languages;Cloud computing;serverless;FaaS;Lambda;aws;azure;functions;performance;cloud},
}

@InProceedings{8551035,
  author    = {B. {Jambunathan} and K. {Yoganathan}},
  title     = {Architecture Decision on using Microservices or Serverless Functions with Containers},
  booktitle = {2018 International Conference on Current Trends towards Converging Technologies (ICCTCT)},
  year      = {2018},
  pages     = {1-7},
  month     = {March},
  abstract  = {Cloud adoption is gaining lots of momentum across the globe and enterprise are focussing not only migration on to cloud but also on developing cloud native application. There are lots of focuses on reducing and optimizing resources and hence developing application in a serverless fashion is going to be the key in the industry. Many organizations are working on application modernization and developing distributed application and hence microservice is the key focus area in converting their monolithic into microservices and use containers for easy portability across the platform and makes it more platform neutral. There are high elements of focus on whether to go for serverless or Microservice mode and should we use containers for deployment is the key debate among the people who are working in this area and are still not clear which way to go forward in the given situation. In this article we would like to explore and discuss about each technology in details and analyse the advantage and challenge of these emerging areas and suggest the ideal way to move forward in cloud.},
  doi       = {10.1109/ICCTCT.2018.8551035},
  keywords  = {cloud computing;distributed processing;platform neutral;microservice;use containers;architecture decision;serverless functions;cloud adoption;cloud native application;reducing optimizing resources;serverless fashion;application modernization;distributed application;Containers;Servers;Cloud computing;Computer architecture;FAA;Buildings;Containers;dockers;Microservices;Serverless;Functions;Lambda;Kubernetes},
}

@InProceedings{8436772,
  author    = {M. {Keltsch} and S. {Prokesch} and O. P. {Gordo} and J. {Serrano} and T. K. {Phan} and I. {Fritzsch}},
  title     = {Remote Production and Mobile Contribution Over 5G Networks: Scenarios, Requirements and Approaches for Broadcast Quality Media Streaming},
  booktitle = {2018 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)},
  year      = {2018},
  pages     = {1-7},
  month     = {June},
  abstract  = {Media applications are amongst the most demanding services requiring high amounts of network capacity as well as extremely low latency for synchronous audio-visual streaming in production quality. Recent technological advances in the 5G domain hold the promise to unlock the potential of the media industry by offering high quality media services through dynamic efficient resource allocation. Actual implementations are now required to validate whether advanced media applications can be realised benefiting from ultra-low latency, very-high bandwidth and flexible dynamic configuration offered by these new 5G networks. A truly integrated approach is needed that focuses on the media applications not only on the management of generic network functions and the orchestration of resources at the various radio, fronthaul/backhaul, edge and core network segments. The H2020 5G PPP Phase 2 project 5G-MEDIA [1] leverages new options for more flexible, ad-hoc and cost-effective production workflows by replacing dedicated lines and hardware equipment with software functions (VNFs) facilitating (semi-) automated smart production in remote locations. Highly scalable virtualized media services deployed on or close to the edge reduce complexity for the user, ensure operational reliability and increase the Quality of Experience (QoE). Virtual compression engines have the potential to replace dedicated encoder/decoder hardware while the network optimisation (Cognitive Network Optimizer) in combination with the Quality of Service (QoS) monitoring helps to overcome the current internet best-effort principle and ensures that the required performance needs are met at all times.},
  doi       = {10.1109/BMSB.2018.8436772},
  issn      = {2155-5052},
  keywords  = {5G mobile communication;decoding;Internet;media streaming;mobile radio;quality of service;resource allocation;telecommunication traffic;virtualisation;media industry;H2020 5G PPP Phase 2 project 5G-MEDIA;resource allocation;audio-visual streaming;network capacity;broadcast Quality media streaming;5G networks;Cognitive Network Optimizer;network optimisation;Production;Streaming media;Media;5G mobile communication;Bandwidth;Cameras;TV;Remote production;smart production;mobile contribution;broadcasting;SDN;NFV;edge computing;NFVI;VNF;FaaS},
}

@InProceedings{8457833,
  author    = {J. {Kijak} and P. {Martyna} and M. {Pawlik} and B. {Balis} and M. {Malawski}},
  title     = {Challenges for Scheduling Scientific Workflows on Cloud Functions},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {460-467},
  month     = {July},
  abstract  = {Serverless computing, also known as Function-as-a-Service (FaaS) or Cloud Functions, is a new method of running distributed applications by executing functions on the infrastructure of cloud providers. Although it frees the developers from managing servers, there are still decisions to be made regarding selection of function configurations based on the desired performance and cost. The billing model of this approach considers time of execution, measured in 100ms units, as well as the size of the memory allocated per function. In this paper, we look into the problem of scheduling scientific workflows, which are applications consisting of multiple tasks connected into a dependency graph. We discuss challenges related to workflow scheduling and propose the Serverless Deadline-Budget Workflow Scheduling (SDBWS) algorithm adapted to serverless platforms. We present preliminary experiments with a small-scale Montage workflow run on the AWS Lambda infrastructure.},
  doi       = {10.1109/CLOUD.2018.00065},
  issn      = {2159-6190},
  keywords  = {cloud computing;graph theory;scheduling;scientific information systems;workflow management software;Function-as-a-Service;cloud providers;Serverless Deadline-Budget Workflow Scheduling;small-scale Montage workflow;scientific workflow scheduling;serverless computing;cloud functions;dependency graph;AWS Lambda infrastructure;Task analysis;Cloud computing;FAA;Processor scheduling;Computational modeling;Adaptation models;Engines;Faas;serverless computing;cloud functions;scientific workflow;task scheduling},
}

@InProceedings{8374513,
  author    = {J. {Kim} and T. J. {Jun} and D. {Kang} and D. {Kim} and D. {Kim}},
  title     = {GPU Enabled Serverless Computing Framework},
  booktitle = {2018 26th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)},
  year      = {2018},
  pages     = {533-540},
  month     = {March},
  abstract  = {A new form of cloud computing, serverless computing, is drawing attention as a new way to design micro-services architectures. In a serverless computing environment, services are developed as service functional units. The function development environment of all serverless computing framework at present is CPU based. In this paper, we propose a GPU-supported serverless computing framework that can deploy services faster than existing serverless computing framework using CPU. Our core approach is to integrate the open source serverless computing framework with NVIDIA-Docker and deploy services based on the GPU support container. We have developed an API that connects the open source framework to the NVIDIA-Docker and commands that enable GPU programming. In our experiments, we measured the performance of the framework in various environments. As a result, developers who want to develop services through the framework can deploy high-performance micro services and developers who want to run deep learning programs without a GPU environment can run code on remote GPUs with little performance degradation.},
  doi       = {10.1109/PDP2018.2018.00090},
  issn      = {2377-5750},
  keywords  = {application program interfaces;cloud computing;graphics processing units;learning (artificial intelligence);public domain software;service-oriented architecture;GPU enabled serverless computing framework;cloud computing;microservices architectures;serverless computing environment;service functional units;function development environment;open source serverless computing framework;deploy services;open source framework;high-performance microservices;NVIDIA-Docker;GPU support container;GPU programming;deep learning programs;performance degradation;Graphics processing units;Servers;Cloud computing;Containers;Computer architecture;Libraries;Standards;Cloud Computing;Serverless Computing;Serverless Architecture;FaaS;GPGPU},
}

@InProceedings{8567385,
  author    = {Y. {Kim} and G. {Cha}},
  title     = {Design of the Cost Effective Execution Worker Scheduling Algorithm for FaaS Platform Using Two-Step Allocation and Dynamic Scaling},
  booktitle = {2018 IEEE 8th International Symposium on Cloud and Service Computing (SC2)},
  year      = {2018},
  pages     = {131-134},
  month     = {Nov},
  abstract  = {Function as a Service(FaaS) has been widely prevalent in the cloud computing area with the evolution of the cloud computing paradigm and the growing demand for event-based computing models. We have analyzed the preparation load required for the actual execution of a function, from assignment of a function execution walker to loading a function on the FaaS platform, by testing the execution of a dummy function on a simple FaaS prototype. According to the analysis results, we found that the cost of first worker allocation requires 1,850ms even though the lightweight container is used, and then the worker re-allocation cost require 470ms at the same node. The result shows that the function service is not enough to be used as a high efficiency processing calculation platform. We propose a new worker scheduling algorithm to appropriately distribute the worker's preparation load related to execution of functions so that FaaS platform is suitable for high efficiency computing environment. Proposed algorithm is to distribute the worker 's allocation tasks in two steps before the request occurs, and predict the number of workers required to be allocated in advance. When applying the proposed worker scheduling algorithm in FaaS platform under development, we estimate that worker allocation request can be processed with an allocation cost of less than 3% compared to the FaaS prototype. Therefore, it is expected that the functional service will become a high efficiency computing platform through the significant improvement of the worker allocation cost.},
  doi       = {10.1109/SC2.2018.00027},
  keywords  = {cloud computing;resource allocation;scheduling;cost effective execution worker scheduling algorithm;FaaS platform;two-step allocation;cloud computing area;event-based computing models;worker re-allocation cost;high efficiency processing calculation platform;high efficiency computing environment;worker allocation request;high efficiency computing platform;worker allocation cost;Function as a Service;dynamic scaling;Resource management;FAA;Cloud computing;Containers;Dynamic scheduling;Scheduling algorithms;Scheduling, Dynamic scaling, Function, Function as a Service},
}

@InProceedings{8457831,
  author    = {Y. {Kim} and J. {Lin}},
  title     = {Serverless Data Analytics with Flint},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {451-455},
  month     = {July},
  abstract  = {Serverless architectures organized around loosely-coupled function invocations represent an emerging design for many applications. Recent work mostly focuses on user-facing products and event-driven processing pipelines. In this paper, we explore a completely different part of the application space and examine the feasibility of analytical processing on big data using a serverless architecture. We present Flint, a prototype Spark execution engine that takes advantage of AWS Lambda to provide a pure pay-as-you-go cost model. With Flint, a developer uses PySpark exactly as before, but without needing an actual Spark cluster. We describe the design, implementation, and performance of Flint, along with the challenges associated with serverless analytics.},
  doi       = {10.1109/CLOUD.2018.00063},
  issn      = {2159-6190},
  keywords  = {Big Data;cloud computing;data analysis;serverless data analytics;Flint;serverless architecture;loosely-coupled function invocations;analytical processing;big data;prototype Spark execution engine;pure pay-as-you-go cost model;actual Spark cluster;serverless analytics;AWS Lambda;PySpark;Sparks;Task analysis;Cloud computing;Engines;Standards;Metadata;Data analysis;serverless computing, cloud computing, data analytics, data science},
}

@InProceedings{8514884,
  author    = {Y. K. {Kim} and M. R. {HoseinyFarahabady} and Y. C. {Lee} and A. Y. {Zomaya} and R. {Jurdak}},
  title     = {Dynamic Control of CPU Usage in a Lambda Platform},
  booktitle = {2018 IEEE International Conference on Cluster Computing (CLUSTER)},
  year      = {2018},
  pages     = {234-244},
  month     = {Sep.},
  abstract  = {Lambda platform is a new concept based on an event-driven server-less computation that empowers application developers to build scalable enterprise software in a virtualized environment without provisioning or managing any physical servers (a server-less solution). In reality, however, devising an effective consolidation method to host multiple Lambda functions into a single machine is challenging. The existing simple resource allocation algorithms, such as the round-robin policy used in many commercial server-less systems, suffer from lack of responsiveness to a sudden surge in the incoming workload. This will result in an unsatisfactory performance degradation that is directly experienced by the end-user of a Lambda application. In this paper, we address the problem of CPU cap management in a Lambda platform for ensuring different QoS enforcement levels in a platform with shared resources, in case of fluctuations and sudden surges in the incoming workload requests. To this end, we present a closed-loop (feedback-based) CPU cap controller, which fulfills the QoS levels enforced by the application owners. The controller adjusts the number of working threads per QoS class and dispatches the outstanding Lambda functions along with the associated events to the most appropriate working thread. The proposed solution reduces the QoS violations by an average of 6.36 times compared to the round-robin policy. It can also maintain the end-to-end response time of applications belonging to the highest priority QoS class close to the target set-point while decreasing the overall response time by up to 52%.},
  doi       = {10.1109/CLUSTER.2018.00041},
  issn      = {2168-9253},
  keywords  = {closed loop systems;microprocessor chips;quality of service;resource allocation;software engineering;virtualisation;simple resource allocation algorithms;QoS enforcement levels;highest priority QoS class close;end-to-end response time;outstanding Lambda functions;closed-loop CPU cap controller;CPU cap management;Lambda application;round-robin policy;multiple Lambda functions;scalable enterprise software;event-driven server-less computation;Lambda platform;CPU usage;dynamic control;Quality of service;Instruction sets;Delays;Time factors;Surges;Resource management;Server-less computing, FaaS (Function as a Service), QoS-aware resource manager, Dynamic CPU usage control},
}

@InProceedings{8515004,
  author    = {V. {Koumaras} and A. {Foteas} and A. {Foteas} and M. {Kapari} and C. {Sakkas} and H. {Koumaras}},
  title     = {5G Performance Testing of Mobile Chatbot Applications},
  booktitle = {2018 IEEE 23rd International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD)},
  year      = {2018},
  pages     = {1-6},
  month     = {Sep.},
  abstract  = {A Chatbot is an application that is designed to provide automated contextual communication. Today most chatbots are implemented on top of or as a gateway to popular messaging services, such as Facebook Messenger, Skype and Viber. Chatbots can be classified into many categories regarding their usage, such as conversational commerce, customer support, education, marketing and others. Due to their agile deployment ability on top of virtualized and serverless environments, chatbots are expected to play a pivotal role in the forthcoming 5G networks, which support virtualization capabilities at the edge of the network, making feasible the provision of diversified chatbot services customized to each user needs and requests. However, chatbot QoS might be affected under congested network conditions or in areas with poor signal reception quality. Currently, the performance of the chatbot has not been researched, while the users are experiencing only the results of the potential QoS degradation, such as loss or re-ordering of messages. This paper provides an experimental study of the chatbot apps performance/QoS under different network and reception conditions. The experiment was conducted using the 5G mobile network emulation testbed created and provided by the EU-funded TRIANGLE project.},
  doi       = {10.1109/CAMAD.2018.8515004},
  issn      = {2378-4873},
  keywords  = {5G mobile communication;electronic messaging;mobile computing;mobile radio;quality of service;social networking (online);mobile Chatbot applications;5G mobile network emulation;congested network conditions;chatbot QoS;diversified chatbot services;automated contextual communication;Testing;Quality of service;5G mobile communication;Keyboards;Servers;Quality of experience;Mobile handsets;Chatbot;QoS;QoE;5G;Benchmarking},
}

@InProceedings{8605774,
  author    = {K. {Kritikos} and P. {Skrzypek}},
  title     = {A Review of Serverless Frameworks},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {161-168},
  month     = {Dec},
  abstract  = {Serverless computing is a new computing paradigm that promises to revolutionize the way applications are built and provisioned. In this computing kind, small pieces of software called functions are deployed in the cloud with zero administration and minimal costs for the software developer. Further, this computing kind has various applications in areas like image processing and scientific computing. Due to the above advantages, the current uptake of serverless computing is being addressed by traditional big cloud providers like Amazon, who offer serverless platforms for serverless application deployment and provisioning. However, as in the case of cloud computing, such providers attempt to lock-in their customers with the supply of complementary services which provide added-value support to serverless applications. To this end, to resolve this issue, serverless frameworks have been recently developed. Such frameworks either abstract away from serverless platform specificities, or they enable the production of a mini serverless platform on top of existing clouds. However, these frameworks differ in various features that do have an impact on the serverless application lifecycle. To this end, to assist the developers in selecting the most suitable framework, this paper attempts to review these frameworks according to a certain set of criteria that directly map to the application lifecycle. Further, based on the review results, some remaining challenges are supplied, which when confronted will make serverless frameworks highly usable and suitable for the handling of both serverless as well as mixed application kinds.},
  doi       = {10.1109/UCC-Companion.2018.00051},
  keywords  = {cloud computing;formal specification;serverless computing;serverless application deployment;cloud computing;serverless platform specificities;mini serverless platform;functions;big cloud providers;Cloud computing;Software;Testing;Computer languages;Runtime;Monitoring;Production;serverless, function-as-a-service, abstraction, provisioning, framework, review},
}

@InProceedings{8605778,
  author    = {J. {Kuhlenkamp} and S. {Werner}},
  title     = {Benchmarking FaaS Platforms: Call for Community Participation},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {189-194},
  month     = {Dec},
  abstract  = {The number of available FaaS platforms increases with the rising popularity of a "serverless" architecture and development paradigm. As a consequence, a high demand for benchmarking FaaS platforms exists. In response to this demand, new benchmarking approaches that focus on different objectives continuously emerge. In this paper, we call for community participation to conduct a collaborative systematic literature review with the goal to establish a community-driven knowledge base.},
  doi       = {10.1109/UCC-Companion.2018.00055},
  keywords  = {cloud computing;knowledge based systems;serverless architecture;development paradigm;community participation;FaaS platforms;collaborative systematic literature review;community-driven knowledge base;FAA;Task analysis;Benchmark testing;Data mining;Search problems;Computer architecture;Bibliographies;FaaS;serverless;benchmarking;secondary study;SLR},
}

@InProceedings{7774691,
  author    = {E. d. {Lara} and C. S. {Gomes} and S. {Langridge} and S. H. {Mortazavi} and M. {Roodi}},
  title     = {Poster Abstract: Hierarchical Serverless Computing for the Mobile Edge},
  booktitle = {2016 IEEE/ACM Symposium on Edge Computing (SEC)},
  year      = {2016},
  pages     = {109-110},
  month     = {Oct},
  abstract  = {EdgeScale is a new platform that leverages serveless cloud computing to enable storage and processing on a hierarchy of data centers positioned over the geographic span of a network between the end-user device and the traditional wide-area cloud datacenter. EdgeScale applications are structured as lightweight stateless handlers that can be rapidely instantiated on demand. EdgeScale provides a scalable and persistent storage service that automatically migrates application state across the data center hierarchy to optimize access latency and reduce bandwidth consumption.},
  doi       = {10.1109/SEC.2016.37},
  keywords  = {cloud computing;computer centres;mobile computing;storage management;hierarchical serverless cloud computing;mobile edge;data storage;data processing;geographic span;end-user device;EdgeScale applications;lightweight stateless handlers;storage service;data center hierarchy;access latency optimization;bandwidth consumption;Cloud computing;Bandwidth;Servers;Hardware;Mobile communication;Computer architecture;serverless computing;edge computing;cloudlets},
}

@InProceedings{8457830,
  author    = {H. {Lee} and K. {Satyam} and G. {Fox}},
  title     = {Evaluation of Production Serverless Computing Environments},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {442-450},
  month     = {July},
  abstract  = {Serverless computing provides a small runtime container to execute lines of codes without infrastructure management which is similar to Platform as a Service (PaaS) but a functional level. Amazon started the event-driven compute named Lambda functions in 2014 with a 25 concurrent limitation, but it now supports at least a thousand of concurrent invocation to process event messages generated by resources like databases, storage and system logs. Other providers, i.e., Google, Microsoft, and IBM offer a dynamic scaling manager to handle parallel requests of stateless functions in which additional containers are provisioning on new compute nodes for distribution. However, while functions are often developed for microservices and lightweight workload, they are associated with distributed data processing using the concurrent invocations. We claim that the current serverless computing environments can support dynamic applications in parallel when a partitioned task is executable on a small function instance. We present results of throughput, network bandwidth, a file I/O and compute performance regarding the concurrent invocations. We deployed a series of functions for distributed data processing to address the elasticity and then demonstrated the differences between serverless computing and virtual machines for cost efficiency and resource utilization.},
  doi       = {10.1109/CLOUD.2018.00062},
  issn      = {2159-6190},
  keywords  = {cloud computing;virtual machines;distributed data processing;stateless functions;parallel requests;dynamic scaling manager;concurrent invocation;Lambda functions;event-driven compute;infrastructure management;production serverless computing environments;Google;Throughput;Cloud computing;Databases;Containers;Runtime;Data processing;FaaS, Serverless, Event-driven Computing, Amazon Lambda, Google Functions, Microsoft Azure Functions, IBM OpenWhisk},
}

@InProceedings{8457807,
  author    = {W. {Lin} and C. {Krintz} and R. {Wolski}},
  title     = {Tracing Function Dependencies across Clouds},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {253-260},
  month     = {July},
  abstract  = {In this paper, we present Lowgo, a crosscloud tracing tool for capturing causal relationships in serverless applications. To do so, Lowgo records dependencies between functions, through cloud services, and across regions to facilitate debugging and reasoning about highly concurrent, multi-cloud applications. We empirically evaluate Lowgo using microbenchmarks and multi-function and multi-cloud applications. We find that Lowgo is able to capture causal dependencies with overhead that ranges from 2-12%, which is less than half that of the best-performing, cloud-specific approach.},
  doi       = {10.1109/CLOUD.2018.00039},
  issn      = {2159-6190},
  keywords  = {causality;cloud computing;program debugging;function dependencies;clouds;crosscloud tracing tool;causal relationships;serverless applications;Lowgo records dependencies;cloud services;multicloud applications;microbenchmarks;causal dependencies;cloud-specific approach;Cloud computing;Tools;Pipelines;Servers;Google;Debugging;Computational modeling;cloud computing;serverless computing;function as a service;faas;AWS Lambda;Azure Functions},
}

@InProceedings{8360312,
  author    = {W. {Lin} and C. {Krintz} and R. {Wolski} and M. {Zhang} and X. {Cai} and T. {Li} and W. {Xu}},
  title     = {Tracking Causal Order in AWS Lambda Applications},
  booktitle = {2018 IEEE International Conference on Cloud Engineering (IC2E)},
  year      = {2018},
  pages     = {50-60},
  month     = {April},
  abstract  = {Serverless computing is a new cloud programming and deployment paradigm that is receiving wide-spread uptake. Serverless offerings such as Amazon Web Services (AWS) Lambda, Google Functions, and Azure Functions automatically execute simple functions uploaded by developers, in response to cloud-based event triggers. The serverless abstraction greatly simplifies integration of concurrency and parallelism into cloud applications, and enables deployment of scalable distributed systems and services at very low cost. Although a significant first step, the serverless abstraction requires tools that software engineers can use to reason about, debug, and optimize their increasingly complex, asynchronous applications. Toward this end, we investigate the design and implementation of GammaRay, a cloud service that extracts causal dependencies across functions and through cloud services, without programmer intervention. We implement GammaRay for AWS Lambda and evaluate the overheads that it introduces for serverless micro-benchmarks and applications written in Python.},
  doi       = {10.1109/IC2E.2018.00027},
  keywords  = {cloud computing;Web services;cloud applications;scalable distributed systems;serverless abstraction;increasingly complex applications;asynchronous applications;cloud service;AWS Lambda applications;serverless computing;cloud programming;wide-spread uptake;serverless offerings;Amazon Web Services Lambda;Google Functions;Azure Functions;simple functions;cloud-based event triggers;concurrency;parallelism;causal dependencies extraction;causal order tracking;GammaRay;Cloud computing;Containers;X-ray imaging;Tools;Monitoring;Concurrent computing;serverless;faas;causal dependencies;AWS Lambda;profiling},
}

@InProceedings{8622861,
  author    = {W. {Ling} and C. {Tian} and L. {Ma} and Z. {Hu}},
  title     = {Lite-Service: A Framework to Build and Schedule Telecom Applications in Device, Edge and Cloud},
  booktitle = {2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
  year      = {2018},
  pages     = {708-717},
  month     = {June},
  abstract  = {In cloud, many different service frameworks with new concepts such as Function-as-a-Service (FaaS), Serverless, and Stateless emerge recently. They are essentially certain instantiations of Micro-Service architecture, in which, a software application is implemented as a set of "loosely-coupled services" [32]. From design perspective, these services are considered as the most fine-grained units implementing business logics. Existing frameworks mostly focus on deployment and management of Micro-Services in cloud. However, the internal management within Micro-Service at thread level and function level does not attract enough attention. These missing pieces are critical for telecom applications, in term of programming model, scalability and performance. In this paper, we present Lite-Service, a new framework addressing device, edge and cloud operation environments in telecom industry. It adopts FaaS, Serverless and Stateless programming models to separate telecom logics from runtimes in order to improve development efficiency and reduce operation cost. The Lite-Service framework features 2 additional runtimes: function-level runtime for complex telecom function scheduling; thread-level intra-service runtime for fast response to load change. An autonomous self-adaptive scheduler and decentralized service management schemes are proposed to enhance performance and service adaptability in different telecom deployment scenarios. Empirical results show that the Lite-Service framework is effective for building and scheduling telecom applications in device, edge and cloud.},
  doi       = {10.1109/HPCC/SmartCity/DSS.2018.00123},
  keywords  = {cloud computing;service-oriented architecture;telecommunication computing;software application;fine-grained units;internal management;telecom industry;telecom logics;function-level runtime;complex telecom function scheduling;thread-level intra-service runtime;service management schemes;Function-as-a-Service;lite-service framework;telecom applications;microservice architecture;self-adaptive scheduler;business logics;loosely-coupled services;FaaS;Telecommunications;Cloud computing;FAA;Runtime;Software;Programming;Computational modeling;Serverless, FaaS, Stateless, Autonomous},
}

@InProceedings{8360324,
  author    = {W. {Lloyd} and S. {Ramesh} and S. {Chinthalapati} and L. {Ly} and S. {Pallickara}},
  title     = {Serverless Computing: An Investigation of Factors Influencing Microservice Performance},
  booktitle = {2018 IEEE International Conference on Cloud Engineering (IC2E)},
  year      = {2018},
  pages     = {159-169},
  month     = {April},
  abstract  = {Serverless computing platforms provide function(s)-as-a-Service (FaaS) to end users while promising reduced hosting costs, high availability, fault tolerance, and dynamic elasticity for hosting individual functions known as microservices. Serverless Computing environments, unlike Infrastructure-as-a-Service (IaaS) cloud platforms, abstract infrastructure management including creation of virtual machines (VMs), operating system containers, and request load balancing from users. To conserve cloud server capacity and energy, cloud providers allow hosting infrastructure to go COLD, deprovisioning containers when service demand is low freeing infrastructure to be harnessed by others. In this paper, we present results from our comprehensive investigation into the factors which influence microservice performance afforded by serverless computing. We examine hosting implications related to infrastructure elasticity, load balancing, provisioning variation, infrastructure retention, and memory reservation size. We identify four states of serverless infrastructure including: provider cold, VM cold, container cold, and warm and demonstrate how microservice performance varies up to 15x based on these states.},
  doi       = {10.1109/IC2E.2018.00039},
  keywords  = {cloud computing;resource allocation;virtual machines;serverless computing platforms;hosting costs;fault tolerance;dynamic elasticity;individual functions;microservices;virtual machines;operating system containers;cloud server capacity;cloud providers;service demand;influence microservice performance;infrastructure elasticity;load balancing;infrastructure retention;serverless infrastructure including;infrastructure-as-a-service;infrastructure management;serverless computing environments;FaaS;IaaS;VM;Containers;Cloud computing;Load management;Elasticity;Servers;Operating systems;Fault tolerance;Resource Management and Performance;Serverless Computing;Function-as-a-Service;Provisioning Variation},
}

@InProceedings{8605779,
  author    = {W. {Lloyd} and M. {Vu} and B. {Zhang} and O. {David} and G. {Leavesley}},
  title     = {Improving Application Migration to Serverless Computing Platforms: Latency Mitigation with Keep-Alive Workloads},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {195-200},
  month     = {Dec},
  abstract  = {Serverless computing platforms provide Function(s)-as-a-Service (FaaS) to end users while promising reduced hosting costs, high availability, fault tolerance, and dynamic elasticity for hosting individual functions known as microservices. Serverless Computing environments abstract infrastructure management including creation of virtual machines (VMs), containers, and load balancing from users. To conserve cloud server capacity and energy, cloud providers allow serverless computing infrastructure to go COLD, deprovisioning hosting infrastructure when demand falls, freeing capacity to be harnessed by others. In this paper, we present on a case study migration of the Precipitation Runoff Modeling System (PRMS), a Java-based environmental modeling application to the AWS Lambda serverless platform. We investigate performance and cost implications of memory reservation size, and evaluate scaling performance for increasing concurrent workloads. We then investigate the use of Keep-Alive workloads to preserve serverless infrastructure to minimize cold starts and ensure fast performance after idle periods for up to 100 concurrent client requests. We show how Keep-Alive workloads can be generated using cloud-based scheduled event triggers, enabling minimization of costs, to provide VM-like performance for applications hosted on serverless platforms for a fraction of the cost.},
  doi       = {10.1109/UCC-Companion.2018.00056},
  keywords  = {cloud computing;Java;resource allocation;scheduling;serverless computing infrastructure;Java-based environmental modeling application;concurrent workloads;serverless infrastructure;serverless platforms;application migration;serverless computing platforms;load balancing;cloud server capacity;cloud providers;infrastructure management;function-as-a-service;serverless computing environments;keep-alive workloads;infrastructure management;precipitation runoff modeling system;AWS lambda serverless platform;cloud-based scheduled event triggers;Cloud computing;FAA;Containers;Computational modeling;Java;Load management;Load modeling;Resource Management and Performance;Serverless Computing;Function-as-a-Service;Application Migration},
}

@InProceedings{8605772,
  author    = {P. {García López} and M. {Sánchez-Artigas} and G. {París} and D. {Barcelona Pons} and Á. {Ruiz Ollobarren} and D. {Arroyo Pinto}},
  title     = {Comparison of FaaS Orchestration Systems},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {148-153},
  month     = {Dec},
  abstract  = {Since the appearance of Amazon Lambda in 2014, all major cloud providers have embraced the "Function as a Service" (FaaS) model, because of its enormous potential for a wide variety of applications. As expected (and also desired), the competition is fierce in the serverless world, and includes aspects such as the run-time support for the orchestration of serverless functions. In this regard, the three major production services are currently Amazon Step Functions (December 2016), Azure Durable Functions (June 2017), and IBM Composer (October 2017), still young and experimental projects with a long way ahead. In this article, we will compare and analyze these three serverless orchestration systems under a common evaluation framework. We will study their architectures, programming and billing models, and their effective support for parallel execution, among others. Through a series of experiments, we will also evaluate the run-time overhead of the different infrastructures for different types of workflows.},
  doi       = {10.1109/UCC-Companion.2018.00049},
  keywords  = {cloud computing;IBM Composer;serverless orchestration systems;billing models;FaaS orchestration systems;Amazon Lambda;cloud providers;Function as a Service model;serverless functions;Amazon Step Functions;Azure Durable Functions;Programming;Software;Packaging;Computer architecture;FAA;Measurement;DSL;Cloud computing;Serverless;Function Composition;Orchestration;Amazon Step Functions;Azure Durable Functions;IBM Composer},
}

@InProceedings{8241104,
  author    = {T. {Lynn} and P. {Rosati} and A. {Lejeune} and V. {Emeakaroha}},
  title     = {A Preliminary Review of Enterprise Serverless Cloud Computing (Function-as-a-Service) Platforms},
  booktitle = {2017 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)},
  year      = {2017},
  pages     = {162-169},
  month     = {Dec},
  abstract  = {In line with cloud computing emergence as the dominant enterprise computing paradigm, our conceptualization of the cloud computing reference architecture and service construction has also evolved. For example, to address the need for cost reduction and rapid provisioning, virtualization has moved beyond hardware to containers. More recently, serverless computing or Function-as-a-Service has been presented as a means to introduce further cost-efficiencies, reduce configuration and management overheads, and rapidly increase an application's ability to speed up, scale up and scale down in the cloud. The potential of this new computation model is reflected in the introduction of serverless computing platforms by the main hyperscale cloud service providers. This paper provides an overview and multi-level feature analysis of seven enterprise serverless computing platforms. It reviews extant research on these platforms and identifies the emergence of AWS Lambda as a de facto base platform for research on enterprise serverless cloud computing. The paper concludes with a summary of avenues for further research.},
  doi       = {10.1109/CloudCom.2017.15},
  issn      = {2330-2186},
  keywords  = {business data processing;cloud computing;service-oriented architecture;virtualisation;Function-as-a-Service;dominant enterprise computing paradigm;computation model;main hyperscale cloud service providers;enterprise serverless cloud computing platforms;cost-efficiencies;configuration overheads;management overheads;multilevel feature analysis;AWS Lambda;Cloud computing;Google;Servers;Real-time systems;Virtualization;Computational modeling;Serverless Computing;Function-as-a-Service;FAAS;AWS Lambda;Google Cloud Functions;Azure Functions;IBM OpenWhisk;Iron.io;Auth0 Webtask;Gestal Laser},
}

@InProceedings{8605777,
  author    = {J. {Manner} and M. {Endreß} and T. {Heckel} and G. {Wirtz}},
  title     = {Cold Start Influencing Factors in Function as a Service},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {181-188},
  month     = {Dec},
  abstract  = {Function as a Service (FaaS) is a young and rapidly evolving cloud paradigm. Due to its hardware abstraction, inherent virtualization problems come into play and need an assessment from the FaaS point of view. Especially avoidance of idling and scaling on demand cause a lot of container starts and as a consequence a lot of cold starts for FaaS users. The aim of this paper is to address the cold start problem in a benchmark and investigate influential factors on the duration of the perceived cold start. We conducted a benchmark on AWS Lambda and Microsoft Azure Functions with 49500 cloud function executions. Formulated as hypotheses, the influence of the chosen programming language, platform, memory size for the cloud function, and size of the deployed artifact are the dimensions of our benchmark. Cold starts on the platform as well as the cold starts for users were measured and compared to each other. Our results show that there is an enormous difference for the overhead the user perceives compared to the billed duration. In our benchmark, the average cold start overheads on the user's side ranged from 300ms to 24s for the chosen configurations.},
  doi       = {10.1109/UCC-Companion.2018.00054},
  keywords  = {cloud computing;virtualisation;container starts;cold start problem;Microsoft Azure Functions;virtualization problems;cloud function executions;Function as a Service;AWS Lambda;FAA;Containers;Benchmark testing;Cloud computing;Java;Pipelines;Serverless Computing, Function as a Service, FaaS, Cloud Functions, Cold Start, Benchmarking},
}

@InProceedings{7979855,
  author    = {G. {McGrath} and P. R. {Brenner}},
  title     = {Serverless Computing: Design, Implementation, and Performance},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems Workshops (ICDCSW)},
  year      = {2017},
  pages     = {405-410},
  month     = {June},
  abstract  = {We present the design of a novel performance-oriented serverless computing platform implemented in. NET, deployed in Microsoft Azure, and utilizing Windows containers as function execution environments. Implementation challenges such as function scaling and container discovery, lifecycle, and reuse are discussed in detail. We propose metrics to evaluate the execution performance of serverless platforms and conduct tests on our prototype as well as AWS Lambda, Azure Functions, Google Cloud Functions, and IBM's deployment of Apache OpenWhisk. Our measurements show the prototype achieving greater throughput than other platforms at most concurrency levels, and we examine the scaling and instance expiration trends in the implementations. Additionally, we discuss the gaps and limitations in our current design, propose possible solutions, and highlight future research.},
  doi       = {10.1109/ICDCSW.2017.36},
  issn      = {2332-5666},
  keywords  = {cloud computing;concurrency (computers);Microsoft Windows (operating systems);performance evaluation;performance-oriented serverless computing;.NET;Microsoft Azure;Windows containers;function execution environments;execution performance evaluation;AWS Lambda;Azure Functions;Google Cloud Functions;IBM's deployment;Apache OpenWhisk;concurrency levels;Containers;Web services;Metadata;Resource management;Prototypes;Runtime;Google;serverless computing;serverless performance;FaaS;Function-as-a-Service;AWS Lambda;Azure Functions;Google Cloud Functions;Apache OpenWhisk;IBM OpenWhisk},
}

@InProceedings{7820297,
  author    = {G. {McGrath} and J. {Short} and S. {Ennis} and B. {Judson} and P. {Brenner}},
  title     = {Cloud Event Programming Paradigms: Applications and Analysis},
  booktitle = {2016 IEEE 9th International Conference on Cloud Computing (CLOUD)},
  year      = {2016},
  pages     = {400-406},
  month     = {June},
  abstract  = {Rapid expansion in cloud event technologies such as Amazon Web Service's Lambda, IBM Bluemix's OpenWhisk, Google Cloud Platform's Cloud Functions, and Microsoft Azure's Functions motivates study of software development in these services and their potential as a disruptive force in commercial cloud technologies. In addition to discussing the current state of cloud event services, this paper presents two real world applications utilizing these platforms: Lambdefy, a library designed to make traditional web application run effectively in AWS Lambda, and a performant media management service designed by Trek10, capable of resizing thousands of images per second. Furthermore, we discuss how cloud event technologies enable and/or limit these applications, motivate new software design paradigms in a cloud event environment, and highlight compelling use case scenarios and barriers to entry for cloud event services. AWS cloud technologies are exclusively used due to their maturity and the recent release of the other platforms, while Node.js and the Serverless Framework are utilized for deployment and application development.},
  doi       = {10.1109/CLOUD.2016.0060},
  issn      = {2159-6190},
  keywords  = {cloud computing;Web services;cloud event programming paradigm;Lambda Amazon Web service;OpenWhisk IBM Bluemix;Cloud Functions Google cloud platform;Microsoft Azure Functions;software development;cloud event services;Lambdefy platform;Web application;AWS Lambda;performant media management service;Trek10;AWS cloud technologies;Node.js;serverless framework;Cloud computing;Logic gates;Containers;Google;Hardware;Programming;cloud events;AWS;Lambda;Serverless;micro-services;containers;Google Cloud Platform;Cloud Functions;Microsoft Azure;IBM Bluemix;OpenWhisk},
}

@InProceedings{7980047,
  author    = {A. {Mehta} and R. {Baddour} and F. {Svensson} and H. {Gustafsson} and E. {Elmroth}},
  title     = {Calvin Constrained — A Framework for IoT Applications in Heterogeneous Environments},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)},
  year      = {2017},
  pages     = {1063-1073},
  month     = {June},
  abstract  = {Calvin is an IoT framework for application development, deployment and execution in heterogeneous environments, that includes clouds, edge resources, and embedded or constrained resources. Inside Calvin, all the distributed resources are viewed as one environment by the application. The framework provides multi-tenancy and simplifies development of IoT applications, which are represented using a dataflow of application components (named actors) and their communication. The idea behind Calvin poses similarity with the serverless architecture and can be seen as Actor as a Service instead of Function as a Service. This makes Calvin very powerful as it does not only scale actors quickly but also provides an easy actor migration capability. In this work, we propose Calvin Constrained, an extension to the Calvin framework to cover resource-constrained devices. Due to limited memory and processing power of embedded devices, the constrained side of the framework can only support a limited subset of the Calvin features. The current implementation of Calvin Constrained supports actors implemented in C as well as Python, where the support for Python actors is enabled by using MicroPython as a statically allocated library, by this we enable the automatic management of state variables and enhance code re-usability. As would be expected, Python-coded actors demand more resources over C-coded ones. We show that the extra resources needed are manageable on current off-the-shelve micro-controller-equipped devices when using the Calvin framework.},
  doi       = {10.1109/ICDCS.2017.181},
  issn      = {1063-6927},
  keywords  = {cloud computing;Internet of Things;IoT applications;heterogeneous environments;application development;distributed resources;application components;actor as a service;Calvin constrained;embedded devices;resource-constrained devices;MicroPython;statically allocated library;automatic management;state variables;code reusability enhancement;Python-coded actors;off-the-shelve microcontroller-equipped devices;Runtime;Temperature measurement;Frequency measurement;Sensors;Cloud computing;Actuators;Ports (Computers);IoT;Distributed Cloud;Serverless Architecture;Dataflow Application Development Model},
}

@InProceedings{8605775,
  author    = {P. {Moczurad} and M. {Malawski}},
  title     = {Visual-Textual Framework for Serverless Computation: A Luna Language Approach},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {169-174},
  month     = {Dec},
  abstract  = {As serverless technologies are emerging as a breakthrough in the cloud computing industry, the lack of proper tooling is becoming apparent. The model of computation that the serverless is imposing is as flexible as it is hard to manage and grasp. We present a novel approach towards serverless computing that tightly integrates it with the visual-textual, functional programming language: Luna. This way we are hoping to achieve the clarity and cognitive ease of visual solutions while retaining the flexibility and expressive power of textual programming languages. We created a proof of concept of the Luna Serverless Framework in which we extend the Luna standard library and we leverage the language features to create an intuitive API for serverless function calls using AWS Lambda and to call external functions implemented in JavaScript.},
  doi       = {10.1109/UCC-Companion.2018.00052},
  keywords  = {application program interfaces;cloud computing;data visualisation;functional programming;Java;visual programming;Luna language approach;serverless technologies;cloud computing industry;serverless computing;visual-textual programming language;functional programming language;textual programming languages;Luna standard library;serverless function;visual-textual framework;Luna serverless framework;AWS Lambda;JavaScript;Visualization;Cloud computing;Computer languages;Computational modeling;Market research;Programming;Semantics;serverless;functional programming;visual programming},
}

@InProceedings{8591002,
  author    = {S. K. {Mohanty} and G. {Premsankar} and M. {di Francesco}},
  title     = {An Evaluation of Open Source Serverless Computing Frameworks},
  booktitle = {2018 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)},
  year      = {2018},
  pages     = {115-120},
  month     = {Dec},
  abstract  = {Recent advancements in virtualization and software architecture have led to the new paradigm of serverless computing, which allows developers to deploy applications as stateless functions without worrying about the underlying infrastructure. Accordingly, a serverless platform handles the lifecycle, execution and scaling of the actual functions; these need to run only when invoked or triggered by an event. Thus, the major benefits of serverless computing are low operational concerns and efficient resource management and utilization. Serverless computing is currently offered by several public cloud service providers. However, there are certain limitations on the public cloud platforms, such as vendor lock-in and restrictions on the computation of the functions. Open source serverless frameworks are a promising solution to avoid these limitations and bring the power of serverless computing to on-premise deployments. However, these frameworks have not been evaluated before. Thus, we carry out a comprehensive feature comparison of popular open source serverless computing frameworks. We then evaluate the performance of selected frameworks: Fission, Kubeless and OpenFaaS. Specifically, we characterize the response time and ratio of successfully received responses under different loads and provide insights into the design choices of each framework.},
  doi       = {10.1109/CloudCom2018.2018.00033},
  issn      = {2330-2186},
  keywords  = {cloud computing;public domain software;software architecture;virtualisation;open source serverless computing frameworks;public cloud service providers;software architecture;resource management;virtualization;Fission;Kubeless;OpenFaaS;Time factors;Cloud computing;Containers;Concurrent computing;FAA;Python;Measurement;serverless computing;function-as-a-service;Kubeless;Fission;OpenFaas;performance evaluation},
}

@Article{7994559,
  author   = {S. {Nastic} and T. {Rausch} and O. {Scekic} and S. {Dustdar} and M. {Gusev} and B. {Koteska} and M. {Kostoska} and B. {Jakimovski} and S. {Ristov} and R. {Prodan}},
  title    = {A Serverless Real-Time Data Analytics Platform for Edge Computing},
  journal  = {IEEE Internet Computing},
  year     = {2017},
  volume   = {21},
  number   = {4},
  pages    = {64-71},
  issn     = {1089-7801},
  abstract = {Contemporary solutions for cloud-supported, edge-data analytics mostly apply analytics techniques in a rigid bottom-up approach, regardless of the data's origin. Typically, data are generated at the edge of the infrastructure and transmitted to the cloud, where traditional data analytics techniques are applied. Currently, developers are forced to resort to ad hoc solutions specifically tailored for the available infrastructure (for example, edge devices) when designing, developing, and operating the data analytics applications. Here, a novel approach implements cloud-supported, real-time data analytics in edge-computing applications. The authors introduce their serverless edge-data analytics platform and application model and discuss their main design requirements and challenges, based on real-life healthcare use case scenarios.},
  doi      = {10.1109/MIC.2017.2911430},
  keywords = {cloud computing;data analysis;serverless real-time data analytics;edge computing;cloud supported;edge data analytics;Cloud computing;Data analysis;Analytical models;Real-time systems;Quality of service;Data models;Computational modeling;Internet/Web technologies;real-time data analysis;cloud computing;IoT;edge computing;Internet of Things;security and privacy},
}

@InProceedings{7979853,
  author    = {E. {Oakes} and L. {Yang} and K. {Houck} and T. {Harter} and A. C. {Arpaci-Dusseau} and R. H. {Arpaci-Dusseau}},
  title     = {Pipsqueak: Lean Lambdas with Large Libraries},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems Workshops (ICDCSW)},
  year      = {2017},
  pages     = {395-400},
  month     = {June},
  abstract  = {Microservices are usually fast to deploy because each microservice is small, and thus each can be installed and started quickly. Unfortunately, lean microservices that depend on large libraries will start slowly and harm elasticity. In this paper, we explore the challenges of lean microservices that rely on large libraries in the context of Python packages and the OpenLambda serverless computing platform. We analyze the package types and compressibility of libraries distributed via the Python Package Index and propose PipBench, a new tool for evaluating package support. We also propose Pipsqueak, a package-aware compute platform based on OpenLambda.},
  doi       = {10.1109/ICDCSW.2017.32},
  issn      = {2332-5666},
  keywords  = {programming languages;software libraries;large libraries;Python packages;OpenLambda serverless computing platform;Python package index;cloud computing;Libraries;Containers;Virtual machining;Tools;Cloud computing;Memory management;Linux;serverless computing;cloud computing;distributed computing;distributed systems;python;distributed cache;software repository},
}

@InProceedings{8544423,
  author    = {K. J. P. G. {Perera} and I. {Perera}},
  title     = {A Rule-based System for Automated Generation of Serverless-Microservices Architecture},
  booktitle = {2018 IEEE International Systems Engineering Symposium (ISSE)},
  year      = {2018},
  pages     = {1-8},
  month     = {Oct},
  abstract  = {Software being ubiquitous in today's systems and business operations, it's highly important to structure the high-level architecture of a software application accordingly to deliver the expected customer requirements while accounting for quality measures such as scalability, high availability and high performance. We propose The Architect, a rule-based system for serverless-microservices based high-level architecture generation. In the process of auto generating serverless-microservices high-level architecture, TheArchitect will preserve the highlighted quality measures. It will also provide a tool based support for the high-level architecture designing process of the software architect. Any software developer will be able to use TheArchitect to generate a proper architecture minimizing the involvement of a software architect. Furthermore, the positives of microservices and serverless technologies have made a significant impact on the software engineering community in terms of shifting from the era of building large monolith applications containing overly complex designs, to microservices and serverless based technologies. Hence The Architect focuses on generating best fitted microservices and serverless based high-level architecture for a given application.},
  doi       = {10.1109/SysEng.2018.8544423},
  keywords  = {knowledge based systems;software architecture;software prototyping;rule-based system;software architect;software developer;software engineering community;serverless based technologies;automated generation;business operations;software application;architect;customer requirements;serverless-microservices high-level architecture generation;quality measures;high-level architecture designing process;microservices based technologies;Computer architecture;Software;Knowledge based systems;Calculators;Software architecture;Architecture;Generators;Software Architecture;Microservices Architecture;Serverless Architecture;Rule-based Systems;Domain Specific Software Architecture},
}

@InProceedings{8466390,
  author    = {K. J. P. G. {Perera} and I. {Perera}},
  title     = {TheArchitect: A Serverless-Microservices Based High-level Architecture Generation Tool},
  booktitle = {2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS)},
  year      = {2018},
  pages     = {204-210},
  month     = {June},
  abstract  = {Software is ubiquitous in today's systems and business operations. Most importantly the architecture of a software system determines its quality and longevity, because the development work related to the software system will be carried out to be in line with its architecture design. Hence, it's highly important to structure the high-level software architecture accordingly to deliver the expected customer requirements while accounting for quality measures such as scalability, high availability and high performance. We propose TheArchitect, a serverless-microservices based high-level architecture generation tool, which will auto generate serverless-microservices based high-level architecture for a given business application, preserving the highlighted quality measures providing a tool based support for the software architect with respect to designing the high-level architecture. TheArchitect will provide any software developer to generate a proper architecture minimizing the involvement of an experienced software architect. Furthermore, the positives that microservices and serverless technologies has brought to the world of software engineering has made the software engineering community shift from the era of building large monolith applications containing overly complex designs, to microservices and serverless based technologies. Hence TheArchitect focuses on generating best fitted microservices and serverless based high-level architecture for a given application.},
  doi       = {10.1109/ICIS.2018.8466390},
  keywords  = {software architecture;business operations;software system;high-level software architecture;software developer;software engineering community shift;serverless based technologies;TheArchitect tool;serverless-microservices based high-level architecture generation tool;Computer architecture;Software;Calculators;Data models;Databases;Tools;Architecture;Software Architecture;Microservices Architecture;Serverless Architecture},
}

@InProceedings{8588841,
  author    = {D. {Pinto} and J. P. {Dias} and H. {Sereno Ferreira}},
  title     = {Dynamic Allocation of Serverless Functions in IoT Environments},
  booktitle = {2018 IEEE 16th International Conference on Embedded and Ubiquitous Computing (EUC)},
  year      = {2018},
  pages     = {1-8},
  month     = {Oct},
  abstract  = {The IoT area has grown significantly in the last few years and is expected to reach a gigantic amount of 50 billion devices by 2020. The appearance of serverless architectures, specifically highlighting FaaS, raises the question of the suitability of using them in IoT environments. Combining IoT with a serverless architectural design can effective when trying to make use of local processing power that exists in a local network of IoT devices and creating a fog layer that leverages computational capabilities that are closer to the end-user. In this approach, which is placed between the device and the serverless function, when a device requests for the execution of a serverless function will decide based on previous metrics of execution if the serverless function should be executed locally, in the fog layer of a local network of IoT devices, or if it should be executed remotely, in one of the available cloud servers. Therefore, this approach allows dynamically allocating functions to the most suitable layer.},
  doi       = {10.1109/EUC.2018.00008},
  keywords  = {cloud computing;computer networks;Internet of Things;serverless function;device requests;fog layer;local network;IoT devices;IoT environments;IoT area;serverless architectural design;local processing power;cloud servers;Cloud computing;Servers;Edge computing;Runtime environment;Internet of Things;Market research;Estimation;Fog Computing;Internet of Things;Multi Armed Bandit;Ubiquitous Computing;Serverless},
}

@InProceedings{8116416,
  author    = {H. {Puripunpinyo} and M. H. {Samadzadeh}},
  title     = {Effect of optimizing Java deployment artifacts on AWS Lambda},
  booktitle = {2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)},
  year      = {2017},
  pages     = {438-443},
  month     = {May},
  abstract  = {AWS Lambda (Amazon Web Services) is the most popular serverless architecture provided by Amazon. It currently supports three platforms: JavaScript, Python, and Java Virtual Machine (JVM). The JVM could be the most complicate platform among the three as there are many languages that target the JVM platform besides Java. In addition, the complex hierarchy of dependencies, versioning, and the class loader are major issues that could cause conflict in a project. Deployment in the context of a serverless architecture means deployment as a function that represents a single service rather than as an application that is comprised of many services. AWS Lambda requires a deployment artifact to be self-contained which means all resources and dependencies must be packaged into a single jar file, and this file could be larger than AWS Lambda's allowable limit. Developers usually use build tool plugins to make self-contained artifacts, and those tools are generally unaware of what class and resource files a function needs. As a result, the artifact is not optimized. This paper demonstrates that optimization of an artifact can in general improve its resource usage and runtime performance. This paper also reports the result of an anecdotal experiment regarding the overhead of calling functions remotely in order to support design decisions in the development of AWS Lambda.},
  doi       = {10.1109/INFCOMW.2017.8116416},
  keywords  = {Java;operating systems (computers);virtual machines;Web services;optimizing Java deployment artifacts;Amazon Web Services;Java Virtual Machine;JVM;complicate platform;deployment artifact;single jar file;AWS Lambda's allowable limit;self-contained artifacts;resource files;Java;Libraries;Computer architecture;Tools;Servers;XML;Big Data;Serverless Architecture;Deployment;AWS Lambda;JVM;Java;Optimization;Performance},
}

@InProceedings{8436935,
  author    = {S. {Rizou} and P. {Athanasoulis} and P. {Andriani} and F. {Iadanza} and G. {Carrozzo} and D. {Breitgand} and A. {Weit} and D. {Griffin} and D. {Jimenez} and U. {Acar} and O. P. {Gordo}},
  title     = {A Service Platform Architecture Enabling Programmable Edge-To-Cloud Virtualization for the 5G Media Industry},
  booktitle = {2018 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)},
  year      = {2018},
  pages     = {1-6},
  month     = {June},
  abstract  = {Media applications are amongst the most demanding services in terms of resources, requiring huge network capacity for high bandwidth audio-visual and other mobile sensory streams. The 5G-MEDIA project aims at innovating media-related applications by investigating how these applications and the underlying 5G network should be coupled and interwork to the benefit of both. The 5G-MEDIA approach aims at delivering an integrated programmable service platform for the development, design and operations of media applications in 5G networks by providing mechanisms to flexibly adapt service operations to dynamic conditions and react upon events (e.g. to transparently accommodate auto-scaling of resources, VNF replacement, etc.). In this paper we present the 5G-MEDIA service platform architecture, which has been specifically designed to enable the development and operation of services for the nascent 5G media industry. Our approach delivers an integrated programmable service platform for the development, design and operations of media applications in 5G networks.},
  doi       = {10.1109/BMSB.2018.8436935},
  issn      = {2155-5052},
  keywords  = {5G mobile communication;cloud computing;multimedia communication;telecommunication computing;virtualisation;Programmable Edge-To-Cloud Virtualization;media applications;5G-MEDIA project;media-related applications;integrated programmable service platform;5G-MEDIA service platform architecture;network capacity;5G network;5G media industry;service platform architecture;mobile sensory streams;high bandwidth audio-visual system;Media;5G mobile communication;Monitoring;FAA;Tools;Computer architecture;Optimization;Network Function Virtualization;edge-cloud;management and operation framework;5G networks for media applications},
}

@InProceedings{8457882,
  author    = {A. {Saha} and S. {Jindal}},
  title     = {EMARS: Efficient Management and Allocation of Resources in Serverless},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {827-830},
  month     = {July},
  abstract  = {We introduce EMARS, an efficient resource management system for serverless cloud computing frameworks with the goal to enhance resource (focus on memory) allocation among containers. We have built our prototype on top of an open-source serverless platform, OpenLambda. It is based upon application workloads and serverless functions' memory needs. As a background motivation we analyzed the latencies and memory requirements of functions running on AWS lambda. The memory limits also lead to variations in number of containers spawned on OpenLambda. We use memory limit settings to propose a model of predictive efficient memory management.},
  doi       = {10.1109/CLOUD.2018.00113},
  issn      = {2159-6190},
  keywords  = {cloud computing;resource allocation;storage management;serverless cloud;open-source serverless platform;OpenLambda;serverless functions;memory limit settings;predictive efficient memory management;resource management system;EMARS system;Containers;Memory management;Resource management;Cloud computing;Servers;Predictive models;Time factors;Serverless;cloud computing;memory limit;response time},
}

@InProceedings{8529465,
  author    = {M. {Sewak} and S. {Singh}},
  title     = {Winning in the Era of Serverless Computing and Function as a Service},
  booktitle = {2018 3rd International Conference for Convergence in Technology (I2CT)},
  year      = {2018},
  pages     = {1-5},
  month     = {April},
  abstract  = {Serverless Computing and Function as a Service (FaaS) is gaining traction in cloud-based application architectures used by startups and matured organizations alike. Organizations that are keen to leverage modern technology to gain a disruptive edge, optimal efficiency, advanced agility and save cost are adopting these architectural styles rapidly. Cloud service provider offer and dynamically manages the allocation of machine resources in serverless computing. The serverless architectures allows the developers to focus on business logic exclusively without worrying about preparing the runtime, managing deployment and infrastructure related concerns. FaaS may be assumed as a subset of Serverless Computing, in which, instead of coding a full-fledged cloud based application, the developer just writes (often small) functions which are piece of code (in one of the multiple programming languages supported by the platform) dedicated to do a focused, often single task that are invoked by triggers. It offers dynamic allocation and scaling of the resources and innovative trigger based costing model. This paper introduces Serverless Computing, and Function as a Service (FaaS), explores its advantages and limitations, options available with popular cloud and Platform as a Service (PaaS) providers, and emerging use cases and success stories.},
  doi       = {10.1109/I2CT.2018.8529465},
  keywords  = {cloud computing;service-oriented architecture;serverless computing;FaaS;cloud-based application architectures;serverless architectures;full-fledged cloud based application;cloud service providers;Function as a Service;Cloud computing;FAA;Google;Computer architecture;Servers;Task analysis;Serverless Computing;Functions as a Service(FaaS);Platform as a Service (PaaS);Microservices;IBM Cloud Functions;Amazon AWS Lambda;Microsoft Azure Functions;Google Cloud Functions;Apache Open Whisk},
}

@InProceedings{8469537,
  author    = {Y. {Song} and J. {Xie} and Q. {Huang} and M. {Wang} and J. {Yu}},
  title     = {Design and Implementation of Turtle Breeding System Based on Embedded Container Cloud},
  booktitle = {2018 2nd IEEE Advanced Information Management,Communicates,Electronic and Automation Control Conference (IMCEC)},
  year      = {2018},
  pages     = {2531-2534},
  month     = {May},
  abstract  = {With the rapid development of computer electronic equipment, embedded devices have been applied in various fields of daily life. Such as intelligent home, intelligent agriculture, intelligent farming and so on. This article describes the intelligent turtle breeding system is a typical application. Different from the combination of traditional embedded devices and servers, this system is based on the combination of the Raspberry Pi and the virtualized container. It is designed as a platform of automatic detection of temperature, humidity, co2concentration, and light intensity at turtle breeding bases. Breaking through the limitations of traditional server monitoring, we use Docker to build a cloud environment and Docker Swarm as a container management technology, and adopt openFaas architecture to complete the serverless integration of virtual container cloud and embedded devices. Making the turtle aquaculture system more secure, more stable, less costly, and more resource efficient.},
  doi       = {10.1109/IMCEC.2018.8469537},
  keywords  = {agriculture;aquaculture;cloud computing;embedded systems;knowledge based systems;virtualisation;embedded container cloud;computer electronic equipment;intelligent home;intelligent agriculture;intelligent farming;intelligent turtle breeding system;Raspberry Pi;virtualized container;cloud environment;container management technology;virtual container cloud;turtle aquaculture system;server monitoring;openFaas architecture;Docker Swarm;Automation;Raspberry pi;container;Turtle farming system},
}

@InProceedings{8301615,
  author    = {Y. {Tian} and R. {Babcock} and C. {Taylor} and Y. {Ji}},
  title     = {A new live video streaming approach based on Amazon S3 pricing model},
  booktitle = {2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC)},
  year      = {2018},
  pages     = {321-328},
  month     = {Jan},
  abstract  = {Video has become a mainstream media source on the web, and live video streaming is growing as a prominent player in the modern marketplace for both businesses and individuals. Due to elasticity and on-demand nature that cloud computing provides, more and more video service providers (VSP) utilize cloud infrastructures to provide cloud-based video applications. In this work, we propose Cloud Live Video Streaming (CLVS) - a new approach to efficiently stream live video that is based on Amazon S3 pricing model. In the design of CLVS, when a source video is being recorded by a mobile device, on which then it is segmented and encoded. Next, those video segments are pushed into a designated Amazon S3 bucket. On end-user devices, the client program of CLVS directly retrieves the most recent video segment from the S3 bucket, then performs decoding and video playing back. By using Amazon S3 service, our CLVS employs what is referred to as a “serverless” design by eliminating the needs for an intermediary and persistently running streaming server. Thus, VSPs do not have to pay for the idle time found on the traditional streaming servers, but still are able to continually provide live video streams to end viewers. We implement a prototype of CLVS and optimize its performance by using multithreaded prefetching and caching techniques. In experiments, we compare our CLVS with an existing video streaming software - Wowza Streaming Engine. Experiment results indicate that CLVS not only outperforms Wowza in many aspects of video streaming performance, but also financially costs less.},
  doi       = {10.1109/CCWC.2018.8301615},
  keywords  = {cloud computing;mobile handsets;multimedia communication;video signal processing;video streaming;live video streaming approach;Amazon S3 pricing model;mainstream media source;video service providers;cloud infrastructures;cloud-based video applications;Cloud Live Video Streaming;CLVS;source video;video segments;Amazon S3 service;traditional streaming servers;Wowza Streaming Engine;video streaming performance;video streaming software;Amazon S3 bucket;VSP;Streaming media;Cloud computing;Servers;Indexes;Pricing;Instruction sets;Cloud computing;Live Video Streaming;Cloud Storage Service},
}

@InProceedings{8666520,
  author    = {R. {Tonelli} and M. I. {Lunesu} and A. {Pinna} and D. {Taibi} and M. {Marchesi}},
  title     = {Implementing a Microservices System with Blockchain Smart Contracts},
  booktitle = {2019 IEEE International Workshop on Blockchain Oriented Software Engineering (IWBOSE)},
  year      = {2019},
  pages     = {22-31},
  month     = {Feb},
  abstract  = {Blockchain technologies and smart contracts are becoming mainstream research fields in computer science and researchers are continuously investigating new frontiers for new applications. Likewise, microservices are getting more and more popular in the latest years thanks to their properties, that allow teams to slice existing information systems into small and independent services that can be developed independently by different teams. A symmetric paradigm applies to smart contracts as well, which represent well defined, usually isolated, executable programs, typically implementing simple and autonomous tasks with a well defined purpose, which can be assumed as services provided by the Contract. In this work we analyze a concrete case study where the microservices architecture environment is replicated and implemented through an equivalent set of smart contracts, showing for the first time the feasibility of implementing a microservices-based system with smart contracts and how the two innovative paradigms match together. Results show that it is possible to implement a simple microservices-based system with smart contracts maintaining the same set of functionalities and results. The result could be highly beneficial in contexts such as smart voting, where not only the data integrity is fundamental but also the source code executed must be trustable.},
  doi       = {10.1109/IWBOSE.2019.8666520},
  keywords  = {contracts;cryptography;distributed databases;innovation management;software architecture;microservices system;blockchain smart contracts;information systems;microservices architecture environment;innovative paradigms;Smart contracts;Blockchain;Computer architecture;Computer languages;Logic gates;Microservice;Cloud Native;Blockchain;Smart contract;Serverless},
}

@InProceedings{8539109,
  author    = {H. {Truong}},
  title     = {Integrated Analytics for IIoT Predictive Maintenance Using IoT Big Data Cloud Systems},
  booktitle = {2018 IEEE International Conference on Industrial Internet (ICII)},
  year      = {2018},
  pages     = {109-118},
  month     = {Oct},
  abstract  = {For predictive maintenance of equipment with Industrial Internet of Things (IIoT) technologies, existing IoT Cloud systems provide strong monitoring and data analysis capabilities for detecting and predicting status of equipment. However, we need to support complex interactions among different software components and human activities to provide an integrated analytics, as software algorithms alone cannot deal with the complexity and scale of data collection and analysis and the diversity of equipment, due to the difficulties of capturing and modeling uncertainties and domain knowledge in predictive maintenance. In this paper, we describe how we design and augment complex IoT big data cloud systems for integrated analytics of IIoT predictive maintenance. Our approach is to identify various complex interactions for solving system incidents together with relevant critical analytics results about equipment. We incorporate humans into various parts of complex IoT Cloud systems to enable situational data collection, services management, and data analytics. We leverage serverless functions, cloud services, and domain knowledge to support dynamic interactions between human and software for maintaining equipment. We use a real-world maintenance of Base Transceiver Stations to illustrate our engineering approach which we have prototyped with state-of-the art cloud and IoT technologies, such as Apache Nifi, Hadoop, Spark and Google Cloud Functions.},
  doi       = {10.1109/ICII.2018.00020},
  keywords  = {Big Data;cloud computing;data analysis;Internet of Things;production engineering computing;software maintenance;system monitoring;Web services;integrated analytics;IIoT predictive maintenance;software algorithms;system incidents;data analytics;cloud services;Google Cloud Functions;system monitoring;data analysis;software components;IoT Cloud systems;Industrial Internet of Things technologies;IoT big data cloud systems;base transceiver stations;Predictive maintenance;Cloud computing;Task analysis;Software;Data analysis;Sensors;IIOT;predictive maintenance;big data;cloud computing;analytics;services computing},
}

@Article{8625892,
  author   = {J. L. {Vázquez-Poletti} and I. M. {Llorente} and K. {Hinsen} and M. {Turk}},
  title    = {Serverless Computing: From Planet Mars to the Cloud},
  journal  = {Computing in Science Engineering},
  year     = {2018},
  volume   = {20},
  number   = {6},
  pages    = {73-79},
  month    = {Nov},
  issn     = {1521-9615},
  abstract = {Serverless computing is a new way of managing computations in the cloud. We show how it can be put to work for scientific data analysis. For this, we detail our serverless architecture for an application analyzing data from one of the instruments onboard the ESA Mars Express orbiter, and then, we compare it with a traditional server solution.},
  doi      = {10.1109/MCSE.2018.2875315},
  keywords = {astronomy computing;data analysis;Mars;space vehicles;planet Mars;serverless computing;scientific data analysis;serverless architecture;ESA Mars Express orbiter;cloud;Cloud computing;Containers;Mars;Servers;Computational modeling;Artificial intelligence;Programming},
}

@InProceedings{7573771,
  author    = {B. {Wagner} and A. {Sood}},
  title     = {Economics of Resilient Cloud Services},
  booktitle = {2016 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
  year      = {2016},
  pages     = {368-374},
  month     = {Aug},
  abstract  = {Today's computer systems must meet and maintain service availability, performance, and security requirements. Each of these demands requires redundancy and some form of isolation. When service requirements are implemented separately, the system architecture cannot easily share common components of redundancy and isolation. We will present these service traits collectively as cyber resilience with a system called Self-Cleansing Intrusion Tolerance (SCIT). Further, we will demonstrate that SCIT provides an effective resilient cloud implementation making cost effective utilization of cloud's excess capacity and economies of scale. Lastly, we will introduce the notion of serverless applications utilizing AWS Lambda and how a stateless architecture can drastically reduce operational costs by utilizing cloud function services.},
  doi       = {10.1109/QRS-C.2016.56},
  keywords  = {cloud computing;economics;security of data;economics;resilient cloud services;security requirements;system architecture;self-cleansing intrusion tolerance;SCIT;resilient cloud implementation;cloud excess capacity;AWS Lambda;stateless architecture;cloud function services;computer systems;service availability;service performance;Servers;Cloud computing;Redundancy;Security;Programming;Resilience;Cyber Resilience;Security Economics;SCIT;AWS Spot;GCP Preemptive},
}

@InProceedings{8121899,
  author    = {T. {Wei} and Y. {Coady} and J. {MacDonald} and K. {Booth} and J. {Salter} and C. {Girling}},
  title     = {Dumb pipes for smart systems: How tomorrow's applications can salvage yesterday's plumbing},
  booktitle = {2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)},
  year      = {2017},
  pages     = {1-5},
  month     = {Aug},
  abstract  = {Tomorrow's immersive applications will leverage Mixed Reality interfaces accessing a multitude of services from distributed clouds. They will face extreme latency constraints, massive datasets, spontaneous collaboration, and constant service churn. This paper outlines our experience evolving an application designed to support collaborative work in Urban Design (UD) practices. The application, UD Co-Spaces, recently weathered significant churn as a core service was discontinued and replaced by a service with a subtly different API. A “dumb pipes” approach, where services communicate through a simple message queue, facilitated this evolution with relatively little disruption to the rest of the system. We show how this strategy can be used to reintroduce new features to the system, and is sustainable as the system's interfaces evolve to use Virtual, Augmented and Mixed Reality environments.},
  doi       = {10.1109/PACRIM.2017.8121899},
  keywords  = {application program interfaces;augmented reality;cloud computing;groupware;human computer interaction;dumb pipes;smart systems;distributed clouds;extreme latency constraints;massive datasets;spontaneous collaboration;constant service churn;collaborative work;Urban Design practices;UD Co-Spaces;simple message queue;mixed reality environments;augmented environments;virtual reality environments;API;Three-dimensional displays;Google;Earth;Servers;Solid modeling;Computer architecture;Buildings;serverless systems;software evolution;virtual reality;computer supported collaborative work},
}

@InProceedings{8622362,
  author    = {S. {Werner} and J. {Kuhlenkamp} and M. {Klems} and J. {Müller} and S. {Tai}},
  title     = {Serverless Big Data Processing using Matrix Multiplication as Example},
  booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
  year      = {2018},
  pages     = {358-365},
  month     = {Dec},
  abstract  = {Serverless computing, or Function-as-a-Service (FaaS), is emerging as a popular alternative model to on-demand cloud computing. Function services are executed by a FaaS provider; a client no longer uses cloud infrastructure directly as in traditional cloud consumption. Is serverless computing a feasible and beneficial approach to big data processing, regarding performance, scalability, and cost effectiveness? In this paper, we explore this research question using matrix multiplication as example. We define requirements for the design of serverless big data applications, present a prototype for matrix multiplication using FaaS, and discuss and synthesize insights from results of extensive experimentation. We show that serverless big data processing can lower operational and infrastructure costs without compromising system qualities; serverless computing can even outperform cluster-based distributed compute frameworks regarding performance and scalability.},
  doi       = {10.1109/BigData.2018.8622362},
  keywords  = {Big Data;cloud computing;matrix multiplication;pattern clustering;service-oriented architecture;serverless big data processing;matrix multiplication;serverless computing;popular alternative model;on-demand cloud computing;FaaS provider;cloud infrastructure;traditional cloud consumption;serverless big data applications;compute frameworks;function-as-a-service;Cloud computing;Scalability;FAA;Prototypes;Big Data applications;Task analysis;serverless;big data;cloud;matrix multiplication},
}

@InProceedings{8514413,
  author    = {M. {Westerlund} and N. {Kratzke}},
  title     = {Towards Distributed Clouds: A Review About the Evolution of Centralized Cloud Computing, Distributed Ledger Technologies, and A Foresight on Unifying Opportunities and Security Implications},
  booktitle = {2018 International Conference on High Performance Computing Simulation (HPCS)},
  year      = {2018},
  pages     = {655-663},
  month     = {July},
  abstract  = {This review focuses on the evolution of cloud computing and distributed ledger technologies (blockchains) over the last decade. Cloud computing relies mainly on a conceptually centralized service provisioning model, while blockchain technologies originate from a peer-to-peer and a completely distributed approach. Still, noteworthy commonalities between both approaches are often overlooked by researchers. Therefore, to the best of the authors knowledge, this paper reviews both domains in parallel for the first time. We conclude that both approaches have advantages and disadvantages. The advantages of centralized service provisioning approaches are often the disadvantages of distributed ledger approaches and vice versa. It is obviously an interesting question whether both approaches could be combined in a way that the advantages can be added while the disadvantages could be avoided. We derive a software stack that could build the foundation unifying the best of these two worlds and that would avoid existing shortcomings like vendor lock-in, some security problems, and inherent platform dependencies.},
  doi       = {10.1109/HPCS.2018.00108},
  keywords  = {cloud computing;security of data;centralized cloud computing;distributed ledger technologies;security implications;conceptually centralized service provisioning model;blockchain technologies;completely distributed approach;centralized service provisioning approaches;distributed ledger approaches;peer-to-peer approach;Cloud computing;Computer architecture;FAA;Servers;Containers},
}

@InProceedings{8599581,
  author    = {M. {Wurster} and U. {Breitenbücher} and K. {Képes} and F. {Leymann} and V. {Yussupov}},
  title     = {Modeling and Automated Deployment of Serverless Applications Using TOSCA},
  booktitle = {2018 IEEE 11th Conference on Service-Oriented Computing and Applications (SOCA)},
  year      = {2018},
  pages     = {73-80},
  month     = {Nov},
  abstract  = {The serverless computing paradigm brings multiple benefits to application developers who are interested in consuming computing resources as services without the need to manage physical capacities or limits. There are several deployment technologies and languages available suitable for deploying applications to a single cloud provider. However, for multi-cloud application deployments, multiple technologies have to be used and orchestrated. In addition, the event-driven nature of serverless computing imposes further requirements on modeling such application structures in order to automate their deployment. In this paper, we tackle these issues by introducing an event-driven deployment modeling approach using the standard Topology and Orchestration Specification for Cloud Applications (TOSCA) that fully employs the suggested standard lifecycle to provision and manage multi-cloud serverless applications. To show the feasibility of our approach, we extended the existing TOSCA-based ecosystem OpenTOSCA.},
  doi       = {10.1109/SOCA.2018.00017},
  issn      = {2163-2871},
  keywords  = {cloud computing;computing resources;physical capacities;deployment technologies;single cloud provider;multicloud application deployments;multiple technologies;event-driven nature;application structures;event-driven deployment modeling approach;multicloud serverless applications;serverless computing paradigm;application developers;cloud applications;TOSCA-based ecosystem;OpenTOSCA;Cloud computing;Computational modeling;Topology;Standards;Computer architecture;FAA;Serverless;Multi-Cloud;Modeling;Automated Deployment;TOSCA},
}

@Article{8327546,
  author   = {M. {Yousif}},
  title    = {The State of the Cloud},
  journal  = {IEEE Cloud Computing},
  year     = {2018},
  volume   = {5},
  number   = {1},
  pages    = {4-5},
  month    = {Jan},
  issn     = {2325-6095},
  abstract = {In the State of the Cloud article in January 2017, IEEE Cloud Computing Editor in Chief Mazin Yousif said that the cloud would become the de-facto hosting platform for all applications and social innovations. He also mentioned that it is becoming the new normal. And that does seem to be the case. Cloud computing is enabling companies to innovate at their own speed, allowing them to spin up and down images, launch applications and analytics as fast as they need. The cloud also lets consumers make choices from a wide variety of services.},
  doi      = {10.1109/MCC.2018.011791706},
  keywords = {cloud computing;consumers;digitalization;serverless computing},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveOrderConfig:specified;author;false;abstract;false;abstract;false;}
