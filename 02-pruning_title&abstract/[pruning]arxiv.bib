% Encoding: UTF-8

@Article{2018arXiv181006080A,
  author        = {{Alder}, Fritz and {Asokan}, N. and {Kurnikov}, Arseny and {Paverd}, Andrew and {Steiner}, Michael},
  title         = {{S-FaaS: Trustworthy and Accountable Function-as-a-Service using Intel SGX}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1810.06080},
  month         = {Oct},
  abstract      = {Function-as-a-Service (FaaS) is a recent and already very popular paradigm in cloud computing. The function provider need only specify the function to be run, usually in a high-level language like JavaScript, and the service provider orchestrates all the necessary infrastructure and software stacks. The function provider is only billed for the actual computational resources used by the function invocation. Compared to previous cloud paradigms, FaaS requires significantly more fine-grained resource measurement mechanisms, e.g. to measure compute time and memory usage of a single function invocation with sub-second accuracy. Thanks to the short duration and stateless nature of functions, and the availability of multiple open-source frameworks, FaaS enables non-traditional service providers e.g. individuals or data centers with spare capacity. However, this exacerbates the challenge of ensuring that resource consumption is measured accurately and reported reliably. It also raises the issues of ensuring computation is done correctly and minimizing the amount of information leaked to service providers. To address these challenges, we introduce S-FaaS, the first architecture and implementation of FaaS to provide strong security and accountability guarantees backed by Intel SGX. To match the dynamic event-driven nature of FaaS, our design introduces a new key distribution enclave and a novel transitive attestation protocol. A core contribution of S-FaaS is our set of resource measurement mechanisms that securely measure compute time inside an enclave, and actual memory allocations. We have integrated S-FaaS into the popular OpenWhisk FaaS framework. We evaluate the security of our architecture, the accuracy of our resource measurement mechanisms, and the performance of our implementation, showing that our resource measurement mechanisms add less than 6.3% latency on standardized benchmarks. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181006080A},
  archiveprefix = {arXiv},
  eid           = {arXiv:1810.06080},
  eprint        = {1810.06080},
  keywords      = {Computer Science - Cryptography and Security},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/pdf/1810.06080.pdf},
}

@Article{2018arXiv180208984A,
  author        = {{Alpernas}, Kalev and {Flanagan}, Cormac and {Fouladi}, Sadjad and {Ryzhyk}, Leonid and {Sagiv}, Mooly and {Schmitz}, Thomas and {Winstein}, Keith},
  title         = {{Secure Serverless Computing Using Dynamic Information Flow Control}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1802.08984},
  month         = {Feb},
  abstract      = {The rise of serverless computing provides an opportunity to rethink cloud security. We present an approach for securing serverless systems using a novel form of dynamic information flow control (IFC). We show that in serverless applications, the termination channel found in most existing IFC systems can be arbitrarily amplified via multiple concurrent requests, necessitating a stronger termination-sensitive non-interference guarantee, which we achieve using a combination of static labeling of serverless processes and dynamic faceted labeling of persistent data. We describe our implementation of this approach on top of JavaScript for AWS Lambda and OpenWhisk serverless platforms, and present three realistic case studies showing that it can enforce important IFC security properties with low overhead.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180208984A},
  archiveprefix = {arXiv},
  eid           = {arXiv:1802.08984},
  eprint        = {1802.08984},
  keywords      = {Computer Science - Programming Languages, Computer Science - Cryptography and Security},
  primaryclass  = {cs.PL},
  url           = {https://arxiv.org/pdf/1802.08984.pdf},
}

@Article{2019arXiv190103161A,
  author        = {{Aytekin}, Arda and {Johansson}, Mikael},
  title         = {{Harnessing the Power of Serverless Runtimes for Large-Scale Optimization}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.03161},
  month         = {Jan},
  abstract      = {The event-driven and elastic nature of serverless runtimes makes them a very efficient and cost-effective alternative for scaling up computations. So far, they have mostly been used for stateless, data parallel and ephemeral computations. In this work, we propose using serverless runtimes to solve generic, large-scale optimization problems. Specifically, we build a master-worker setup using AWS Lambda as the source of our workers, implement a parallel optimization algorithm to solve a regularized logistic regression problem, and show that relative speedups up to 256 workers and efficiencies above 70% up to 64 workers can be expected. We also identify possible algorithmic and system-level bottlenecks, propose improvements, and discuss the limitations and challenges in realizing these improvements. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190103161A},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.03161},
  eprint        = {1901.03161},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1901.03161.pdf},
}

@Article{2017arXiv170603178B,
  author        = {{Baldini}, Ioana and {Castro}, Paul and {Chang}, Kerry and {Cheng}, Perry and {Fink}, Stephen and {Ishakian}, Vatche and {Mitchell}, Nick and {Muthusamy}, Vinod and {Rabbah}, Rodric and {Slominski}, Aleksander and {Suter}, Philippe},
  title         = {{Serverless Computing: Current Trends and Open Problems}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1706.03178},
  month         = {Jun},
  abstract      = {Serverless computing has emerged as a new compelling paradigm for the deployment of applications and services. It represents an evolution of cloud programming models, abstractions, and platforms, and is a testament to the maturity and wide adoption of cloud technologies. In this chapter, we survey existing serverless platforms from industry, academia, and open source projects, identify key characteristics and use cases, and describe technical challenges and open problems.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170603178B},
  archiveprefix = {arXiv},
  eid           = {arXiv:1706.03178},
  eprint        = {1706.03178},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1706.03178.pdf},
}

@Article{2019arXiv190106811B,
  author        = {{Bartan}, Burak and {Pilanci}, Mert},
  title         = {{Polar Coded Distributed Matrix Multiplication}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.06811},
  month         = {Jan},
  abstract      = {We propose a polar coding mechanism for distributed matrix multiplication. Polar codes provably achieve channel capacity and have the advantage of low encoding and decoding complexity. These aspects of polar codes enable a scalable scheme for hundreds of compute nodes in coded computation. We analyze the polarization phenomenon in the context of run times of compute nodes and characterize polarizing matrices over real numbers. We design a sequential decoder specifically for polar codes in erasure channels with real-valued input and outputs. The proposed coded computation scheme is implemented for a serverless computing platform and numerical results are provided. Numerical results illustrate that proposed coded computation scheme achieves significant speed-ups. Finally, experiments are conducted where the performance of the proposed coded computation technique is tested in solving a least squares problem using gradient descent. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190106811B},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.06811},
  eprint        = {1901.06811},
  keywords      = {Computer Science - Information Theory, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
  primaryclass  = {cs.IT},
  url           = {https://arxiv.org/pdf/1901.06811.pdf},
}

@Article{2019arXiv190401576B,
  author        = {{Bhattacharjee}, Anirban and {Chhokra}, Ajay Dev and {Kang}, Zhuangwei and {Sun}, Hongyang and {Gokhale}, Aniruddha and {Karsai}, Gabor},
  title         = {{BARISTA: Efficient and Scalable Serverless Serving System for Deep Learning Prediction Services}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1904.01576},
  month         = {Apr},
  abstract      = {Pre-trained deep learning models are increasingly being used to offer a variety of compute-intensive predictive analytics services such as fitness tracking, speech and image recognition. The stateless and highly parallelizable nature of deep learning models makes them well-suited for serverless computing paradigm. However, making effective resource management decisions for these services is a hard problem due to the dynamic workloads and diverse set of available resource configurations that have their deployment and management costs. To address these challenges, we present a distributed and scalable deep-learning prediction serving system called Barista and make the following contributions. First, we present a fast and effective methodology for forecasting workloads by identifying various trends. Second, we formulate an optimization problem to minimize the total cost incurred while ensuring bounded prediction latency with reasonable accuracy. Third, we propose an efficient heuristic to identify suitable compute resource configurations. Fourth, we propose an intelligent agent to allocate and manage the compute resources by horizontal and vertical scaling to maintain the required prediction latency. Finally, using representative real-world workloads for urban transportation service, we demonstrate and validate the capabilities of Barista. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190401576B},
  archiveprefix = {arXiv},
  eid           = {arXiv:1904.01576},
  eprint        = {1904.01576},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1904.01576.pdf},
}

@Article{2017arXiv171109123B,
  author        = {{Buyya}, Rajkumar and {Narayana Srirama}, Satish and {Casale}, Giuliano and {Calheiros}, Rodrigo and {Simmhan}, Yogesh and {Varghese}, Blesson and {Gelenbe}, Erol and {Javadi}, Bahman and {Vaquero}, Luis Miguel and {Netto}, Marco A.~S. and {Nadjaran Toosi}, Adel and {Rodriguez}, Maria Alejandra and {Llorente}, Ignacio M. and {De Capitani di Vimercati}, Sabrina and {Samarati}, Pierangela and {Milojicic}, Dejan and {Varela}, Carlos and {Bahsoon}, Rami and {Dias de Assuncao}, Marcos and {Rana}, Omer and {Zhou}, Wanlei and {Jin}, Hai and {Gentzsch}, Wolfgang and {Zomaya}, Albert Y. and {Shen}, Haiying},
  title         = {{A Manifesto for Future Generation Cloud Computing: Research Directions for the Next Decade}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1711.09123},
  month         = {Nov},
  abstract      = {The Cloud computing paradigm has revolutionised the computer science horizon during the past decade and has enabled the emergence of computing as the fifth utility. It has captured significant attention of academia, industries, and government bodies. Now, it has emerged as the backbone of modern economy by offering subscription-based services anytime, anywhere following a pay-as-you-go model. This has instigated (1) shorter establishment times for start-ups, (2) creation of scalable global enterprise applications, (3) better cost-to-value associativity for scientific and high performance computing applications, and (4) different invocation/execution models for pervasive and ubiquitous applications. The recent technological developments and paradigms such as serverless computing, software-defined networking, Internet of Things, and processing at network edge are creating new opportunities for Cloud computing. However, they are also posing several new challenges and creating the need for new approaches and research strategies, as well as the re-evaluation of the models that were developed to address issues such as scalability, elasticity, reliability, security, sustainability, and application models. The proposed manifesto addresses them by identifying the major open challenges in Cloud computing, emerging trends, and impact areas. It then offers research directions for the next decade, thus helping in the realisation of Future Generation Cloud Computing. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv171109123B},
  archiveprefix = {arXiv},
  eid           = {arXiv:1711.09123},
  eprint        = {1711.09123},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1711.09123.pdf},
}

@Article{2018arXiv181109732D,
  author        = {{Dakkak}, Abdul and {Li}, Cheng and {Garcia de Gonzalo}, Simon and {Xiong}, Jinjun and {Hwu}, Wen-mei},
  title         = {{TrIMS: Transparent and Isolated Model Sharing for Low Latency Deep LearningInference in Function as a Service Environments}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1811.09732},
  month         = {Nov},
  abstract      = {Deep neural networks (DNNs) have become core computation components within low latency Function as a Service (FaaS) prediction pipelines: including image recognition, object detection, natural language processing, speech synthesis, and personalized recommendation pipelines. Cloud computing, as the de-facto backbone of modern computing infrastructure for both enterprise and consumer applications, has to be able to handle user-defined pipelines of diverse DNN inference workloads while maintaining isolation and latency guarantees, and minimizing resource waste. The current solution for guaranteeing isolation within FaaS is suboptimal -- suffering from "cold start" latency. A major cause of such inefficiency is the need to move large amount of model data within and across servers. We propose TrIMS as a novel solution to address these issues. Our proposed solution consists of a persistent model store across the GPU, CPU, local storage, and cloud storage hierarchy, an efficient resource management layer that provides isolation, and a succinct set of application APIs and container technologies for easy and transparent integration with FaaS, Deep Learning (DL) frameworks, and user code. We demonstrate our solution by interfacing TrIMS with the Apache MXNet framework and demonstrate up to 24x speedup in latency for image classification models and up to 210x speedup for large models. We achieve up to 8x system throughput improvement. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181109732D},
  archiveprefix = {arXiv},
  eid           = {arXiv:1811.09732},
  eprint        = {1811.09732},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1811.09732.pdf},
}

@Article{2019arXiv190100302D,
  author        = {{Danayi}, Abolfazl and {Sharifian}, Saeed},
  title         = {{openCoT: The opensource Cloud of Things platform}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.00302},
  month         = {Jan},
  abstract      = {In order to address the complexity and extensiveness of technology, Cloud Computing is utilized with four main service models. The most recent service model, function-as-a-service, enables developers to develop their application in a function-based structure and then deploy it to the Cloud. Using an optimum elastic auto-scaling, the performance of executing an application over FaaS Cloud, overcomes the extra overhead and reduces the total cost. However, researchers need a simple and well-documented FaaS Cloud manager in order to implement their proposed Auto-scaling algorithms. In this paper, we represent the openCoT platform and explain its building blocks and details. Experimental results show that executing a function (invoking and passing arguments) and returning the result using openCoT takes 21 ms over a remote connection. The source code of openCoT is available in the GitHub repository of the project (\code{www.github.com/adanayi/opencot}) for public usage. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190100302D},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.00302},
  eprint        = {1901.00302},
  keywords      = {Computer Science - Networking and Internet Architecture, Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.NI},
  url           = {https://arxiv.org/pdf/1901.00302.pdf},
}

@Article{2018arXiv181105948D,
  author        = {{Das}, Anirban and {Patterson}, Stacy and {Wittie}, Mike P.},
  title         = {{EdgeBench: Benchmarking Edge Computing Platforms}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1811.05948},
  month         = {Nov},
  abstract      = {The emerging trend of edge computing has led several cloud providers to release their own platforms for performing computation at the 'edge' of the network. We compare two such platforms, Amazon AWS Greengrass and Microsoft Azure IoT Edge, using a new benchmark comprising a suite of performance metrics. We also compare the performance of the edge frameworks to cloud-only implementations available in their respective cloud ecosystems. Amazon AWS Greengrass and Azure IoT Edge use different underlying technologies, edge Lambda functions vs. containers, and so we also elaborate on platform features available to developers. Our study shows that both of these edge platforms provide comparable performance, which nevertheless differs in important ways for key types of workloads used in edge applications. Finally, we discuss several current issues and challenges we faced in deploying these platforms. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181105948D},
  archiveprefix = {arXiv},
  eid           = {arXiv:1811.05948},
  eprint        = {1811.05948},
  keywords      = {Computer Science - Networking and Internet Architecture, Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.NI},
  url           = {https://arxiv.org/pdf/1811.05948.pdf},
}

@ARTICLE{2019arXiv190504456D,
author = {{Denninnart}, Chavit and {Gentry}, James and {Amini Salehi}, Mohsen},
title = "{Improving Robustness of Heterogeneous Serverless Computing Systems Via Probabilistic Task Pruning}",
journal = {arXiv e-prints},
keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
year = "2019",
month = "May",
eid = {arXiv:1905.04456},
pages = {arXiv:1905.04456},
archivePrefix = {arXiv},
eprint = {1905.04456},
primaryClass = {cs.DC},
adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190504456D},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@Article{2018arXiv181109721E,
  author        = {{Elgamal}, Tarek and {Sandur}, Atul and {Nahrstedt}, Klara and {Agha}, Gul},
  title         = {{Costless: Optimizing Cost of Serverless Computing through Function Fusion and Placement}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1811.09721},
  month         = {Nov},
  abstract      = {Serverless computing has recently experienced significant adoption by several applications, especially Internet of Things (IoT) applications. In serverless computing, rather than deploying and managing dedicated virtual machines, users are able to deploy individual functions, and pay only for the time that their code is actually executing. However, since serverless platforms are relatively new, they have a completely different pricing model that depends on the memory, duration, and the number of executions of a sequence/workflow of functions. In this paper we present an algorithm that optimizes the price of serverless applications in AWS Lambda. We first describe the factors affecting price of serverless applications which include: (1) fusing a sequence of functions, (2) splitting functions across edge and cloud resources, and (3) allocating the memory for each function. We then present an efficient algorithm to explore different function fusion-placement solutions and find the solution that optimizes the application's price while keeping the latency under a certain threshold. Our results on image processing workflows show that the algorithm can find solutions optimizing the price by more than 35%-57% with only 5%-15% increase in latency. We also show that our algorithm can find non-trivial memory configurations that reduce both latency and price. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181109721E},
  archiveprefix = {arXiv},
  eid           = {arXiv:1811.09721},
  eprint        = {1811.09721},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1811.09721.pdf},
}

@ARTICLE{2019arXiv190504460F,
author = {{Farhan Hussain}, Razin and {Amini Salehi}, Mohsen and {Semiari}, Omid},
title = "{Serverless Edge Computing for Green Oil and Gas Industry}",
journal = {arXiv e-prints},
keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
year = "2019",
month = "May",
eid = {arXiv:1905.04460},
pages = {arXiv:1905.04460},
archivePrefix = {arXiv},
eprint = {1905.04460},
primaryClass = {cs.DC},
adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190504460F},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@Article{2017arXiv170808028F,
  author        = {{Fox}, Geoffrey C. and {Ishakian}, Vatche and {Muthusamy}, Vinod and {Slominski}, Aleksander},
  title         = {{Status of Serverless Computing and Function-as-a-Service(FaaS) in Industry and Research}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1708.08028},
  month         = {Aug},
  abstract      = {This whitepaper summarizes issues raised during the First International Workshop on Serverless Computing (WoSC) 2017 held June 5th 2017 and especially in the panel and associated discussion that concluded the workshop. We also include comments from the keynote and submitted papers. A glossary at the end (section 8) defines many technical terms used in this report. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170808028F},
  archiveprefix = {arXiv},
  eid           = {arXiv:1708.08028},
  eprint        = {1708.08028},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/ftp/arxiv/papers/1708/1708.08028.pdf},
}

@Article{2019arXiv190307962G,
  author        = {{Gabbrielli}, Maurizio and {Giallorenzo}, Saverio and {Lanese}, Ivan and {Montesi}, Fabrizio and {Peressotti}, Marco and {Zingaro}, Stefano Pio},
  title         = {{No more, no less - A formal model for serverless computing}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1903.07962},
  month         = {Mar},
  abstract      = {Serverless computing, also known as Functions-as-a-Service, is a recent paradigm aimed at simplifying the programming of cloud applications. The idea is that developers design applications in terms of functions, which are then deployed on a cloud infrastructure. The infrastructure takes care of executing the functions whenever requested by remote clients, dealing automatically with distribution and scaling with respect to inbound traffic. While vendors already support a variety of programming languages for serverless computing (e.g. Go, Java, Javascript, Python), as far as we know there is no reference model yet to formally reason on this paradigm. In this paper, we propose the first formal programming model for serverless computing, which combines ideas from both the λ-calculus (for functions) and the π-calculus (for communication). To illustrate our proposal, we model a real-world serverless system. Thanks to our model, we are also able to capture and pinpoint the limitations of current vendor technologies, proposing possible amendments. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190307962G},
  archiveprefix = {arXiv},
  eid           = {arXiv:1903.07962},
  eprint        = {1903.07962},
  keywords      = {Computer Science - Programming Languages},
  primaryclass  = {cs.PL},
  url           = {https://arxiv.org/pdf/1903.07962.pdf},
}

@Article{2018arXiv180711248G,
  author        = {{Garc{\'\i}a L{\'o}pez}, Pedro and {S{\'a}nchez-Artigas}, Marc and {Par{\'\i}s}, Gerard and {Barcelona Pons}, Daniel and {Ruiz Ollobarren}, {\'A}lvaro and {Arroyo Pinto}, David},
  title         = {{Comparison of FaaS Orchestration Systems}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1807.11248},
  month         = {Jul},
  abstract      = {Since the appearance of Amazon Lambda in 2014, all major cloud providers have embraced the Function as a Service (FaaS) model, because of its enormous potential for a wide variety of applications. As expected (and also desired), the competition is fierce in the serverless world, and includes aspects such as the run-time support for the orchestration of serverless functions. In this regard, the three major production services are currently Amazon Step Functions (December 2016), Azure Durable Functions (June 2017), and IBM Composer (October 2017), still young and experimental projects with a long way ahead. In this article, we will compare and analyze these three serverless orchestration systems under a common evaluation framework. We will study their architectures, programming and billing models, and their effective support for parallel execution, among others. Through a series of experiments, we will also evaluate the run-time overhead of the different infrastructures for different types of workflows. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180711248G},
  archiveprefix = {arXiv},
  eid           = {arXiv:1807.11248},
  eprint        = {1807.11248},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1807.11248.pdf},
}

@Article{2018arXiv180801353G,
  author        = {{Gibert Renart}, Eduard and {Balouek-Thomert}, Daniel and {Parashar}, Manish},
  title         = {{Edge Based Data-Driven Pipelines (Technical Report)}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1808.01353},
  month         = {Aug},
  abstract      = {This research reports investigates an edge on-device stream processing platform, which extends the serverless com- puting model to the edge to help facilitate real-time data analytics across the cloud and edge in a uniform manner. We investigate associated use cases and architectural design. We deployed and tested our system on edge devices (Raspberry Pi and Android Phone), which proves that stream processing analytics can be performed at the edge of the network with single board computers in a real-time fashion.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180801353G},
  archiveprefix = {arXiv},
  eid           = {arXiv:1808.01353},
  eprint        = {1808.01353},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1808.01353.pdf},
}

@Article{2018arXiv181102638G,
  author        = {{Gorlatova}, Maria and {Inaltekin}, Hazer and {Chiang}, Mung},
  title         = {{Characterizing Task Completion Latencies in Fog Computing}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1811.02638},
  month         = {Nov},
  abstract      = {Fog computing, which distributes computing resources to multiple locations between the Internet of Things (IoT) devices and the cloud, is attracting considerable attention from academia and industry. Yet, despite the excitement about the potential of fog computing, few comprehensive quantitative characteristics of the properties of fog computing architectures have been conducted. In this paper we examine the properties of task completion latencies in fog computing. First, we present the results of our empirical benchmarking-based study of task completion latencies. The study covered a range of settings, and uniquely considered both traditional and serverless fog computing execution points. It demonstrated the range of execution point characteristics in different locations and the relative stability of latency characteristics for a given location. It also highlighted properties of serverless execution that are not incorporated in existing fog computing algorithms. Second, we present a framework we developed for co-optimizing task completion quality and latency, which was inspired by the insights of our empirical study. We describe fog computing task assignment problems we formulated under this framework, and present the algorithms we developed for solving them.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181102638G},
  archiveprefix = {arXiv},
  eid           = {arXiv:1811.02638},
  eprint        = {1811.02638},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1811.02638.pdf},
}

@Article{2019arXiv190308857G,
  author        = {{Gupta}, Vipul and {Kadhe}, Swanand and {Courtade}, Thomas and {Mahoney}, Michael W. and {Ramchandran}, Kannan},
  title         = {{OverSketched Newton: Fast Convex Optimization for Serverless Systems}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1903.08857},
  month         = {Mar},
  abstract      = {Motivated by recent developments in serverless systems for large-scale machine learning as well as improvements in scalable randomized matrix algorithms, we develop OverSketched Newton, a randomized Hessian-based optimization algorithm to solve large-scale smooth and strongly-convex problems in serverless systems. OverSketched Newton leverages matrix sketching ideas from Randomized Numerical Linear Algebra to compute the Hessian approximately. These sketching methods lead to inbuilt resiliency against stragglers that are a characteristic of serverless architectures. We establish that OverSketched Newton has a linear-quadratic convergence rate, and we empirically validate our results by solving large-scale supervised learning problems on real-world datasets. Experiments demonstrate a reduction of ~50% in total running time on AWS Lambda, compared to state-of-the-art distributed optimization schemes. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190308857G},
  archiveprefix = {arXiv},
  eid           = {arXiv:1903.08857},
  eprint        = {1903.08857},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Information Theory, Computer Science - Machine Learning},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1903.08857.pdf},
}

@Article{2018arXiv181102653G,
  author        = {{Gupta}, Vipul and {Wang}, Shusen and {Courtade}, Thomas and {Ramchand ran}, Kannan},
  title         = {{OverSketch: Approximate Matrix Multiplication for the Cloud}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1811.02653},
  month         = {Nov},
  abstract      = {We propose OverSketch, an approximate algorithm for distributed matrix multiplication in serverless computing. OverSketch leverages ideas from matrix sketching and high-performance computing to enable cost-efficient multiplication that is resilient to faults and straggling nodes pervasive in low-cost serverless architectures. We establish statistical guarantees on the accuracy of OverSketch and empirically validate our results by solving a large-scale linear program using interior-point methods and demonstrate a 34% reduction in compute time on AWS Lambda.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181102653G},
  archiveprefix = {arXiv},
  eid           = {arXiv:1811.02653},
  eprint        = {1811.02653},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Information Theory},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1811.02653.pdf},
}

@Article{2018arXiv181203651H,
  author        = {{Hellerstein}, Joseph M. and {Faleiro}, Jose and {Gonzalez}, Joseph E. and {Schleier-Smith}, Johann and {Sreekanti}, Vikram and {Tumanov}, Alexey and {Wu}, Chenggang},
  title         = {{Serverless Computing: One Step Forward, Two Steps Back}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1812.03651},
  month         = {Dec},
  abstract      = {Serverless computing offers the potential to program the cloud in an autoscaling, pay-as-you go manner. In this paper we address critical gaps in first-generation serverless computing, which place its autoscaling potential at odds with dominant trends in modern computing: notably data-centric and distributed computing, but also open source and custom hardware. Put together, these gaps make current serverless offerings a bad fit for cloud innovation and particularly bad for data systems innovation. In addition to pinpointing some of the main shortfalls of current serverless architectures, we raise a set of challenges we believe must be met to unlock the radical potential that the cloud---with its exabytes of storage and millions of cores---should offer to innovative developers. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181203651H},
  archiveprefix = {arXiv},
  eid           = {arXiv:1812.03651},
  eprint        = {1812.03651},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Databases},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1812.03651.pdf},
}

@Article{2017arXiv171008460I,
  author        = {{Ishakian}, Vatche and {Muthusamy}, Vinod and {Slominski}, Aleksander},
  title         = {{Serving deep learning models in a serverless platform}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1710.08460},
  month         = {Oct},
  abstract      = {Serverless computing has emerged as a compelling paradigm for the development and deployment of a wide range of event based cloud applications. At the same time, cloud providers and enterprise companies are heavily adopting machine learning and Artificial Intelligence to either differentiate themselves, or provide their customers with value added services. In this work we evaluate the suitability of a serverless computing environment for the inferencing of large neural network models. Our experimental evaluations are executed on the AWS Lambda environment using the MxNet deep learning framework. Our experimental results show that while the inferencing latency can be within an acceptable range, longer delays due to cold starts can skew the latency distribution and hence risk violating more stringent SLAs. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv171008460I},
  archiveprefix = {arXiv},
  eid           = {arXiv:1710.08460},
  eprint        = {1710.08460},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1710.08460.pdf},
}

@Article{2019arXiv190205870J,
  author        = {{Jangda}, Abhinav and {Pinckney}, Donald and {Baxter}, Samuel and {Devore-McDonald}, Breanna and {Spitzer}, Joseph and {Brun}, Yuriy and {Guha}, Arjun},
  title         = {{Formal Foundations of Serverless Computing}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1902.05870},
  month         = {Feb},
  abstract      = {A robust, large-scale web service can be difficult to engineer. When demand spikes, it must configure new machines and manage load-balancing; when demand falls, it must shut down idle machines to reduce costs; and when a machine crashes, it must quickly work around the failure without losing data. In recent years, serverless computing, a new cloud computing abstraction, has emerged to help address these challenges. In serverless computing, programmers write serverless functions, and the cloud platform transparently manages the operating system, resource allocation, load-balancing, and fault tolerance. In 2014, Amazon Web Services introduced the first serverless platform, AWS Lambda, and similar abstractions are now available on all major clouds. Unfortunately, the serverless computing abstraction exposes several low-level operational details that make it hard for programmers to write and reason about their code. This paper sheds light on this problem by presenting λλ, an operational semantics of the essence of serverless computing. Despite being a small core calculus (less than one column), λλ models all the low-level details that serverless functions can observe. To show that λλ is useful, we present three applications. First, to make it easier for programmers to reason about their code, we present a simplified semantics of serverless execution and precisely characterize when the simplified semantics and λλ coincide. Second, we augment λλ with a key-value store, which allows us to reason about stateful serverless functions. Third, since a handful of serverless platforms support serverless function composition, we show how to extend λλ with a composition language. We have implemented this composition language and show that it outperforms prior work. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190205870J},
  archiveprefix = {arXiv},
  eid           = {arXiv:1902.05870},
  eprint        = {1902.05870},
  keywords      = {Computer Science - Programming Languages},
  primaryclass  = {cs.PL},
  url           = {https://arxiv.org/pdf/1902.05870.pdf},
}

@Article{2019arXiv190203383J,
  author        = {{Jonas}, Eric and {Schleier-Smith}, Johann and {Sreekanti}, Vikram and {Tsai}, Chia-Che and {Khandelwal}, Anurag and {Pu}, Qifan and {Shankar}, Vaishaal and {Carreira}, Joao and {Krauth}, Karl and {Yadwadkar}, Neeraja and {Gonzalez}, Joseph E. and {Popa}, Raluca Ada and {Stoica}, Ion and {Patterson}, David A.},
  title         = {{Cloud Programming Simplified: A Berkeley View on Serverless Computing}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1902.03383},
  month         = {Feb},
  abstract      = {Serverless cloud computing handles virtually all the system administration operations needed to make it easier for programmers to use the cloud. It provides an interface that greatly simplifies cloud programming, and represents an evolution that parallels the transition from assembly language to high-level programming languages. This paper gives a quick history of cloud computing, including an accounting of the predictions of the 2009 Berkeley View of Cloud Computing paper, explains the motivation for serverless computing, describes applications that stretch the current limits of serverless, and then lists obstacles and research opportunities required for serverless computing to fulfill its full potential. Just as the 2009 paper identified challenges for the cloud and predicted they would be addressed and that cloud use would accelerate, we predict these issues are solvable and that serverless computing will grow to dominate the future of cloud computing. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190203383J},
  archiveprefix = {arXiv},
  eid           = {arXiv:1902.03383},
  eprint        = {1902.03383},
  keywords      = {Computer Science - Operating Systems},
  primaryclass  = {cs.OS},
  url           = {https://arxiv.org/pdf/1902.03383.pdf},
}

@Article{2019arXiv190109842K,
  author        = {{Kesidis}, George},
  title         = {{Temporal Overbooking of Lambda Functions in the Cloud}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.09842},
  month         = {Jan},
  abstract      = {We consider the problem of scheduling "serverless computing" instances such as Amazon Lambda functions. Instead of a quota per tenant/customer, we assume demand for Lambda functions is modulated by token-bucket mechanisms per tenant. Based on an upper bound on the stationary number of active "Lambda servers" considering the execution-time distribution of Lambda functions, we describe an approach that the cloud could use to overbook Lambda functions for improved utilization of IT resources. An earlier bound for a single service tier is extended to the case of multiple service tiers.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190109842K},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.09842},
  eprint        = {1901.09842},
  keywords      = {Computer Science - Performance},
  primaryclass  = {cs.PF},
  url           = {https://arxiv.org/pdf/1901.09842.pdf},
}

@Article{2018arXiv180306354K,
  author        = {{Kim}, Youngbin and {Lin}, Jimmy},
  title         = {{Serverless Data Analytics with Flint}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1803.06354},
  month         = {Mar},
  abstract      = {Serverless architectures organized around loosely-coupled function invocations represent an emerging design for many applications. Recent work mostly focuses on user-facing products and event-driven processing pipelines. In this paper, we explore a completely different part of the application space and examine the feasibility of analytical processing on big data using a serverless architecture. We present Flint, a prototype Spark execution engine that takes advantage of AWS Lambda to provide a pure pay-as-you-go cost model. With Flint, a developer uses PySpark exactly as before, but without needing an actual Spark cluster. We describe the design, implementation, and performance of Flint, along with the challenges associated with serverless analytics. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180306354K},
  archiveprefix = {arXiv},
  eid           = {arXiv:1803.06354},
  eprint        = {1803.06354},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Databases},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1803.06354.pdf},
}

@Article{2019arXiv190103984L,
  author        = {{Lavoie}, Samuel and {Garant}, Anthony and {Petrillo}, Fabio},
  title         = {{Serverless architecture efficiency: an exploratory study}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.03984},
  month         = {Jan},
  abstract      = {Cloud service provider propose services to insensitive customers to use their platform. Different services can achieve the same result at different cost. In this paper, we study the efficiency of a serverless architecture for running highly parallelizable tasks to compare theses services in order to find the most efficient in term of performance and cost. More precisely, we look at the compute time and at the cost per task for a given task. The tasks studied is the count of the occurrence of a given word in a corpus. We compare the serverless architecture to the Apache Spark map reduce technique commonly used for this type of task. Using AWS Lambda for the serverless architecture and Amazon EMR for the Apache Spark map reduce, with similar compute power, we show that the serverless technique achieve comparable performance in term of compute time and cost. We observed that the lambda function is a great approach for real time computing, while EMR is preferable for task that require long compute time. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190103984L},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.03984},
  eprint        = {1901.03984},
  keywords      = {Computer Science - Software Engineering, Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/pdf/1901.03984.pdf},
}

@Article{2019arXiv190312221L,
  author        = {{Lin}, Ping-Min and {Glikson}, Alex},
  title         = {{Mitigating Cold Starts in Serverless Platforms: A Pool-Based Approach}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1903.12221},
  month         = {Mar},
  abstract      = {Rapid adoption of the serverless (or Function-as-a-Service, FaaS) paradigm, pioneered by Amazon with AWS Lambda and followed by numerous commercial offerings and open source projects, introduces new challenges in designing the cloud infrastructure, balancing between performance and cost. While instant per-request elasticity that FaaS platforms typically offer application developers makes it possible to achieve high performance of bursty workloads without over-provisioning, such elasticity often involves extra latency associated with on-demand provisioning of individual runtime containers that serve the functions. This phenomenon is often called cold starts, as opposed to the situation when a function is served by a pre-provisioned "warm" container, ready to serve requests with close to zero overhead. Providers are constantly working on techniques aimed at reducing cold starts. A common approach to reduce cold starts is to maintain a pool of warm containers, in anticipation of future requests. In this report, we address the cold start problem in serverless architectures, specifically under the Knative Serving FaaS platform. We describe our implementation leveraging a pool of function instances, and evaluate the latency compared to the original implementation, resulting in a 85% reduction of P99 response time for a single instance pool. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190312221L},
  archiveprefix = {arXiv},
  eid           = {arXiv:1903.12221},
  eprint        = {1903.12221},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://ui.adsabs.harvard.edu/link_gateway/2019arXiv190312221L/EPRINT_PDF},
}

@ARTICLE{2019arXiv190507228P,
author = {{Pellegrini}, Roland and {Ivkic}, Igor and {Tauber}, Markus},
title = "{Towards a Security-Aware Benchmarking Framework for Function-as-a-Service}",
journal = {arXiv e-prints},
keywords = {Computer Science - Software Engineering, Computer Science - Cryptography and Security},
year = "2019",
month = "May",
eid = {arXiv:1905.07228},
pages = {arXiv:1905.07228},
archivePrefix = {arXiv},
eprint = {1905.07228},
primaryClass = {cs.SE},
adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190507228P},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019arXiv190511707P,
author = {{Pellegrini}, Roland and {Ivkic}, Igor and {Tauber}, Markus},
title = "{Function-as-a-Service Benchmarking Framework}",
journal = {arXiv e-prints},
keywords = {Computer Science - Performance, Computer Science - Distributed, Parallel, and Cluster Computing},
year = "2019",
month = "May",
eid = {arXiv:1905.11707},
pages = {arXiv:1905.11707},
archivePrefix = {arXiv},
eprint = {1905.11707},
primaryClass = {cs.PF},
adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190511707P},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@Article{2018arXiv180703755P,
  author        = {{Pinto}, Duarte and {Dias}, Jo{\~a}o Pedro and {Sereno Ferreira}, Hugo},
  title         = {{Dynamic Allocation of Serverless Functions in IoT Environments}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1807.03755},
  month         = {Jul},
  abstract      = {The IoT area has grown significantly in the last few years and is expected to reach a gigantic amount of 50 billion devices by 2020. The appearance of serverless architectures, specifically highlighting FaaS, raises the question of the of using such in IoT environments. Combining IoT with a serverless architectural design can be effective when trying to make use of the local processing power that exists in a local network of IoT devices and creating a fog layer that leverages computational capabilities that are closer to the end-user. In this approach, which is placed between the device and the serverless function, when a device requests for the execution of a serverless function will decide based on previous metrics of execution if the serverless function should be executed locally, in the fog layer of a local network of IoT devices, or if it should be executed remotely, in one of the available cloud servers. Therefore, this approach allows to dynamically allocating functions to the most suitable layer. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180703755P},
  archiveprefix = {arXiv},
  eid           = {arXiv:1807.03755},
  eprint        = {1807.03755},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1807.03755.pdf},
}

@Article{2018arXiv181009679S,
  author        = {{Shankar}, Vaishaal and {Krauth}, Karl and {Pu}, Qifan and {Jonas}, Eric and {Venkataraman}, Shivaram and {Stoica}, Ion and {Recht}, Benjamin and {Ragan-Kelley}, Jonathan},
  title         = {{numpywren: serverless linear algebra}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1810.09679},
  month         = {Oct},
  abstract      = {Linear algebra operations are widely used in scientific computing and machine learning applications. However, it is challenging for scientists and data analysts to run linear algebra at scales beyond a single machine. Traditional approaches either require access to supercomputing clusters, or impose configuration and cluster management challenges. In this paper we show how the disaggregation of storage and compute resources in so-called "serverless" environments, combined with compute-intensive workload characteristics, can be exploited to achieve elastic scalability and ease of management. We present numpywren, a system for linear algebra built on a serverless architecture. We also introduce LAmbdaPACK, a domain-specific language designed to implement highly parallel linear algebra algorithms in a serverless setting. We show that, for certain linear algebra algorithms such as matrix multiply, singular value decomposition, and Cholesky decomposition, numpywren's performance (completion time) is within 33% of ScaLAPACK, and its compute efficiency (total CPU-hours) is up to 240% better due to elasticity, while providing an easier to use interface and better fault tolerance. At the same time, we show that the inability of serverless runtimes to exploit locality across the cores in a machine fundamentally limits their network efficiency, which limits performance on other algorithms such as QR factorization. This highlights how cloud providers could better support these types of computations through small changes in their infrastructure.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181009679S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1810.09679},
  eprint        = {1810.09679},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1810.09679.pdf},
}

@Article{2017arXiv170508169S,
  author        = {{Spillner}, Josef},
  title         = {{Transformation of Python Applications into Function-as-a-Service Deployments}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1705.08169},
  month         = {May},
  abstract      = {New cloud programming and deployment models pose challenges to software application engineers who are looking, often in vain, for tools to automate any necessary code adaptation and transformation. Function-as-a-Service interfaces are particular non-trivial targets when considering that most cloud applications are implemented in non-functional languages. Among the most widely used of these languages is Python. This starting position calls for an automated approach to transform monolithic Python code into modular FaaS units by partially automated decomposition. Hence, this paper introduces and evaluates Lambada, a Python module to dynamically decompose, convert and deploy unmodified Python code into AWS Lambda functions. Beyond the tooling in the form of a measured open source prototype implementation, the paper contributes a description of the algorithms and code rewriting rules as blueprints for transformations of other scripting languages. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170508169S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1705.08169},
  eprint        = {1705.08169},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Software Engineering, D.2.1, I.2.2, C.2.4},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1705.08169.pdf},
}

@Article{2017arXiv170307562S,
  author        = {{Spillner}, Josef},
  title         = {{Snafu: Function-as-a-Service (FaaS) Runtime Design and Implementation}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1703.07562},
  month         = {Mar},
  abstract      = {Snafu, or Snake Functions, is a modular system to host, execute and manage language-level functions offered as stateless (micro-)services to diverse external triggers. The system interfaces resemble those of commercial FaaS providers but its implementation provides distinct features which make it overall useful to research on FaaS and prototyping of FaaS-based applications. This paper argues about the system motivation in the presence of already existing alternatives, its design and architecture, the open source implementation and collected metrics which characterise the system. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170307562S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1703.07562},
  eprint        = {1703.07562},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, C.2.4, H.3.5, D.1.1},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1703.07562.pdf},
}

@ARTICLE{2019arXiv190504800S,
author = {{Spillner}, Josef},
title = "{Quantitative Analysis of Cloud Function Evolution in the AWS Serverless Application Repository}",
journal = {arXiv e-prints},
keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, D.2.8, C.2.4, K.6.3},
year = "2019",
month = "May",
eid = {arXiv:1905.04800},
pages = {arXiv:1905.04800},
archivePrefix = {arXiv},
eprint = {1905.04800},
primaryClass = {cs.DC},
adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190504800S},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@Article{2017arXiv170205510S,
  author        = {{Spillner}, Josef and {Dorodko}, Serhii},
  title         = {{Java Code Analysis and Transformation into AWS Lambda Functions}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1702.05510},
  month         = {Feb},
  abstract      = {Software developers are faced with the issue of either adapting their programming model to the execution model (e.g. cloud platforms) or finding appropriate tools to adapt the model and code automatically. A recent execution model which would benefit from automated enablement is Function-as-a-Service. Automating this process requires a pipeline which includes steps for code analysis, transformation and deployment. In this paper, we outline the design and runtime characteristics of Podilizer, a tool which implements the pipeline specifically for Java source code as input and AWS Lambda as output. We contribute technical and economic metrics about this concrete 'FaaSification' process by observing the behaviour of Podilizer with two representative Java software projects.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170205510S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1702.05510},
  eprint        = {1702.05510},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Software Engineering, D.2.1, I.2.2, C.2.4},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1702.05510.pdf},
}

@Article{2019arXiv190103086S,
  author        = {{Stein}, Manuel},
  title         = {{Adaptive Event Dispatching in Serverless Computing Infrastructures}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.03086},
  month         = {Jan},
  abstract      = {Serverless computing is an emerging Cloud service model. It is currently gaining momentum as the next step in the evolution of hosted computing from capacitated machine virtualisation and microservices towards utility computing. The term "serverless" has become a synonym for the entirely resource-transparent deployment model of cloud-based event-driven distributed applications. This work investigates how adaptive event dispatching can improve serverless platform resource efficiency and contributes a novel approach that allows for better scaling and fitting of the platform's resource consumption to actual demand. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190103086S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.03086},
  eprint        = {1901.03086},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1901.03086.pdf},
}

@Article{2019arXiv190102680S,
  author        = {{Stein}, Manuel},
  title         = {{Interim Report on Adaptive Event Dispatching in Serverless Computing Infrastructures}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.02680},
  month         = {Jan},
  abstract      = {Serverless computing is an emerging service model in distributed computing systems. The term captures cloud-based event-driven distributed application design and stems from its completely resource-transparent deployment model, i.e. serverless. This work thesisizes that adaptive event dispatching can improve current serverless platform resource efficiency by considering locality and dependencies. These design contemplations have also been formulated by Hendrickson et al., which identifies the requirement that "Serverless load balancers must make low-latency decisions while considering session, code and data locality". This interim report investigates the economical importance of the emerging trend and asserts that existing serverless platforms still do not optimize for data locality, whereas a variety of scheduling methods are available from distributed computing research which have proven to increase resource efficiency. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190102680S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.02680},
  eprint        = {1901.02680},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1901.02680.pdf},
}

@Article{2018arXiv180906100S,
  author        = {{Stein}, Manuel},
  title         = {{The Serverless Scheduling Problem and NOAH}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1809.06100},
  month         = {Sep},
  abstract      = {The serverless scheduling problem poses a new challenge to Cloud service platform providers because it is rather a job scheduling problem than a traditional resource allocation or request load balancing problem. Traditionally, elastic cloud applications use managed virtual resource allocation and employ request load balancers to orchestrate the deployment. With serverless, the provider needs to solve both the load balancing and the allocation. This work reviews the current Apache OpenWhisk serverless event load balancing and a noncooperative game-theoretic load balancing approach for response time minimization in distributed systems. It is shown by simulation that neither performs well under high system utilization which inspired a noncooperative online allocation heuristic that allows tuning the trade-off between for response time and resource cost of each serverless function. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180906100S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1809.06100},
  eprint        = {1809.06100},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1809.06100.pdf},
}

@Article{2018arXiv180600764V,
  author        = {{Vaquero}, Luis M. and {Cuadrado}, Felix and {Elkhatib}, Yehia and {Bernal-Bernabe}, Jorge and {Srirama}, Satish N. and {Faten Zhani}, Mohamed},
  title         = {{Research Challenges in Nextgen Service Orchestration}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1806.00764},
  month         = {Jun},
  abstract      = {Fog/edge computing, function as a service, and programmable infrastructures, like software-defined networking or network function virtualisation, are becoming ubiquitously used in modern Information Technology infrastructures. These technologies change the characteristics and capabilities of the underlying computational substrate where services run (e.g. higher volatility, scarcer computational power, or programmability). As a consequence, the nature of the services that can be run on them changes too (smaller codebases, more fragmented state, etc.). These changes bring new requirements for service orchestrators, which need to evolve so as to support new scenarios where a close interaction between service and infrastructure becomes essential to deliver a seamless user experience. Here, we present the challenges brought forward by this new breed of technologies and where current orchestration techniques stand with regards to the new challenges. We also present a set of promising technologies that can help tame this brave new world. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180600764V},
  archiveprefix = {arXiv},
  eid           = {arXiv:1806.00764},
  eprint        = {1806.00764},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1806.00764.pdf},
}

@Article{2016arXiv160708508W,
  author        = {{Wagner}, Brandon and {Sood}, Arun},
  title         = {{Economics of Resilient Cloud Services}},
  journal       = {arXiv e-prints},
  year          = {2016},
  pages         = {arXiv:1607.08508},
  month         = {Jul},
  abstract      = {Computer systems today must meet and maintain service availability, performance, and security requirements. Each of these demands requires redundancy and some form of isolation. When service requirements are implemented separately, the system architecture cannot easily share common components of redundancy and isolation. We will present these service traits collectively as cyber resilience with a system called Self Cleansing Intrusion Tolerance or SCIT. Further, we will demonstrate that SCIT provides an effective resilient cloud implementation making cost effective utilization of cloud excess capacity and economies of scale. Lastly, we will introduce the notion of serverless applications utilizing AWS Lambda and how a stateless architecture can drastically reduce operational costs by utilizing cloud function services.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv160708508W},
  archiveprefix = {arXiv},
  eid           = {arXiv:1607.08508},
  eprint        = {1607.08508},
  keywords      = {Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/ftp/arxiv/papers/1607/1607.08508.pdf},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveOrderConfig:specified;author;false;abstract;false;abstract;false;}
