% Encoding: UTF-8

@InProceedings{Abad:2018:PSF:3185768.3186294,
  author    = {Abad, Cristina L. and Boza, Edwin F. and van Eyk, Erwin},
  title     = {Package-Aware Scheduling of FaaS Functions},
  booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
  year      = {2018},
  series    = {ICPE '18},
  pages     = {101--106},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3186294},
  doi       = {10.1145/3185768.3186294},
  isbn      = {978-1-4503-5629-9},
  keywords  = {cloud computing, functions-as-a-service, load balancing, scheduling, serverless computing},
  location  = {Berlin, Germany},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3185768.3186294},
}
@INPROCEEDINGS{8725610,
author={U. {Acar} and R. F. {Ustok} and S. {Keskin} and D. {Breitgand} and A. {Weit}},
booktitle={2018 IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN)},
title={Programming Tools for Rapid NFV-Based Media Application Development in 5G Networks},
year={2018},
volume={},
number={},
pages={1-5},
abstract={The emergence of virtualisation and Infrastructure-as-a-Service (IaaS) have dramatically transformed the telecom industry through network function virtualisation (NFV). A recently introduced cloud-native concept, Platform as a Service (PaaS), ensures to further boost the performance, portability and cost efficiency of the NFV. The 5G-MEDIA project proposes the application of a serverless paradigm known as Function-as-a- Service (FaaS) to NFV for the media applications exploiting the 5G technologies. In addition to integration of FaaS, the 5G-MEDIA application/service development kit (SDK) supports microservice-based application development for both hypervisor-based and containerized approaches, specifically supporting Docker, unikernel and LXC. In this paper, we provide an overview of the 5G-MEDIA SDK which is built to support NFV-based next generation media applications and to achieve a development time in the order of minutes. Furthermore, implementations of FaaS Emulation and FaaS command line interface (CLI) tools are also presented.},
keywords={Tools;Media;FAA;5G mobile communication;Emulation;Containers;Computer architecture;5G;5G MEDIA;NFV;SDK},
doi={10.1109/NFV-SDN.2018.8725610},
ISSN={},
month={Nov},}

@Article{8653379,
  author   = {P. {Aditya} and I. E. {Akkus} and A. {Beck} and R. {Chen} and V. {Hilt} and I. {Rimac} and K. {Satzke} and M. {Stein}},
  title    = {Will Serverless Computing Revolutionize NFV?},
  journal  = {Proceedings of the IEEE},
  year     = {2019},
  volume   = {107},
  number   = {4},
  pages    = {667-678},
  month    = {April},
  issn     = {0018-9219},
  abstract = {Communication networks need to be both adaptive and scalable. The last few years have seen an explosive growth of software-defined networking (SDN) and network function virtualization (NFV) to address this need. Both technologies help enable networking software to be decoupled from the hardware so that software functionality is no longer constrained by the underlying hardware and can evolve independently. Both SDN and NFV aim to advance a software-based approach to networking, where networking functionality is implemented in software modules and executed on a suitable cloud computing platform. Achieving this goal requires the virtualization paradigm used in these services that play an important role in the transition to software-based networks. Consequently, the corresponding computing platforms accompanying the virtualization technologies need to provide the required agility, robustness, and scalability for the services executed. Serverless computing has recently emerged as a new paradigm in virtualization and has already significantly changed the economics of offloading computations to the cloud. It is considered as a low-latency, resource-efficient, and rapidly deployable alternative to traditional virtualization approaches, such as those based on virtual machines and containers. Serverless computing provides scalability and cost reduction, without requiring any additional configuration overhead on the part of the developer. In this paper, we explore and survey how serverless computing technology can help building adaptive and scalable networks and show the potential pitfalls of doing so.},
  doi      = {10.1109/JPROC.2019.2898101},
  keywords = {Communication netwowrks;Cloud computing;Virtualization;Hardware;Servers;Edge computing;Network function virtualization;Scalability;Application virtualization;cloud computing;edge computing;network function virtualization (NFV);serverless computing;software-defined networking (SDN)},
}

@InProceedings{Adzic:2017:SCE:3106237.3117767,
  author    = {Adzic, Gojko and Chatley, Robert},
  title     = {Serverless Computing: Economic and Architectural Impact},
  booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
  year      = {2017},
  series    = {ESEC/FSE 2017},
  pages     = {884--889},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3117767},
  doi       = {10.1145/3106237.3117767},
  isbn      = {978-1-4503-5105-8},
  keywords  = {Cloud Computing, Economics, Serverless},
  location  = {Paderborn, Germany},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3106237.3117767},
}

@InProceedings{Akkus:2018:STH:3277355.3277444,
  author    = {Akkus, Istemi Ekin and Chen, Ruichuan and Rimac, Ivica and Stein, Manuel and Satzke, Klaus and Beck, Andre and Aditya, Paarijaat and Hilt, Volker},
  title     = {SAND: Towards High-performance Serverless Computing},
  booktitle = {Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference},
  year      = {2018},
  series    = {USENIX ATC '18},
  pages     = {923--935},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3277444},
  isbn      = {978-1-931971-44-7},
  location  = {Boston, MA, USA},
  numpages  = {13},
  url       = {http://dl.acm.org/citation.cfm?id=3277355.3277444},
}

@InProceedings{8457832,
  author    = {Z. {Al-Ali} and S. {Goodarzy} and E. {Hunter} and S. {Ha} and R. {Han} and E. {Keller} and E. {Rozner}},
  title     = {Making Serverless Computing More Serverless},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {456-459},
  month     = {July},
  abstract  = {In serverless computing, developers define a function to handle an event, and the serverless framework horizontally scales the application as needed. The downside of this function-based abstraction is it limits the type of application supported and places a bound on the function to be within the physical resource limitations of the server the function executes on. In this paper we propose a new abstraction for serverless computing: a developer supplies a process and the serverless framework seamlessly scales out the process's resource usage across the datacenter. This abstraction enables processing to not only be more general purpose, but also allows a process to break out of the limitations of a single server - making serverless computing more serverless. To realize this abstraction, we propose ServerlessOS, comprised of three key components: (i) a new disaggregation model, which leverages disaggregation for abstraction, but enables resources to move fluidly between servers for performance; (ii) a cloud orchestration layer which manages fine-grained resource allocation and placement throughout the application's lifetime via local and global decision making; and (iii) an isolation capability that enforces data and resource isolation across disaggregation, effectively extending Linux cgroup functionality to span servers.},
  doi       = {10.1109/CLOUD.2018.00064},
  issn      = {2159-6190},
  keywords  = {cloud computing;computer centres;decision making;Linux;resource allocation;serverless computing;function-based abstraction;physical resource limitations;resource usage;datacenter;disaggregation model;ServerlessOS;cloud orchestration layer;fine-grained resource allocation;global decision making;local decision making;isolation capability;disaggregation;Linux cgroup functionality;Servers;Instruction sets;Cloud computing;Sockets;Couplings;Micromechanical devices;Memory management;serverless;cloud;virtualization;isolation;orchestration;resource disaggregation},
}

@Article{2018arXiv181006080A,
  author        = {{Alder}, Fritz and {Asokan}, N. and {Kurnikov}, Arseny and {Paverd}, Andrew and {Steiner}, Michael},
  title         = {{S-FaaS: Trustworthy and Accountable Function-as-a-Service using Intel SGX}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1810.06080},
  month         = {Oct},
  abstract      = {Function-as-a-Service (FaaS) is a recent and already very popular paradigm in cloud computing. The function provider need only specify the function to be run, usually in a high-level language like JavaScript, and the service provider orchestrates all the necessary infrastructure and software stacks. The function provider is only billed for the actual computational resources used by the function invocation. Compared to previous cloud paradigms, FaaS requires significantly more fine-grained resource measurement mechanisms, e.g. to measure compute time and memory usage of a single function invocation with sub-second accuracy. Thanks to the short duration and stateless nature of functions, and the availability of multiple open-source frameworks, FaaS enables non-traditional service providers e.g. individuals or data centers with spare capacity. However, this exacerbates the challenge of ensuring that resource consumption is measured accurately and reported reliably. It also raises the issues of ensuring computation is done correctly and minimizing the amount of information leaked to service providers. To address these challenges, we introduce S-FaaS, the first architecture and implementation of FaaS to provide strong security and accountability guarantees backed by Intel SGX. To match the dynamic event-driven nature of FaaS, our design introduces a new key distribution enclave and a novel transitive attestation protocol. A core contribution of S-FaaS is our set of resource measurement mechanisms that securely measure compute time inside an enclave, and actual memory allocations. We have integrated S-FaaS into the popular OpenWhisk FaaS framework. We evaluate the security of our architecture, the accuracy of our resource measurement mechanisms, and the performance of our implementation, showing that our resource measurement mechanisms add less than 6.3% latency on standardized benchmarks. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181006080A},
  archiveprefix = {arXiv},
  eid           = {arXiv:1810.06080},
  eprint        = {1810.06080},
  keywords      = {Computer Science - Cryptography and Security},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/pdf/1810.06080.pdf},
}

@InProceedings{8622117,
  author    = {E. {Al-Masri} and I. {Diabate} and R. {Jain} and M. H. {Lam} and S. {Reddy Nathala}},
  title     = {Recycle.io: An IoT-Enabled Framework for Urban Waste Management},
  booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
  year      = {2018},
  pages     = {5285-5287},
  month     = {Dec},
  abstract  = {Addressing environmentally safe management of waste is becoming increasingly a challenging task. The predicament of the rate at which waste is generated due to increasing populations is also contributing to this challenge. One possible approach for effectively handling waste can be achieved by source reduction and recycling. The problem, however, improving the collection of waste can be costly particularly during the source separation process after waste is collected. It would be desirable if there exists a mechanism that can help municipalities, local governments or waste management companies to monitor in real-time sources of violations prior to the waste collection process. In this paper, we introduce recycle.io, an Internet of Things (IoT)-enabled waste management system that is based on a serverless architecture that can identify these sources of violations. Using recycle.io, it is then possible to track the violations geographically which can help local governments, for example, to improve or enforce tighter regulations for waste disposal. Our recycle.io system uses Microsoft Azure IoT Hub for device management. Throughout the paper, we demonstrate usefulness of using our approach for urban waste management in smart cities.},
  doi       = {10.1109/BigData.2018.8622117},
  keywords  = {environmental science computing;Internet of Things;recycling;waste disposal;Recycle.io;Internet of Things;waste disposal;waste collection process;recycling;urban waste management;IoT-enabled framework;Image edge detection;Waste management;Real-time systems;Cloud computing;Sensors;Edge computing;Hardware;IoT devices;IoT gateways;waste management;industrial internet of things;IIoT;garbage collection;smart bin;smart garbage;smart city},
}

@InProceedings{8539123,
  author    = {E. {Al-Masri} and I. {Diabate} and R. {Jain} and M. H. L. {Lam} and S. R. {Nathala}},
  title     = {A Serverless IoT Architecture for Smart Waste Management Systems},
  booktitle = {2018 IEEE International Conference on Industrial Internet (ICII)},
  year      = {2018},
  pages     = {179-180},
  month     = {Oct},
  abstract  = {In recent years, the waste management and recycling industry has been facing multiple challenges such as protecting the public from harmful hazardous waste, promoting recycling and material source separation into common material streams. For instance, separating waste of hazardous wastes such as paint or batteries can be costly during the source separation process after waste is collected. What is therefore needed is a smart waste management system that is capable of identifying waste materials prior to the separation process. To overcome these challenges and more, we introduce recycle.io, a serverless Internet of Things (IoT) architecture for smart waste management systems. Using recycle.io, it is then possible to determine in real-time the types of source material violations prior to the waste collection. In this manner, waste management systems can identify sources of violations and rectify this by bringing awareness to the public or issuing fines to prevent violations from occurring. We demonstrate usefulness of our approach throughout the paper.},
  doi       = {10.1109/ICII.2018.00034},
  keywords  = {distributed processing;hazardous materials;Internet of Things;pollution control;real-time systems;recycling;separation;source separation process;smart waste management system;recycle.io;source material violations;waste collection;serverless IoT architecture;recycling industry;harmful hazardous waste;material source separation;public protection;real-time system;Waste management;Recycling;Source separation;Real-time systems;Internet of Things;Image edge detection;Cloud computing;IIoT;IoT devices;Waste Management;Industrial Internet;Garbage Collection;Smart Bin;Smart Garbage;Smart City},
}

@Article{Alpernas:2018:SSC:3288538.3276488,
  author     = {Alpernas, Kalev and Flanagan, Cormac and Fouladi, Sadjad and Ryzhyk, Leonid and Sagiv, Mooly and Schmitz, Thomas and Winstein, Keith},
  title      = {Secure Serverless Computing Using Dynamic Information Flow Control},
  journal    = {Proc. ACM Program. Lang.},
  year       = {2018},
  volume     = {2},
  number     = {OOPSLA},
  pages      = {118:1--118:26},
  month      = oct,
  issn       = {2475-1421},
  acmid      = {3276488},
  address    = {New York, NY, USA},
  articleno  = {118},
  doi        = {10.1145/3276488},
  issue_date = {November 2018},
  keywords   = {Cloud Computing, Information Flow Control, Serverless},
  numpages   = {26},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3276488},
}

@Article{8667014,
  author   = {F. {Alvarez} and D. {Breitgand} and D. {Griffin} and P. {Andriani} and S. {Rizou} and N. {Zioulis} and F. {Moscatelli} and J. {Serrano} and M. {Keltsch} and P. {Trakadas} and T. K. {Phan} and A. {Weit} and U. {Acar} and O. {Prieto} and F. {Iadanza} and G. {Carrozzo} and H. {Koumaras} and D. {Zarpalas} and D. {Jimenez}},
  title    = {An Edge-to-Cloud Virtualized Multimedia Service Platform for 5G Networks},
  journal  = {IEEE Transactions on Broadcasting},
  year     = {2019},
  pages    = {1-12},
  issn     = {0018-9316},
  abstract = {The focus of research into 5G networks to date has been largely on the required advances in network architectures, technologies, and infrastructures. Less effort has been put on the applications and services that will make use of and exploit the flexibility of 5G networks built upon the concept of software-defined networking (SDN) and network function virtualization (NFV). Media-based applications are amongst the most demanding services, requiring large bandwidths for high audio-visual quality, low-latency for interactivity, and sufficient infrastructure resources to deliver the computational power for running the media applications in the networked cloud. This paper presents a novel service virtualization platform (SVP), called 5G-MEDIA SVP, which leverages the principles of NFV and SDN to facilitate the development, deployment, and operation of media services on 5G networks. The platform offers an advanced cognitive management environment for the provisioning of network services (NSs) and media-related applications, which directly link their lifecycle management with user experience as well as optimization of infrastructure resource utilization. Another innovation of 5G-MEDIA SVP is the integration of serverless computing with media intensive applications in 5G networks, increasing cost effectiveness of operation and simplifying development and deployment time. The proposed SVP is being validated against three media use cases: 1) immersive virtual reality 3-D gaming application; 2) remote production of broadcast content incorporating user generated contents; and 3) dynamically adaptive content distribution networks for the intelligent distribution of ultrahigh definition content. The preliminary results of the 5G-MEDIA SVP platform evaluation are compared against current practice and show that the proposed platform provides enhanced functionality for the operators and infrastructure owners, while ensuring better NS performance to service providers and end users.},
  doi      = {10.1109/TBC.2019.2901400},
  keywords = {5G mobile communication;Media;Computer architecture;Cloud computing;Streaming media;Tools;Bandwidth;5G networks;network functions virtualization;serverless computing;immersive media;remote production;content delivery networks},
}

@InProceedings{Ao:2018:SSV:3267809.3267815,
  author    = {Ao, Lixiang and Izhikevich, Liz and Voelker, Geoffrey M. and Porter, George},
  title     = {Sprocket: A Serverless Video Processing Framework},
  booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
  year      = {2018},
  series    = {SoCC '18},
  pages     = {263--274},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3267815},
  doi       = {10.1145/3267809.3267815},
  isbn      = {978-1-4503-6011-1},
  location  = {Carlsbad, CA, USA},
  numpages  = {12},
  url       = {http://doi.acm.org/10.1145/3267809.3267815},
}

@InProceedings{8622913,
  author    = {T. {Asghar} and S. {Rasool} and M. {Iqbal} and Z. u. {Qayyum} and A. N. {Mian} and G. {Ubakanma}},
  title     = {Feasibility of Serverless Cloud Services for Disaster Management Information Systems},
  booktitle = {2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
  year      = {2018},
  pages     = {1054-1057},
  month     = {June},
  abstract  = {Serverless is the new generation of cloud services that supports the pay-per-use policy in true spirit by charging only for the execution time of the hosted code. Amazon introduced serverless service of Lambda in 2014 and it is consider as the most popular serverless cloud service till date. This paper focuses on the serverless cloud services of Lambda and elaborates the importance of Lambda based serverless cloud services for hosting the disaster management information systems (DMIS). We have identified two repeatedly occurring phases of the life cycle of a DMIS. These phases are high activity phase and low activity phase. Our findings state that serverless cloud services are well-suited for both of these phases of a DMIS. Serverless reduce the operational cost during the low activity phase by detaching the code from running containers and it improves the scalability during the high activity phase by quickly assigning the already available containers from the container pool. However, this all comes with the price of reduced QoS for initial requests and our experimental results reflect the extent of this quality degradation.},
  doi       = {10.1109/HPCC/SmartCity/DSS.2018.00175},
  keywords  = {cloud computing;emergency management;information systems;quality of service;pay-per-use policy;Amazon;DMIS;operational cost reduction;scalability;QoS;quality degradation;Lambda based serverless cloud services;disaster management information systems;Containers;Quality of service;Cloud computing;Virtualization;Scalability;Color;Degradation;FaaS;Serverless;Lambda;QoS;DMIS;Disaster},
}

@InProceedings{Aske:2018:SMS:3229710.3229742,
  author    = {Aske, Austin and Zhao, Xinghui},
  title     = {Supporting Multi-Provider Serverless Computing on the Edge},
  booktitle = {Proceedings of the 47th International Conference on Parallel Processing Companion},
  year      = {2018},
  series    = {ICPP '18},
  pages     = {20:1--20:6},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3229742},
  articleno = {20},
  doi       = {10.1145/3229710.3229742},
  isbn      = {978-1-4503-6523-9},
  keywords  = {Edge Computing, Function-as-a-Service, Performance, Scheduling, Serverless Computing},
  location  = {Eugene, OR, USA},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3229710.3229742},
}

@InProceedings{Ast:2017:SWC:3154847.3154849,
  author    = {Ast, Markus and Gaedke, Martin},
  title     = {Self-contained Web Components Through Serverless Computing},
  booktitle = {Proceedings of the 2Nd International Workshop on Serverless Computing},
  year      = {2017},
  series    = {WoSC '17},
  pages     = {28--33},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3154849},
  doi       = {10.1145/3154847.3154849},
  isbn      = {978-1-4503-5434-9},
  keywords  = {end-user development, function as a service, micro services, serverless computing, web components, web services},
  location  = {Las Vegas, Nevada},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3154847.3154849},
}

@Article{2019arXiv190103161A,
  author        = {{Aytekin}, Arda and {Johansson}, Mikael},
  title         = {{Harnessing the Power of Serverless Runtimes for Large-Scale Optimization}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.03161},
  month         = {Jan},
  abstract      = {The event-driven and elastic nature of serverless runtimes makes them a very efficient and cost-effective alternative for scaling up computations. So far, they have mostly been used for stateless, data parallel and ephemeral computations. In this work, we propose using serverless runtimes to solve generic, large-scale optimization problems. Specifically, we build a master-worker setup using AWS Lambda as the source of our workers, implement a parallel optimization algorithm to solve a regularized logistic regression problem, and show that relative speedups up to 256 workers and efficiencies above 70% up to 64 workers can be expected. We also identify possible algorithmic and system-level bottlenecks, propose improvements, and discuss the limitations and challenges in realizing these improvements. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190103161A},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.03161},
  eprint        = {1901.03161},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1901.03161.pdf},
}

@InProceedings{10.1007/978-3-319-99819-0_11,
author="Back, Timon
and Andrikopoulos, Vasilios",
editor="Kritikos, Kyriakos
and Plebani, Pierluigi 
and de Paoli, Flavio",
title="Using a Microbenchmark to Compare Function as a Service Solutions",
booktitle="Service-Oriented and Cloud Computing",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="146--160",
abstract="The Function as a Service (FaaS) subtype of serverless computing provides the means for abstracting away from servers on which developed software is meant to be executed. It essentially offers an event-driven and scalable environment in which billing is based on the invocation of functions and not on the provisioning of resources. This makes it very attractive for many classes of applications with bursty workload. However, the terms under which FaaS services are structured and offered to consumers uses mechanisms like GB--seconds (that is, X GigaBytes of memory used for Y seconds of execution) that differ from the usual models for compute resources in cloud computing. Aiming to clarify these terms, in this work we develop a microbenchmark that we use to evaluate the performance and cost model of popular FaaS solutions using well known algorithmic tasks. The results of this process show a field still very much under development, and justify the need for further extensive benchmarking of these services.",
isbn="978-3-319-99819-0"
}

@Inbook{Baldini2017,
author="Baldini, Ioana
and Castro, Paul
and Chang, Kerry
and Cheng, Perry
and Fink, Stephen
and Ishakian, Vatche
and Mitchell, Nick
and Muthusamy, Vinod
and Rabbah, Rodric
and Slominski, Aleksander
and Suter, Philippe",
editor="Chaudhary, Sanjay
and Somani, Gaurav
and Buyya, Rajkumar",
title="Serverless Computing: Current Trends and Open Problems",
bookTitle="Research Advances in Cloud Computing",
year="2017",
publisher="Springer Singapore",
address="Singapore",
pages="1--20",
abstract="Serverless computing has emerged as a new compelling paradigm for the deployment of applications and services. It represents an evolution of cloud programming models, abstractions, and platforms, and is a testament to the maturity and wide adoption of cloud technologies. In this chapter, we survey existing serverless platforms from industry, academia, and open-source projects, identify key characteristics and use cases, and describe technical challenges and open problems.",
isbn="978-981-10-5026-8",
doi="10.1007/978-981-10-5026-8_1",
url="https://doi.org/10.1007/978-981-10-5026-8_1"
}

@InProceedings{7833001,
  author    = {I. {Baldini} and P. {Castro} and P. {Cheng} and S. {Fink} and V. {Ishakian} and N. {Mitchell} and V. {Muthusamy} and R. {Rabbah} and P. {Suter}},
  title     = {Cloud-Native, Event-Based Programming for Mobile Applications},
  booktitle = {2016 IEEE/ACM International Conference on Mobile Software Engineering and Systems (MOBILESoft)},
  year      = {2016},
  pages     = {287-288},
  month     = {May},
  abstract  = {Creating mobile applications often requires both client and server- side code development, each requiring vastly differentskills. Recently, cloud providers like Amazon and Google introduced "server-less" programming models that abstract away many infrastructure concerns and allow developers to focus on their application logic. In this demonstration, we introduce OpenWhisk, our system for constructing cloud native actions, within the context of mobile application development process. We demonstrate how OpenWhisk is used in mobile application development, allows cloud API customizations for mobile, and simplifies mobile application architectures.},
  doi       = {10.1109/MobileSoft.2016.063},
  keywords  = {application program interfaces;cloud computing;mobile computing;software engineering;cloud-native programming;event-based programming;mobile applications;client-side code development;server-side code development;cloud providers;Amazon;Google;serverless programming models;OpenWhisk;cloud API customizations;Mobile communication;Programming;Speech;Mobile applications;Application programming interfaces;Cloud computing;Mobile development;server-less programming models},
}

@InProceedings{Baldini:2017:STF:3133850.3133855,
  author    = {Baldini, Ioana and Cheng, Perry and Fink, Stephen J. and Mitchell, Nick and Muthusamy, Vinod and Rabbah, Rodric and Suter, Philippe and Tardieu, Olivier},
  title     = {The Serverless Trilemma: Function Composition for Serverless Computing},
  booktitle = {Proceedings of the 2017 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
  year      = {2017},
  series    = {Onward! 2017},
  pages     = {89--103},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3133855},
  doi       = {10.1145/3133850.3133855},
  isbn      = {978-1-4503-5530-8},
  keywords  = {cloud, composition, functional, serverless},
  location  = {Vancouver, BC, Canada},
  numpages  = {15},
  url       = {http://doi.acm.org/10.1145/3133850.3133855},
}

@InProceedings{8513710,
  author    = {D. {Bardsley} and L. {Ryan} and J. {Howard}},
  title     = {Serverless Performance and Optimization Strategies},
  booktitle = {2018 IEEE International Conference on Smart Cloud (SmartCloud)},
  year      = {2018},
  pages     = {19-26},
  month     = {Sep.},
  abstract  = {In the constantly changing technological landscape the concept of serverless computing in a public Cloud is a relatively new development. Over recent years the serverless abstraction has gained significant traction within the IT industry. Google, Microsoft and AWS all now provide feature equivalent serverless implementations as part of their Cloud-based offerings and solution architects throughout the industry are utilizing serverless as part of mission critical enterprise systems. Throughout this paper we examine the performance profile of the serverless ecosystem in a low latency, high availability context, present results on the integral performance of such systems and outline some practical mitigation strategies to optimize serverless architectures. We confine our investigation specifically to one aspect of the AWS implementation of serverless; known as AWS Lambda. Our results show there are opportunities to tune the performance characteristics of Lambda-based architectures and we outline considerations such as cold starts and potential latency characteristics created by a combination of factors including external systems and events. We propose a diverse set of strategies, approaches and techniques which, when successfully implemented and deployed, simultaneously play to its strengths with the ultimate goal of providing a set of design patterns aimed at increasing the capability of serverless computing to a wider set of problem domains.},
  doi       = {10.1109/SmartCloud.2018.00012},
  keywords  = {cloud computing;object-oriented programming;open systems;software architecture;software performance evaluation;Web services;Lambda-based architectures;serverless computing;public Cloud;IT industry;mission critical enterprise systems;serverless ecosystem;serverless architectures;AWS Lambda;optimization strategies;Computer architecture;Logic gates;Servers;Time factors;Ecosystems;Monitoring;Computational modeling;serverless;aws;lambdas;architecture;performance},
}

@InProceedings{10.1007/978-3-319-67262-5_15,
author="Baresi, Luciano
and Filgueira Mendon{\c{c}}a, Danilo
and Garriga, Martin",
editor="De Paoli, Flavio
and Schulte, Stefan
and Broch Johnsen, Einar",
title="Empowering Low-Latency Applications Through a Serverless Edge Computing Architecture",
booktitle="Service-Oriented and Cloud Computing",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="196--210",
abstract="The exponential increase of the data generated by pervasive and mobile devices requires disrupting approaches for the realization of emerging mobile and IoT applications. Although cloud computing provides virtually unlimited computational resources, low-latency applications cannot afford the high latencies introduced by sending and retrieving data from/to the cloud. In this scenario, edge computing appears as a promising solution by bringing computation and data near to users and devices. However, the resource-finite nature of edge servers constrains the possibility of deploying full applications on them. To cope with these problems, we propose a serverless architecture at the edge, bringing a highly scalable, intelligent and cost-effective use of edge infrastructure's resources with minimal configuration and operation efforts. The feasibility of our approach is shown through an augmented reality use case for mobile devices, in which we offload computation and data intensive tasks from the devices to serverless functions at the edge, outperforming the cloud alternative up to 80{\%} in terms of throughput and latency.",
isbn="978-3-319-67262-5"
}

@Article{Baresi:2019:UMM:3322882.3226644,
  author     = {Baresi, L. and Mendon\c{c}a, D. F. and Garriga, M. and Guinea, S. and Quattrocchi, G.},
  title      = {A Unified Model for the Mobile-Edge-Cloud Continuum},
  journal    = {ACM Trans. Internet Technol.},
  year       = {2019},
  volume     = {19},
  number     = {2},
  pages      = {29:1--29:21},
  month      = apr,
  issn       = {1533-5399},
  acmid      = {3226644},
  address    = {New York, NY, USA},
  articleno  = {29},
  doi        = {10.1145/3226644},
  issue_date = {April 2019},
  keywords   = {Computing continuum, Functions-as-a-Service, edge computing, fog computing, mobile computing, ops automation, real-time systems},
  numpages   = {21},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3226644},
}

@Article{2019arXiv190106811B,
  author        = {{Bartan}, Burak and {Pilanci}, Mert},
  title         = {{Polar Coded Distributed Matrix Multiplication}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.06811},
  month         = {Jan},
  abstract      = {We propose a polar coding mechanism for distributed matrix multiplication. Polar codes provably achieve channel capacity and have the advantage of low encoding and decoding complexity. These aspects of polar codes enable a scalable scheme for hundreds of compute nodes in coded computation. We analyze the polarization phenomenon in the context of run times of compute nodes and characterize polarizing matrices over real numbers. We design a sequential decoder specifically for polar codes in erasure channels with real-valued input and outputs. The proposed coded computation scheme is implemented for a serverless computing platform and numerical results are provided. Numerical results illustrate that proposed coded computation scheme achieves significant speed-ups. Finally, experiments are conducted where the performance of the proposed coded computation technique is tested in solving a least squares problem using gradient descent. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190106811B},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.06811},
  eprint        = {1901.06811},
  keywords      = {Computer Science - Information Theory, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
  primaryclass  = {cs.IT},
  url           = {https://arxiv.org/pdf/1901.06811.pdf},
}

@Article{Becker2019,
author="Becker, Christian
and Julien, Christine
and Lalanda, Philippe
and Zambonelli, Franco",
title="Pervasive computing middleware: current trends and emerging challenges",
journal="CCF Transactions on Pervasive Computing and Interaction",
year="2019",
month="Feb",
day="19",
abstract="Driven by the increasing diffusion of embedded sensors and actuators, and more in general by ``Internet of Things'' (IoT) devices, pervasive computing is becoming a reality. Yet, most actual implementations of pervasive computing environments rely on rather centralized architectures and on middleware solutions that integrate only the minimal set of services to enable interoperabilty and data integration. In this article, after having overviewed the state of the art in the area of pervasive computing middleware, we discuss the many challenges that still have to be faced for pervasive computing middleware to be able to support elastic, easy to configure, easy to develop, safe, and ethically acceptable, pervasive computing services and applications.",
issn="2524-5228",
doi="10.1007/s42486-019-00005-2",
url="https://doi.org/10.1007/s42486-019-00005-2"
}

@InProceedings{10.1007/978-3-319-91764-1_16,
author="Bermbach, David
and Pallas, Frank
and P{\'e}rez, David Garc{\'i}a
and Plebani, Pierluigi
and Anderson, Maya
and Kat, Ronen
and Tai, Stefan",
editor="Braubach, Lars
and Murillo, Juan M.
and Kaviani, Nima
and Lama, Manuel
and Burgue{\~{n}}o, Loli
and Moha, Naouel
and Oriol, Marc",
title="A Research Perspective on Fog Computing",
booktitle="Service-Oriented Computing -- ICSOC 2017 Workshops",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="198--210",
abstract="State-of-the-art applications are typically deployed on top of cloud services which offer the illusion of infinite resources, elastic scalability, and a simple pay-per-use billing model. While this is very convenient for developers, it also comes with relatively high access latency for end users. Future application domains such as the Internet of Things, autonomous driving, or future 5G mobile apps, however, require low latency access which is typically achieved by moving computation towards the edge of the network. This natural extension of the cloud towards the edge is typically referred to as Fog Computing and has lately found a lot of attention. However, Fog Computing as a deployment platform has not yet found widespread adoption; this, we believe, could be helped through a consistent use of the service-oriented computing paradigm for fog infrastructure services. Based on this motivation, this paper describes the concept of Fog Computing in detail, discusses the main obstacles for Fog Computing adoption, and derives open research challenges.",
isbn="978-3-319-91764-1"
}

@Article{8405632,
  author   = {K. {Bhatia}},
  title    = {Nate Taggart on Serverless},
  journal  = {IEEE Software},
  year     = {2018},
  volume   = {35},
  number   = {4},
  pages    = {101-104},
  month    = {July},
  issn     = {0740-7459},
  abstract = {In this excerpt from Software Engineering Radio, Nate Taggart, cofounder and CEO of Stackery, discusses serverless—the ability to purchase function as a service in which the cloud provider assumes responsibility for providing a server and an execution environment on demand to run a piece of code. To hear the full interview, visit www.se-radio.net or access our archives via RSS at feeds.feedburner.com/se-radio.},
  doi      = {10.1109/MS.2018.2801544},
  keywords = {Software engineering;Computational modeling;FAA;Nate Taggart;serverless;Software Engineering Radio;software engineering;software development},
}

@Article{2019arXiv190401576B,
  author        = {{Bhattacharjee}, Anirban and {Chhokra}, Ajay Dev and {Kang}, Zhuangwei and {Sun}, Hongyang and {Gokhale}, Aniruddha and {Karsai}, Gabor},
  title         = {{BARISTA: Efficient and Scalable Serverless Serving System for Deep Learning Prediction Services}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1904.01576},
  month         = {Apr},
  abstract      = {Pre-trained deep learning models are increasingly being used to offer a variety of compute-intensive predictive analytics services such as fitness tracking, speech and image recognition. The stateless and highly parallelizable nature of deep learning models makes them well-suited for serverless computing paradigm. However, making effective resource management decisions for these services is a hard problem due to the dynamic workloads and diverse set of available resource configurations that have their deployment and management costs. To address these challenges, we present a distributed and scalable deep-learning prediction serving system called Barista and make the following contributions. First, we present a fast and effective methodology for forecasting workloads by identifying various trends. Second, we formulate an optimization problem to minimize the total cost incurred while ensuring bounded prediction latency with reasonable accuracy. Third, we propose an efficient heuristic to identify suitable compute resource configurations. Fourth, we propose an intelligent agent to allocate and manage the compute resources by horizontal and vertical scaling to maintain the required prediction latency. Finally, using representative real-world workloads for urban transportation service, we demonstrate and validate the capabilities of Barista. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190401576B},
  archiveprefix = {arXiv},
  eid           = {arXiv:1904.01576},
  eprint        = {1904.01576},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1904.01576.pdf},
}

@InProceedings{7979854,
  author    = {N. {Bila} and P. {Dettori} and A. {Kanso} and Y. {Watanabe} and A. {Youssef}},
  title     = {Leveraging the Serverless Architecture for Securing Linux Containers},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems Workshops (ICDCSW)},
  year      = {2017},
  pages     = {401-404},
  month     = {June},
  abstract  = {Linux containers present a lightweight solution to package applications into images and instantiate them in isolated environments. Such images may include vulnerabilities that can be exploited at runtime. A vulnerability scanning service can detect these vulnerabilities by periodically scanning the containers and their images for potential threats. When a threat is detected, an event may be generated to (1) quarantine or terminate the compromised container(s) and optionally (2) remedy the vulnerability by rebuilding a secure image. We believe that such event-driven process is a great fit to be implemented in a serverless architecture. In this paper we explore the design of an automated threat mitigation architecture based on OpenWhisk and Kubernetes.},
  doi       = {10.1109/ICDCSW.2017.66},
  issn      = {2332-5666},
  keywords  = {Linux;serverless architecture;Linux containers;vulnerability scanning service;secure image;automated threat mitigation architecture;Kubernetes;OpenWhisk;Containers;Security;Computer architecture;Engines;Linux;Tools;Runtime;Linux containers;serverless architecture;Kubernetes;OpenWhisk;Docker;security analysis},
}

@InProceedings{Boucher:2018:PMB:3277355.3277417,
  author    = {Boucher, Sol and Kalia, Anuj and Andersen, David G. and Kaminsky, Michael},
  title     = {Putting the "Micro" Back in Microservice},
  booktitle = {Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference},
  year      = {2018},
  series    = {USENIX ATC '18},
  pages     = {645--650},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3277417},
  isbn      = {978-1-931971-44-7},
  location  = {Boston, MA, USA},
  numpages  = {6},
  url       = {http://dl.acm.org/citation.cfm?id=3277355.3277417},
}

@InProceedings{8247460,
  author    = {E. F. {Boza} and C. L. {Abad} and M. {Villavicencio} and S. {Quimba} and J. A. {Plaza}},
  title     = {Reserved, on demand or serverless: Model-based simulations for cloud budget planning},
  booktitle = {2017 IEEE Second Ecuador Technical Chapters Meeting (ETCM)},
  year      = {2017},
  pages     = {1-6},
  month     = {Oct},
  abstract  = {Cloud computing providers offer a variety of pricing models, complicating the client decision, as no single model is the cheapest in all scenarios. In addition, small to medium-sized organizations frequently lack personnel that can navigate the intricacies of each pricing model, and as a result, end up opting for a sub-optimal strategy, leading to overpaying for computing resources or not being able to meet performance goals. In this paper, we: (1) present the results of a study that shows that, in Ecuador, a considerable percentage of companies choose conservative pricing strategies, (2) present a case study that shows that the conservative pricing strategy is suboptimal under certain workloads, and (3) propose a set of models, a tool and a process that can be used by tenants to properly plan and budget their cloud computing costs. Our tool is based on M (t)/M/* queuing theory models and is easy to configure and use. Note that, even though we are motivated by our study of adoption of cloud computing technologies in Ecuador, our tool and process are widely applicable and not restricted to the Ecuadorian context.},
  doi       = {10.1109/ETCM.2017.8247460},
  keywords  = {cloud computing;decision making;optimisation;pricing;queueing theory;securities trading;cloud budget planning;cloud computing providers;pricing model;client decision;conservative pricing strategy;cloud computing costs;suboptimal strategy;model-based simulations;small to medium-sized organizations;M (t)/M/* queuing theory models;Cloud computing;Pricing;Computational modeling;Tools;Companies;Electronic mail;Cloud;reserved;on-demand;serverless;budget;simulation;queuing theory},
}

@InProceedings{8588721,
  author    = {M. {Branowski} and A. {Belloum}},
  title     = {Cookery: A Framework for Creating Data Processing Pipeline Using Online Services},
  booktitle = {2018 IEEE 14th International Conference on e-Science (e-Science)},
  year      = {2018},
  pages     = {368-369},
  month     = {Oct},
  abstract  = {With the increasing amount of data the importance of data analysis has grown. A large amount of this data has shifted to cloud-based storage. The cloud offers storage and computation power. The Cookery framework is a tool developed to build application in the cloud for scientists without a complete understanding of programming. In this paper with present the cookery systems and how it can be used to authenticate and use standard online 3rd party services to easily create data analytics pipeline. Cookery framework is not limited to work with standard web services, it can also integrate and work with the emerging AWS Lambda. The combination of AWS Lambda and Cookery, which makes it possible for people, who do not have any program experience, to create data processing pipeline using cloud services in short time.},
  doi       = {10.1109/eScience.2018.00102},
  keywords  = {cloud computing;data analysis;Web services;cookery systems;data analytics pipeline;cloud services;AWS Lambda;data processing pipeline;online 3rd party services;standard Web services;Electronic mail;Pipelines;Data analysis;Google;Programming;Market research;Function-as-a-Service (FaaS);AWS Lambda;OAuth;IFTTT;Cloud},
}

@InProceedings{10.1007/978-3-319-64203-1_26,
author="Bravo Ferreira, Jos{\'e}
and Cello, Marco
and Iglesias, Jes{\'u}s Omana",
editor="Rivera, Francisco F.
and Pena, Tom{\'a}s F.
and Cabaleiro, Jos{\'e} C.",
title="More Sharing, More Benefits? A Study of Library Sharing in Container-Based Infrastructures",
booktitle="Euro-Par 2017: Parallel Processing",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="358--371",
abstract="Container-based infrastructures have surged in popularity, offering advantages in agility and scaling, while also presenting new challenges in resource utilization due to unnecessary library duplication. In this paper, we consider sharing libraries across containers, and study the impact of such a strategy on overall resource requirements, scheduling, and utilization. Our analysis and simulations suggest significant benefits arising from library sharing. Furthermore, a small fraction of libraries shared between any two containers, on average, is enough to reap most of the benefits, and even na{\"i}ve schedulers, such as a First Fit scheduler, succeed at doing so. We also propose a score maximization, mixed-integer linear-programming scheduler for handling bulk request arrivals (such as large jobs composed of many smaller tasks), which compares favorably against state-of-the-art schedulers in these scenarios.",
isbn="978-3-319-64203-1"
}

@InProceedings{Breitgand:2018:TSN:3211890.3211916,
  author    = {Breitgand, David and Weit, Avi and Rizou, Stamatia and Griffin, David and Acar, Ugur and Carrozzo, Gino and Zioulis, Nikolaos and Andriani, Pasquale and Iadanza, Francesco},
  title     = {Towards Serverless NFV for 5G Media Applications},
  booktitle = {Proceedings of the 11th ACM International Systems and Storage Conference},
  year      = {2018},
  series    = {SYSTOR '18},
  pages     = {118--118},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3211916},
  doi       = {10.1145/3211890.3211916},
  isbn      = {978-1-4503-5849-1},
  location  = {Haifa, Israel},
  numpages  = {1},
  url       = {http://doi.acm.org/10.1145/3211890.3211916},
}

@inproceedings{Brenner:2019:TMS:3319647.3325825,
 author = {Brenner, Stefan and Kapitza, R\"{u}diger},
 title = {Trust More, Serverless},
 booktitle = {Proceedings of the 12th ACM International Conference on Systems and Storage},
 series = {SYSTOR '19},
 year = {2019},
 isbn = {978-1-4503-6749-3},
 location = {Haifa, Israel},
 pages = {33--43},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3319647.3325825},
 doi = {10.1145/3319647.3325825},
 acmid = {3325825},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {intel SGX, serverless cloud, trusted function-as-a-service},
}

@InProceedings{10.1007/978-3-030-00470-5_30,
author="Bushouse, Micah
and Reeves, Douglas",
editor="Bailey, Michael
and Holz, Thorsten
and Stamatogiannakis, Manolis
and Ioannidis, Sotiris",
title="Furnace: Self-service Tenant VMI for the Cloud",
booktitle="Research in Attacks, Intrusions, and Defenses",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="647--669",
abstract="Although Virtual Machine Introspection (VMI) tools are increasingly capable, modern multi-tenant cloud providers are hesitant to expose the sensitive hypervisor APIs necessary for tenants to use them. Outside the cloud, VMI and virtualization-based security's adoption rates are rising and increasingly considered necessary to counter sophisticated threats. This paper introduces Furnace, an open source VMI framework that outperforms prior frameworks by satisfying both a cloud provider's expectation of security and a tenant's desire to run their own custom VMI tools underneath their cloud VMs. Furnace's flexibility and ease of use is demonstrated by porting four existing security and monitoring tools as Furnace VMI apps; these apps are shown to be resource efficient while executing up to 300x faster than those in previous VMI frameworks. Furnace's security properties are shown to protect against the actions of malicious tenant apps.",
isbn="978-3-030-00470-5"
}

@Article{Buyya:2018:MFG:3271482.3241737,
  author     = {Buyya, Rajkumar and Srirama, Satish Narayana and Casale, Giuliano and Calheiros, Rodrigo and Simmhan, Yogesh and Varghese, Blesson and Gelenbe, Erol and Javadi, Bahman and Vaquero, Luis Miguel and Netto, Marco A. S. and Toosi, Adel Nadjaran and Rodriguez, Maria Alejandra and Llorente, Ignacio M. and Vimercati, Sabrina De Capitani Di and Samarati, Pierangela and Milojicic, Dejan and Varela, Carlos and Bahsoon, Rami and Assuncao, Marcos Dias De and Rana, Omer and Zhou, Wanlei and Jin, Hai and Gentzsch, Wolfgang and Zomaya, Albert Y. and Shen, Haiying},
  title      = {A Manifesto for Future Generation Cloud Computing: Research Directions for the Next Decade},
  journal    = {ACM Comput. Surv.},
  year       = {2018},
  volume     = {51},
  number     = {5},
  pages      = {105:1--105:38},
  month      = nov,
  issn       = {0360-0300},
  acmid      = {3241737},
  address    = {New York, NY, USA},
  articleno  = {105},
  doi        = {10.1145/3241737},
  issue_date = {January 2019},
  keywords   = {Cloud computing, Cloud economics, Fog computing, InterCloud, application development, data management, scalability, serverless computing, sustainability},
  numpages   = {38},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3241737},
}

@InProceedings{7980271,
  author    = {P. {Castro} and V. {Ishakian} and V. {Muthusamy} and A. {Slominski}},
  title     = {Serverless Programming (Function as a Service)},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)},
  year      = {2017},
  pages     = {2658-2659},
  month     = {June},
  abstract  = {In this tutorial, we will present serverless computing, survey existing serverless platforms from industry, academia, and open source projects, identify key characteristics and use cases, and describe technical challenges and open problems. Our tutorial will involve a hands-on experience of using the serverless technologies available from different cloud providers (e.g. IBM, Amazon, Google and Microsoft). We expect our users to have basic knowledge of programming and basic knowledge of cloud computing.},
  doi       = {10.1109/ICDCS.2017.305},
  issn      = {1063-6927},
  keywords  = {cloud computing;serverless programming;function as a service;serverless computing;serverless technologies;cloud providers;cloud computing;Programming;Cloud computing;Computer architecture;Mobile communication;Conferences;Tutorials;Quality of service},
}

@InProceedings{8103476,
  author    = {K. S. {Chang} and S. J. {Fink}},
  title     = {Visualizing serverless cloud application logs for program understanding},
  booktitle = {2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)},
  year      = {2017},
  pages     = {261-265},
  month     = {Oct},
  abstract  = {A cloud platform records a wealth of information regarding program execution. Most cloud service providers offer dashboard monitoring tools that visualize resource usage and billing information, and support debugging. In this paper, we present a tool that visualizes cloud execution logs for a different goal - to facilitate program understanding and generate documentations for an application using runtime data. Our tool introduces a new timeline visualization, a new method and user interface to summarize multiple JSON objects and present the result, and interaction techniques that facilitate navigating among functions. Together, these features explain a serverless cloud application's composition, performance, dataflow and data schema. We report some initial user feedback from several expert developers that were involved in the tool's design and development process.},
  doi       = {10.1109/VLHCC.2017.8103476},
  issn      = {1943-6106},
  keywords  = {cloud computing;data flow analysis;data visualisation;program debugging;resource allocation;system monitoring;user interfaces;program understanding;runtime data;timeline visualization;user interface;serverless cloud application;cloud service providers;resource usage;billing information;debugging;cloud platform;program execution;dashboard monitoring tools;cloud execution logs;JSON objects;Tools;Cloud computing;Data visualization;Unified modeling language;Bars;Computational modeling;Visualization;serverless computing;function as a service;program understanding;log visualization;cloud computing},
}

@InProceedings{7979852,
  author    = {R. {Chard} and K. {Chard} and J. {Alt} and D. Y. {Parkinson} and S. {Tuecke} and I. {Foster}},
  title     = {Ripple: Home Automation for Research Data Management},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems Workshops (ICDCSW)},
  year      = {2017},
  pages     = {389-394},
  month     = {June},
  abstract  = {Exploding data volumes and acquisition rates, plus ever more complex research processes, place significant strain on research data management processes. It is increasingly common for data to flow through pipelines comprised of dozens of different management, organization, and analysis steps distributed across multiple institutions and storage systems. To alleviate the resulting complexity, we propose a home automation approach to managing data throughout its lifecycle, in which users specify via high-level rules the actions that should be performed on data at different times and locations. To this end, we have developed Ripple, a responsive storage architecture that allows users to express data management tasks via a rules notation. Ripple monitors storage systems for events, evaluates rules, and uses serverless computing techniques to execute actions in response to these events. We evaluate our solution by applying Ripple to the data lifecycles of two real-world projects, in astronomy and light source science, and show that it can automate many mundane and cumbersome data management processes.},
  doi       = {10.1109/ICDCSW.2017.30},
  issn      = {2332-5666},
  keywords  = {astronomy computing;data acquisition;home automation;light sources;storage management;data volumes;data acquisition rates;complex research processes;research data management;home automation;high-level rules;RIPPLE;responsive storage architecture;storage systems;serverless computing;data lifecycles;astronomy;light source science;Monitoring;Telescopes;Observers;Home automation;Light sources;Reliability;Distributed databases;Responsive storage;Serverless;Software defined cyberinfrastructure},
}

@InProceedings{10.1007/978-3-319-94478-4_17,
author="Chen, Huan
and Zhang, Liang-Jie",
editor="Chen, Shiping
and Wang, Harry
and Zhang, Liang-Jie",
title="FBaaS: Functional Blockchain as a Service",
booktitle="Blockchain -- ICBC 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="243--250",
abstract="Serverless architecture has been gaining popularity in the last three years. Function as a Service (FaaS) is a concrete realization of the Serverless architecture and has several advantages and features. This paper proposes a new service model which is based on FaaS model, named FBaaS -- Functional Blockchain as a Service. Compared with Blockchain as a Service (BaaS), FBaaS has a lighter implementation of top-level business logics, which brings a number of advantages. Firstly, it could improve the operation speed of a blockchain. Secondly, the continuous advances in high robustness, high availability of the underlying FaaS network can be naturally adapted to the FBaaS because of its hierarchical architecture. Thirdly, FaaS implements higher level of abstraction of the logics that is much succinct. Moreover, this paper proposes an abstraction method in the realization of a business logic of consortium blockchain that could further improve the performance. In this paper, we also unfold the details of a concrete example network, which is the conference blockchain network for Services Conference Federation (SCF) 2018.",
isbn="978-3-319-94478-4"
}

@InProceedings{8560183,
  author    = {Y. {Cheng} and Z. {Zhou}},
  title     = {Autonomous Resource Scheduling for Real-Time and Stream Processing},
  booktitle = {2018 IEEE SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computing, Scalable Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
  year      = {2018},
  pages     = {1181-1184},
  month     = {Oct},
  abstract  = {This work proposes the ARS(FaaS) framework, scheduling and provisioning resources for streaming applications autonomously. It ensures real-time response on unpredictable and fluctuating streaming data. We use a HPC Cloud platform as the de facto platform, and explore FaaS for stream processing on it. The major contribution of this work is effective and efficient autonomous resource scheduling for real-time streaming analytic.},
  doi       = {10.1109/SmartWorld.2018.00205},
  keywords  = {cloud computing;parallel processing;resource allocation;scheduling;autonomous resource scheduling;stream processing;provisioning resources;real-time response;unpredictable streaming data;fluctuating streaming data;facto platform;FaaS;HPC cloud platform;autonomous scheduling;steam processing;cloud computing;FaaS},
}

@InProceedings{8389457,
  author    = {S. {Chinchole} and A. {Kulkarni} and L. {Matai} and C. {Kotadiya}},
  title     = {A real-time cloud-based messaging system for delivering medication to the rural areas},
  booktitle = {2017 International Conference on Intelligent Sustainable Systems (ICISS)},
  year      = {2017},
  pages     = {475-479},
  month     = {Dec},
  abstract  = {The rural areas of most of the developing nations suffer from lack of proper schools, poor road infrastructure, scarcity of pharmacies and hospitals and reliable public transport service amongst many other things. These conditions have a major impact on the people residing in such areas in a negative way. The illiterate, visually impaired, handicapped and old aged people suffer the most because they do not have access to quality healthcare in such areas. The system presented in the paper utilizes the mobile internet, serverless technology and cloud computing to serve these people. The system acts as a picture centered real-time messaging system to enable the people to order medication online and get it delivered to their residences.},
  doi       = {10.1109/ISS1.2017.8389457},
  keywords  = {cloud computing;geriatrics;handicapped aids;health care;Internet;medical information systems;mobile computing;telemedicine;real-time cloud;rural areas;developing nations;pharmacies;hospitals;handicapped people;old aged people;serverless technology;real-time messaging system;schools;road infrastructure;public transport service;online medication;mobile Internet;Cloud computing;Real-time systems;Portals;Task analysis;Conferences;Cameras;Medical services;Cloud Computing;Healthcare;Real-Time System;Serverless Architecture},
}

@InProceedings{8590993,
  author    = {C. {Cicconetti} and M. {Conti} and A. {Passarella}},
  title     = {An Architectural Framework for Serverless Edge Computing: Design and Emulation Tools},
  booktitle = {2018 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)},
  year      = {2018},
  pages     = {48-55},
  month     = {Dec},
  abstract  = {We consider a Software Defined Networking (SDN)-enabled edge computing domain, where networking devices also have processing capabilities. In particular, we investigate the problem of dynamic allocation of stateless computations, that we call lambda functions, and propose an architectural framework through which requests for execution of lambda functions originated by mobile nodes can be appropriately routed to specific edge devices following a serverless model. In addition, we propose a detailed emulation environment to test the architecture. Our framework supports many possible distributed algorithms to dynamically adapt the choice where requests should be executed, in order to optimize a given performance target. In the paper we consider a few such policies, to test the flexibility of the architecture. We thus present extensive performance results of the considered policies.},
  doi       = {10.1109/CloudCom2018.2018.00024},
  issn      = {2330-2186},
  keywords  = {distributed algorithms;mobile computing;software defined networking;telecommunication network routing;software defined networking-enabled edge computing domain;distributed algorithms;serverless model;specific edge devices;mobile nodes;lambda functions;stateless computations;dynamic allocation;processing capabilities;networking devices;SDN;serverless edge computing;architectural framework;Edge computing;Computer architecture;Performance evaluation;Emulation;Servers;Optimization;Face;edge computing;software defined networking;serverless computing;computation delegation},
}

@InProceedings{Coleman:2017:SUC:3154847.3154851,
  author    = {Coleman, Kyle and Esposito, Flavio and Charney, Rachel},
  title     = {Speeding Up Children Reunification in Disaster Scenarios via Serverless Computing},
  booktitle = {Proceedings of the 2Nd International Workshop on Serverless Computing},
  year      = {2017},
  series    = {WoSC '17},
  pages     = {5--5},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3154851},
  doi       = {10.1145/3154847.3154851},
  isbn      = {978-1-4503-5434-9},
  location  = {Las Vegas, Nevada},
  numpages  = {1},
  url       = {http://doi.acm.org/10.1145/3154847.3154851},
}

@InProceedings{Crane:2017:ESA:3121050.3121086,
  author    = {Crane, Matt and Lin, Jimmy},
  title     = {An Exploration of Serverless Architectures for Information Retrieval},
  booktitle = {Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval},
  year      = {2017},
  series    = {ICTIR '17},
  pages     = {241--244},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3121086},
  doi       = {10.1145/3121050.3121086},
  isbn      = {978-1-4503-4490-6},
  keywords  = {DynamoDB, amazon lambda, cloud computing, score-at-a-time query evaluation},
  location  = {Amsterdam, The Netherlands},
  numpages  = {4},
  url       = {http://doi.acm.org/10.1145/3121050.3121086},
}

@Article{2018arXiv181109732D,
  author        = {{Dakkak}, Abdul and {Li}, Cheng and {Garcia de Gonzalo}, Simon and {Xiong}, Jinjun and {Hwu}, Wen-mei},
  title         = {{TrIMS: Transparent and Isolated Model Sharing for Low Latency Deep LearningInference in Function as a Service Environments}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1811.09732},
  month         = {Nov},
  abstract      = {Deep neural networks (DNNs) have become core computation components within low latency Function as a Service (FaaS) prediction pipelines: including image recognition, object detection, natural language processing, speech synthesis, and personalized recommendation pipelines. Cloud computing, as the de-facto backbone of modern computing infrastructure for both enterprise and consumer applications, has to be able to handle user-defined pipelines of diverse DNN inference workloads while maintaining isolation and latency guarantees, and minimizing resource waste. The current solution for guaranteeing isolation within FaaS is suboptimal -- suffering from "cold start" latency. A major cause of such inefficiency is the need to move large amount of model data within and across servers. We propose TrIMS as a novel solution to address these issues. Our proposed solution consists of a persistent model store across the GPU, CPU, local storage, and cloud storage hierarchy, an efficient resource management layer that provides isolation, and a succinct set of application APIs and container technologies for easy and transparent integration with FaaS, Deep Learning (DL) frameworks, and user code. We demonstrate our solution by interfacing TrIMS with the Apache MXNet framework and demonstrate up to 24x speedup in latency for image classification models and up to 210x speedup for large models. We achieve up to 8x system throughput improvement. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181109732D},
  archiveprefix = {arXiv},
  eid           = {arXiv:1811.09732},
  eprint        = {1811.09732},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1811.09732.pdf},
}
@INPROCEEDINGS{8700543,
author={A. {Danayi} and S. {Sharifian}},
booktitle={2018 4th Iranian Conference on Signal Processing and Intelligent Systems (ICSPIS)},
title={PESS-MinA: A Proactive Stochastic Task Allocation Algorithm for FaaS Edge-Cloud environments},
year={2018},
volume={},
number={},
pages={27-31},
abstract={By the advent of FaaS Cloud services and the micro-services programming architecture, designing task allocation algorithms with higher performance has become a crucial task. Motivated by this high-interested challenge, we propose a new allocation algorithm called PESS-MinA based on our novel modular model for FaaS Edge-Cloud environments. In contradiction to widely-used Max-Min and Min-Min algorithms which are both reactive and deterministic, this algorithm is based on stochastic score, and thus provides proactivity considerations. Experiments with Google Cloud Trace dataset show that our algorithm exhibits better performance in both resource load balancing and QoS assurance of FaaS. According to simulations, PESS-MinA decreased the dropped tasks percentage from 2.9% to 0.01%, alongside with a triple balancing score.},
keywords={cloud computing;minimax techniques;quality of service;resource allocation;stochastic processes;PESS-MinA;FaaS Cloud services;max-min algorithms;min-min algorithms;FaaS edge-cloud environments;stochastic task allocation algorithm;microservices programming architecture;Google Cloud Trace dataset;load balancing;QoS assurance;Task analysis;Signal processing algorithms;Cloud computing;FAA;Resource management;Stochastic processes;Containers;Proactive;Task Allocation;FaaS;Edge-Cloud},
doi={10.1109/ICSPIS.2018.8700543},
ISSN={},
month={Dec},}

@Article{2019arXiv190100302D,
  author        = {{Danayi}, Abolfazl and {Sharifian}, Saeed},
  title         = {{openCoT: The opensource Cloud of Things platform}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.00302},
  month         = {Jan},
  abstract      = {In order to address the complexity and extensiveness of technology, Cloud Computing is utilized with four main service models. The most recent service model, function-as-a-service, enables developers to develop their application in a function-based structure and then deploy it to the Cloud. Using an optimum elastic auto-scaling, the performance of executing an application over FaaS Cloud, overcomes the extra overhead and reduces the total cost. However, researchers need a simple and well-documented FaaS Cloud manager in order to implement their proposed Auto-scaling algorithms. In this paper, we represent the openCoT platform and explain its building blocks and details. Experimental results show that executing a function (invoking and passing arguments) and returning the result using openCoT takes 21 ms over a remote connection. The source code of openCoT is available in the GitHub repository of the project (\code{www.github.com/adanayi/opencot}) for public usage. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190100302D},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.00302},
  eprint        = {1901.00302},
  keywords      = {Computer Science - Networking and Internet Architecture, Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.NI},
  url           = {https://arxiv.org/pdf/1901.00302.pdf},
}

@InProceedings{8605776,
  author    = {A. {Das} and S. {Patterson} and M. {Wittie}},
  title     = {EdgeBench: Benchmarking Edge Computing Platforms},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {175-180},
  month     = {Dec},
  abstract  = {The emerging trend of edge computing has led several cloud providers to release their own platforms for performing computation at the 'edge' of the network. We compare two such platforms, Amazon AWS Greengrass and Microsoft Azure IoT Edge, using a new benchmark comprising a suite of performance metrics. We also compare the performance of the edge frameworks to cloud-only implementations available in their respective cloud ecosystems. Amazon AWS Greengrass and Azure IoT Edge use different underlying technologies, edge Lambda functions vs. containers, and so we also elaborate on platform features available to developers. Our study shows that both of these edge platforms provide comparable performance, which nevertheless differs in important ways for key types of workloads used in edge applications. Finally, we discuss several current issues and challenges we faced in deploying these platforms.},
  doi       = {10.1109/UCC-Companion.2018.00053},
  keywords  = {benchmark testing;cloud computing;Internet of Things;performance metrics;edge frameworks;cloud-only implementations;Amazon AWS Greengrass;edge platforms;edge applications;cloud providers;Microsoft Azure IoT Edge;cloud ecosystems;edge computing platform benchmarking;edge Lambda functions;EdgeBench;Cloud computing;Image edge detection;Benchmark testing;Edge computing;Performance evaluation;Pipelines;Internet of Things;Serverless Computing;Cloud Functions},
}

@InProceedings{8077240,
  author    = {S. {Dash} and D. K. {Dash}},
  title     = {Serverless cloud computing framework for smart grid architecture},
  booktitle = {2016 IEEE 7th Power India International Conference (PIICON)},
  year      = {2016},
  pages     = {1-6},
  month     = {Nov},
  abstract  = {Additional digital layer is an important aspect in the Smart grid architecture. We gather the data using this layer and the whole grid is controlled accordingly. Energy cost can be estimated and the demand can be predicted. Estimation of health issues of electrical equipment can be done from the control room by gathering the data from remote locations. In the case of computer-aided digital relay model if we choose the pilot relay action by remote data then that would be the good application of this digital layer. In this paper, we have suggested an alternative method- to gather and analyze the remote data using the serverless cloud computing framework. The prime objective is to design a cheaper and technically easier simulation strategy with this new framework. At the end, we have tried to create a mathematical model to estimate the cost and quality of service. The system is simulated using MATLAB, Amazon Web Service (AWS) products and a batch program.},
  doi       = {10.1109/POWERI.2016.8077240},
  keywords  = {cloud computing;data analysis;demand forecasting;digital simulation;estimation theory;load forecasting;power engineering computing;power system protection;power system relaying;relay protection;smart power grids;Web services;serverless cloud computing framework;smart grid architecture;additional digital layer;control room;remote locations;digital relay model;pilot relay action;energy cost estimation;simulation strategy;demand prediction;electrical equipment health estimation;remote data analysis;MATLAB;Amazon Web service;AWS;Smart grid;Cloud computing;Serverless Cloud computing architecture},
}

@InProceedings{8411080,
  author    = {A. {Deese}},
  title     = {Implementation of Unsupervised k-Means Clustering Algorithm Within Amazon Web Services Lambda},
  booktitle = {2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)},
  year      = {2018},
  pages     = {626-632},
  month     = {May},
  abstract  = {This work demonstrates how an unsupervised learning algorithm based on k-Means Clustering with Kaufman Initialization may be implemented effectively as an Amazon Web Services Lambda Function, within their serverless cloud computing service. It emphasizes the need to employ a lean and modular design philosophy, transfer data efficiently between Lambda and DynamoDB, as well as employ Lambda Functions within mobile applications seamlessly and with negligible latency. This work presents a novel application of serverless cloud computing and provides specific examples that will allow readers to develop similar algorithms. The author provides compares the computation speed and cost of machine learning implementations on traditional PC and mobile hardware (running locally) as well as implementations that employ Lambda.},
  doi       = {10.1109/CCGRID.2018.00093},
  keywords  = {cloud computing;learning (artificial intelligence);mobile computing;pattern clustering;unsupervised learning;Web services;unsupervised k-Means Clustering algorithm;unsupervised learning algorithm;Kaufman Initialization;Amazon Web Services Lambda Function;serverless cloud computing service;lean design philosophy;modular design philosophy;mobile applications;DynamoDB;data transfer;machine learning;Cloud computing;Machine learning;Libraries;Training data;Servers;Task analysis;Pipelines;machine learning;k-Means Clustering;Kaufman;serverless computing;Amazon Web Services Lambda;cloud computing},
}
@ARTICLE{2019arXiv190504456D,
author = {{Denninnart}, Chavit and {Gentry}, James and {Amini Salehi}, Mohsen},
title = "{Improving Robustness of Heterogeneous Serverless Computing Systems Via Probabilistic Task Pruning}",
journal = {arXiv e-prints},
keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
year = "2019",
month = "May",
eid = {arXiv:1905.04456},
pages = {arXiv:1905.04456},
archivePrefix = {arXiv},
eprint = {1905.04456},
primaryClass = {cs.DC},
adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190504456D},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@InProceedings{10.1007/978-3-319-67380-6_4,
author="Devos, Mathieu
and Masek, Pavel",
editor="Galinina, Olga
and Andreev, Sergey
and Balandin, Sergey
and Koucheryavy, Yevgeni",
title="Battery Monitoring Within Industry 4.0 Landscape: Solution as a Service (SaaS) for Industrial Power Unit Systems",
booktitle="Internet of Things, Smart Spaces, and Next Generation Networks and Systems",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="40--52",
abstract="The current globalization already faces the challenge of meeting the continuously growing demand for new consumer goods by simultaneously ensuring a sustainable evolution of human existence. The industrial value creation must be geared towards sustainability. In order to overcome this challenge, tightly coupling the production and its axiomatization processes is required in the paradigm of Industry 4.0. This technology bridges together a vast amount of new interconnected smart devices being mostly battery powered. Batteries are the heart of industrial motive power and electric energy storing solutions in the infrastructures of today. The charges related to the batteries are among the biggest cost (2.000--5.000 EUR per unit). Unfortunately, the batteries are not always treated properly and the badly managed ones lose their ability to store energy quickly. In this work, we present the developed modular Cloud solution utilizing Solution as a Service (SaaS) to monitor and manage industrial power unit systems. Modular approach is realized using simple miniature non-intrusive wireless sensors combined with cloud platform that provides the battery intelligence.",
isbn="978-3-319-67380-6"
}

@InProceedings{Dikaleh:2018:BCS:3291291.3291336,
  author    = {Dikaleh, Serjik and Charpentier, Eric and Liu, John and DeLima, Neil and Yuen, Vince},
  title     = {Build a Cognitive Serverless Slack App with IBM Cloud Functions \&\#38; IBM Watson API},
  booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
  year      = {2018},
  series    = {CASCON '18},
  pages     = {354--355},
  address   = {Riverton, NJ, USA},
  publisher = {IBM Corp.},
  acmid     = {3291336},
  location  = {Markham, Ontario, Canada},
  numpages  = {2},
  url       = {http://dl.acm.org/citation.cfm?id=3291291.3291336},
}

@InProceedings{Dziurzanski:2018:VMO:3205455.3205501,
  author    = {Dziurzanski, Piotr and Swan, Jerry and Indrusiak, Leandro Soares},
  title     = {Value-based Manufacturing Optimisation in Serverless Clouds for Industry 4.0},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  year      = {2018},
  series    = {GECCO '18},
  pages     = {1222--1229},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3205501},
  doi       = {10.1145/3205455.3205501},
  isbn      = {978-1-4503-5618-3},
  keywords  = {FaaS, function as a service, genetic algorithm, plant optimisation, serverless clouds, stopping condition, value curve},
  location  = {Kyoto, Japan},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/3205455.3205501},
}

@Article{7912239,
  author   = {A. {Eivy}},
  title    = {Be Wary of the Economics of "Serverless" Cloud Computing},
  journal  = {IEEE Cloud Computing},
  year     = {2017},
  volume   = {4},
  number   = {2},
  pages    = {6-12},
  month    = {March},
  issn     = {2325-6095},
  abstract = {One of the latest developments in cloud computing is usually called "serverless" computing, despite the fact that servers are still where processing takes place. The economic benefits of serverless computing heavily depend on the execution behavior and volumes of the application workload. Serverless has the potential to be a great abstraction offering economic advantages for simple workflows, however it's important to model the economic impact of your architecture and operation choices.},
  doi      = {10.1109/MCC.2017.32},
  keywords = {cloud computing;socio-economic effects;serverless cloud computing;economic benefits;economic advantages;economic impact;Cloud computing;Economics;Pricing;Servers;Google;Scalability;cloud computing;serverless;cloud economics;pap-per-use resources;frameworks},
}

@InProceedings{8567674,
  author    = {T. {Elgamal}},
  title     = {Costless: Optimizing Cost of Serverless Computing through Function Fusion and Placement},
  booktitle = {2018 IEEE/ACM Symposium on Edge Computing (SEC)},
  year      = {2018},
  pages     = {300-312},
  month     = {Oct},
  abstract  = {Serverless computing has recently experienced significant adoption by several applications, especially Internet of Things (IoT) applications. In serverless computing, rather than deploying and managing dedicated virtual machines, users are able to deploy individual functions, and pay only for the time that their code is actually executing. However, since serverless platforms are relatively new, they have a completely different pricing model that depends on the memory, duration, and the number of executions of a sequence/workflow of functions. In this paper we present an algorithm that optimizes the price of serverless applications in AWS Lambda. We first describe the factors affecting price of serverless applications which include: (1) fusing a sequence of functions, (2) splitting functions across edge and cloud resources, and (3) allocating the memory for each function. We then present an efficient algorithm to explore different function fusion-placement solutions and find the solution that optimizes the application's price while keeping the latency under a certain threshold. Our results on image processing workflows show that the algorithm can find solutions optimizing the price by more than 35%-57% with only 5%-15% increase in latency. We also show that our algorithm can find non-trivial memory configurations that reduce both latency and price.},
  doi       = {10.1109/SEC.2018.00029},
  keywords  = {cloud computing;image fusion;Internet of Things;virtual machines;serverless computing;serverless platforms;serverless applications;Internet of Things;virtual machines;splitting functions;function fusion-placement solutions;AWS Lambda;edge resources;cloud resources;image processing workflows;nontrivial memory configuration;Computational modeling;Fuses;Pricing;Cloud computing;Internet of Things;Memory management;Face;Serverless;Edge computing;AWS Lambda;Cloud Computing;Cost Optimization},
}

@InProceedings{vanEyk:2018:SRC:3185768.3186308,
  author    = {van Eyk, Erwin and Iosup, Alexandru and Abad, Cristina L. and Grohmann, Johannes and Eismann, Simon},
  title     = {A SPEC RG Cloud Group's Vision on the Performance Challenges of FaaS Cloud Architectures},
  booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
  year      = {2018},
  series    = {ICPE '18},
  pages     = {21--24},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3186308},
  doi       = {10.1145/3185768.3186308},
  isbn      = {978-1-4503-5629-9},
  keywords  = {FaaS, benchmarking, function-as-a-service, performance evaluation, reference architecture, serverless computing},
  location  = {Berlin, Germany},
  numpages  = {4},
  url       = {http://doi.acm.org/10.1145/3185768.3186308},
}

@InProceedings{vanEyk:2017:SCG:3154847.3154848,
  author    = {van Eyk, Erwin and Iosup, Alexandru and Seif, Simon and Th\"{o}mmes, Markus},
  title     = {The SPEC Cloud Group's Research Vision on FaaS and Serverless Architectures},
  booktitle = {Proceedings of the 2Nd International Workshop on Serverless Computing},
  year      = {2017},
  series    = {WoSC '17},
  pages     = {1--4},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3154848},
  doi       = {10.1145/3154847.3154848},
  isbn      = {978-1-4503-5434-9},
  keywords  = {FaaS, cloud computing, serverless, software architecture, vision},
  location  = {Las Vegas, Nevada},
  numpages  = {4},
  url       = {http://doi.acm.org/10.1145/3154847.3154848},
}

@Article{8481652,
  author   = {E. {van Eyk} and L. {Toader} and S. {Talluri} and L. {Versluis} and A. {Uță} and A. {Iosup}},
  title    = {Serverless is More: From PaaS to Present Cloud Computing},
  journal  = {IEEE Internet Computing},
  year     = {2018},
  volume   = {22},
  number   = {5},
  pages    = {8-17},
  month    = {Sep.},
  issn     = {1089-7801},
  abstract = {In the late-1950s, leasing time on an IBM 704 cost hundreds of dollars per minute. Today, cloud computing, that is, using IT as a service, on-demand and pay-per-use, is a widely used computing paradigm that offers large economies of scale. Born from a need to make platform as a service (PaaS) more accessible, fine-grained, and affordable, serverless computing has garnered interest from both industry and academia. This article aims to give an understanding of these early days of serverless computing: what it is, where it comes from, what is the current status of serverless technology, and what are its main obstacles and opportunities.},
  doi      = {10.1109/MIC.2018.053681358},
  keywords = {cloud computing;serverless computing;serverless technology;PaaS;affordable computing;cloud computing;computing paradigm;IT service;Cloud computing;Internet;Economics;Servers;serverless;function-as-a-service;cloud computing;workflows;internet;internet computing},
}

@InProceedings{8549544,
  author    = {M. {Ezzeddine} and R. {Morcel} and H. {Artail} and M. A. R. {Saghir} and H. {Akkary} and H. {Hajj}},
  title     = {RESTful Hardware Microservices Using Reconfigurable Networked Accelerators in Cloud and Edge Datacenters},
  booktitle = {2018 IEEE 7th International Conference on Cloud Networking (CloudNet)},
  year      = {2018},
  pages     = {1-4},
  month     = {Oct},
  abstract  = {We propose enabling cloud datacenters with Reconfigurable Networked Accelerators RNAs. RNAs are FPGA and memory compute nodes connected to the main network of the datacenter. To enable seamless integration of RNAs, we propose RESTful hardware microservices in cloud datacenters. We show how a front-end model view controller (MVC) web application can issue a call to remote RNA-accelerated RESTful microservices to decrease the latency of a single client query and increase the throughput of clients served. As a use case, we investigate just in time classification of client uploaded media (e.g., images, videos, etc.) against adult or hateful content. The system architecture is implemented using Spring MVC (Spring Boot) and AlexNet convolutional neural network CNN for image classification. Observed results show up to more than 10x improvements in throughput and energy efficiency depending on the target RNA (FPGA) device and the level of optimization of the employed hardware classifier.},
  doi       = {10.1109/CloudNet.2018.8549544},
  keywords  = {cloud computing;computer centres;field programmable gate arrays;image classification;neural nets;power aware computing;software architecture;RESTful hardware microservices;cloud datacenters;memory compute nodes;seamless integration;remote RNA-accelerated RESTful microservices;single client query;client uploaded media;target RNA device;employed hardware classifier;front-end model view controller Web application;image classification;reconfigurable networked accelerators;RNA;Acceleration;Cloud computing;Field programmable gate arrays;Servers;Hardware;Ethernet;FPGA;HTTP;REST;model view controller;MVC;AlexNet;convolutional neural network;CNN;Spark;data center;cloud computing;representational state transfer;edge computing;fog computing;serverless;cloud functions;Spring framework},
}
@ARTICLE{2019arXiv190504460F,
author = {{Farhan Hussain}, Razin and {Amini Salehi}, Mohsen and {Semiari}, Omid},
title = "{Serverless Edge Computing for Green Oil and Gas Industry}",
journal = {arXiv e-prints},
keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
year = "2019",
month = "May",
eid = {arXiv:1905.04460},
pages = {arXiv:1905.04460},
archivePrefix = {arXiv},
eprint = {1905.04460},
primaryClass = {cs.DC},
adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190504460F},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@InProceedings{8457817,
  author    = {L. {Feng} and P. {Kudva} and D. {Da Silva} and J. {Hu}},
  title     = {Exploring Serverless Computing for Neural Network Training},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {334-341},
  month     = {July},
  abstract  = {Serverless or functions as a service runtimes have shown significant benefits to efficiency and cost for event-driven cloud applications. Although serverless runtimes are limited to applications requiring lightweight computation and memory, such as machine learning prediction and inference, they have shown improvements on these applications beyond other cloud runtimes. Training deep learning can be both compute and memory intensive. We investigate the use of serverless runtimes while leveraging data parallelism for large models, show the challenges and limitations due to the tightly coupled nature of such models, and propose modifications to the underlying runtime implementations that would mitigate them. For hyperparameter optimization of smaller deep learning models, we show that serverless runtimes can provide significant benefit.},
  doi       = {10.1109/CLOUD.2018.00049},
  issn      = {2159-6190},
  keywords  = {cloud computing;learning (artificial intelligence);neural nets;serverless computing;neural network training;serverless functions;service runtimes;event-driven cloud applications;serverless runtimes;lightweight computation;machine learning prediction;cloud runtimes;lightweight memory;machine learning inference;deep learning models;hyperparameter optimization;data parallelism;Servers;Training;Merging;Runtime;Neural networks;Data transfer;Machine learning;serverless computing;cloud computing;deep learning;cloud scaling;cloud cost and performance},
}

@Article{doi:10.1002/cpe.4792,
  author   = {Figiela, Kamil and Gajek, Adam and Zima, Adam and Obrok, Beata and Malawski, Maciej},
  title    = {Performance evaluation of heterogeneous cloud functions},
  journal  = {Concurrency and Computation: Practice and Experience},
  year     = {2018},
  volume   = {30},
  number   = {23},
  pages    = {e4792},
  note     = {e4792 cpe.4792},
  abstract = {Summary Cloud Functions, often called Function-as-a-Service (FaaS), pioneered by AWS Lambda, are an increasingly popular method of running distributed applications. As in other cloud offerings, cloud functions are heterogeneous due to variations in underlying hardware, runtime systems, as well as resource management and billing models. In this paper, we focus on performance evaluation of cloud functions, taking into account heterogeneity aspects. We developed a cloud function benchmarking framework, consisting of one suite based on Serverless Framework and one based on HyperFlow. We deployed the CPU-intensive benchmarks: Mersenne Twister and Linpack. We measured the data transfer times between cloud functions and storage, and we measured the lifetime of the runtime environment. We evaluated all the major cloud function providers: AWS Lambda, Azure Functions, Google Cloud Functions, and IBM Cloud Functions. We made our results available online and continuously updated. We report on the results of the performance evaluation, and we discuss the discovered insights into resource allocation policies.},
  doi      = {10.1002/cpe.4792},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.4792},
  keywords = {cloud computing, cloud functions, FaaS, performance evaluation, serverless},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4792},
}
@INPROCEEDINGS{8730873,
author={H. {Flores} and P. {Nurmi} and P. {Hui}},
booktitle={2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)},
title={AI on the Move: From On-Device to On-Multi-Device},
year={2019},
volume={},
number={},
pages={310-315},
abstract={On-Device AI is an emerging paradigm that aims to make devices more intelligent, autonomous and proactive by equipping them with machine and deep learning routines for robust decision making and optimal execution in devices' operations. On-Device intelligence promises the possibility of computing huge amounts of data close to its source, e.g., sensor and multimedia data. By doing so, devices can complement their counterpart cloud services with more sophisticated functionality to provide better applications and services. However, increased computational capabilities of smart devices, wearables and IoT devices along with the emergence of services at the Edge of the network are driving the trend of migrating and distributing computation between devices. Indeed, devices can reduce the burden of executing resource intensive tasks via collaborations in the wild. While several work has shown the benefits of an opportunistic collaboration of a device with others, not much is known regarding how devices can be organized as a group as they move together. In this paper, we contribute by analyzing how dynamic group organization of devices can be utilized to distribute intelligence on the moving Edge. The key insight is that instead of On-Device solutions complementing with cloud, dynamic groups can be formed to complement each other in an On-Multi-Device manner. Thus, we highlight the challenges and opportunities from extending the scope of On-Device AI from an egocentric view to a collaborative, multi-device view.},
keywords={Artificial intelligence;Cloud computing;Collaboration;Task analysis;Device-to-device communication;Smart devices;Performance evaluation;Cloud;Edge;Cloudlet;Artificial Intelligence;Device-to-Device;Data Analytics;Serverless},
doi={10.1109/PERCOMW.2019.8730873},
ISSN={},
month={March},
}

@InProceedings{Fox:2017:CRB:3147213.3155012,
  author    = {Fox, Geoffrey},
  title     = {Components and Rationale of a Big Data Toolkit Spanning HPC, Grid, Edge and Cloud Computing},
  booktitle = {Proceedings of the10th International Conference on Utility and Cloud Computing},
  year      = {2017},
  series    = {UCC '17},
  pages     = {1--1},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3155012},
  doi       = {10.1145/3147213.3155012},
  isbn      = {978-1-4503-5149-2},
  keywords  = {cloud computing, dataflow, edge computing, global machine learning, hpc, mapreduce, mpi},
  location  = {Austin, Texas, USA},
  numpages  = {1},
  url       = {http://doi.acm.org/10.1145/3147213.3155012},
}

@Article{2017arXiv170808028F,
  author        = {{Fox}, Geoffrey C. and {Ishakian}, Vatche and {Muthusamy}, Vinod and {Slominski}, Aleksander},
  title         = {{Status of Serverless Computing and Function-as-a-Service(FaaS) in Industry and Research}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1708.08028},
  month         = {Aug},
  abstract      = {This whitepaper summarizes issues raised during the First International Workshop on Serverless Computing (WoSC) 2017 held June 5th 2017 and especially in the panel and associated discussion that concluded the workshop. We also include comments from the keynote and submitted papers. A glossary at the end (section 8) defines many technical terms used in this report. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170808028F},
  archiveprefix = {arXiv},
  eid           = {arXiv:1708.08028},
  eprint        = {1708.08028},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/ftp/arxiv/papers/1708/1708.08028.pdf},
}

@InProceedings{8475098,
  author    = {J. {Franz} and T. {Nagasuri} and A. {Wartman} and A. V. {Ventrella} and F. {Esposito}},
  title     = {Reunifying Families after a Disaster via Serverless Computing and Raspberry Pis},
  booktitle = {2018 IEEE International Symposium on Local and Metropolitan Area Networks (LANMAN)},
  year      = {2018},
  pages     = {131-132},
  month     = {June},
  abstract  = {Children constitute a vulnerable population and special considerations are necessary in order to provide proper care for them during disasters. After disasters such as Hurricane Katrina, the rapid identification and protection of separated children and their reunification with legal guardians is necessary to minimize secondary injuries (i:e, physical and sexual abuse, neglect and abduction). At Camp Gruber, an Oklahoma shelter for Louisianan's displaced by Hurricane Katrina, of the 254 children at the camp, 36 ((i.e, 14.2%) were separated from their legal guardians. It took 6 months to reunify the last children; 70% of the children were with their legal guardian after 2 weeks. Imagine not knowing for 2 weeks (or 6 months) if your children are dead or alive. To exacerbate these natural challenges, during a disaster Internet connectivity is scarse or unreliable.},
  doi       = {10.1109/LANMAN.2018.8475098},
  issn      = {1944-0375},
  keywords  = {disasters;emergency management;injuries;Internet;minimisation;paediatrics;storms;disaster Internet connectivity;serverless computing;vulnerable population;Hurricane Katrina;separated children;legal guardian;Camp Gruber;reunifying families;Raspberry Pis;secondary injuries minimization;physical abuse;sexual abuse;Oklahoma shelter;Databases;Law;Face recognition;Training;Hurricanes;Internet},
}

@Inbook{Frischbier2010,
author="Frischbier, Sebastian
and Petrov, Ilia",
editor="Sachs, Kai
and Petrov, Ilia
and Guerrero, Pablo",
title="Aspects of Data-Intensive Cloud Computing",
bookTitle="From Active Data Management to Event-Based Systems and More: Papers in Honor of Alejandro Buchmann on the Occasion of His 60th Birthday",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="57--77",
abstract="The concept of Cloud Computing is by now at the peak of public attention and adoption. Driven by several economic and technological enablers, Cloud Computing is going to change the way we have to design, maintain and optimise large-scale data-intensive software systems in the future. Moving large-scale, data-intensive systems into the Cloud may not always be possible, but would solve many of today's typical problems. In this paper we focus on the opportunities and restrictions of current Cloud solutions regarding the data model of such software systems. We identify the technological issues coming along with this new paradigm and discuss the requirements to be met by Cloud solutions in order to provide a meaningful alternative to on-premise configurations.",
isbn="978-3-642-17226-7",
doi="10.1007/978-3-642-17226-7_4",
url="https://doi.org/10.1007/978-3-642-17226-7_4"
}

@InProceedings{10.1007/978-3-030-22397-7_9,
author="Gabbrielli, Maurizio
and Giallorenzo, Saverio
and Lanese, Ivan
and Montesi, Fabrizio
and Peressotti, Marco
and Zingaro, Stefano Pio",
editor="Riis Nielson, Hanne 
and Tuosto, Emilio",
title="No More, No Less",
booktitle="Coordination Models and Languages",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="148--157",
abstract="Serverless computing, also known as Functions-as-a-Service, is a recent paradigm aimed at simplifying the programming of cloud applications. The idea is that developers design applications in terms of functions, which are then deployed on a cloud infrastructure. The infrastructure takes care of executing the functions whenever requested by remote clients, dealing automatically with distribution and scaling with respect to inbound traffic.",
isbn="978-3-030-22397-7"
}

@Article{2019arXiv190307962G,
  author        = {{Gabbrielli}, Maurizio and {Giallorenzo}, Saverio and {Lanese}, Ivan and {Montesi}, Fabrizio and {Peressotti}, Marco and {Zingaro}, Stefano Pio},
  title         = {{No more, no less - A formal model for serverless computing}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1903.07962},
  month         = {Mar},
  abstract      = {Serverless computing, also known as Functions-as-a-Service, is a recent paradigm aimed at simplifying the programming of cloud applications. The idea is that developers design applications in terms of functions, which are then deployed on a cloud infrastructure. The infrastructure takes care of executing the functions whenever requested by remote clients, dealing automatically with distribution and scaling with respect to inbound traffic. While vendors already support a variety of programming languages for serverless computing (e.g. Go, Java, Javascript, Python), as far as we know there is no reference model yet to formally reason on this paradigm. In this paper, we propose the first formal programming model for serverless computing, which combines ideas from both the λ-calculus (for functions) and the π-calculus (for communication). To illustrate our proposal, we model a real-world serverless system. Thanks to our model, we are also able to capture and pinpoint the limitations of current vendor technologies, proposing possible amendments. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190307962G},
  archiveprefix = {arXiv},
  eid           = {arXiv:1903.07962},
  eprint        = {1903.07962},
  keywords      = {Computer Science - Programming Languages},
  primaryclass  = {cs.PL},
  url           = {https://arxiv.org/pdf/1903.07962.pdf},
}

@InProceedings{Gan:2019:OBS:3297858.3304013,
  author    = {Gan, Yu and Zhang, Yanqi and Cheng, Dailun and Shetty, Ankitha and Rathi, Priyal and Katarki, Nayan and Bruno, Ariana and Hu, Justin and Ritchken, Brian and Jackson, Brendon and Hu, Kelvin and Pancholi, Meghna and He, Yuan and Clancy, Brett and Colen, Chris and Wen, Fukang and Leung, Catherine and Wang, Siyuan and Zaruvinsky, Leon and Espinosa, Mateo and Lin, Rick and Liu, Zhongling and Padilla, Jake and Delimitrou, Christina},
  title     = {An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud \&\#38; Edge Systems},
  booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
  year      = {2019},
  series    = {ASPLOS '19},
  pages     = {3--18},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3304013},
  doi       = {10.1145/3297858.3304013},
  isbn      = {978-1-4503-6240-5},
  keywords  = {acceleration, cloud computing, cluster management, datacenters, fpga, microservices, qos, serverless},
  location  = {Providence, RI, USA},
  numpages  = {16},
  url       = {http://doi.acm.org/10.1145/3297858.3304013},
}

@Article{8125550,
  author   = {D. {Gannon} and R. {Barga} and N. {Sundaresan}},
  title    = {Cloud-Native Applications},
  journal  = {IEEE Cloud Computing},
  year     = {2017},
  volume   = {4},
  number   = {5},
  pages    = {16-21},
  month    = {Sep.},
  issn     = {2325-6095},
  abstract = {Cloud-native is a term that is invoked often but seldom defined beyond saying “we built it in the cloud” as opposed to “on-prem”. However, there is now an emerging consensus around key ideas and informal applications design patterns that have been adopted and used in many successful cloud applications. In this introduction, we will describe these cloud-native concepts and illustrate them with examples. We will also look at the technical trends that may give us an idea about the future of cloud applications. We begin by discussing the basic properties that many cloud-native apps have in common. Once we have characterized them, we can then describe how these properties emerge from the technical design patterns.},
  doi      = {10.1109/MCC.2017.4250939},
  keywords = {cloud computing;cloud-native applications;technical design patterns;cloud-native;distributed computing;microservices;serverless;cloud computing},
}

@InProceedings{Garcia-Valdez:2018:MEA:3205651.3205719,
  author    = {Garc\'{\i}a-Valdez, Jos{\'e}-Mario and Merelo-Guerv\'{o}s, Juan-Juli\'{a}n},
  title     = {A Modern, Event-based Architecture for Distributed Evolutionary Algorithms},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  year      = {2018},
  series    = {GECCO '18},
  pages     = {233--234},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3205719},
  doi       = {10.1145/3205651.3205719},
  isbn      = {978-1-4503-5764-7},
  keywords  = {cloud computing, distributed computing, event-based systems, functions as a service, microservices, stateless algorithms},
  location  = {Kyoto, Japan},
  numpages  = {2},
  url       = {http://doi.acm.org/10.1145/3205651.3205719},
}

@InProceedings{10.1007/978-3-319-74781-1_15,
author="Garriga, Martin",
editor="Cerone, Antonio
and Roveri, Marco",
title="Towards a Taxonomy of Microservices Architectures",
booktitle="Software Engineering and Formal Methods",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="203--218",
abstract="The microservices architectural style is gaining more and more momentum for the development of applications as suites of small, autonomous, and conversational services, which are then easy to understand, deploy and scale. However, the proliferation of approaches leveraging microservices calls for a systematic way of analyzing and assessing them as a completely new ecosystem: the first cloud-native architectural style. This paper defines a preliminary analysis framework in the form of a taxonomy of concepts, encompassing the whole microservices lifecycle, as well as organizational aspects. This framework is necessary to enable effective exploration, understanding, assessing, comparing, and selecting microservice-based models, languages, techniques, platforms, and tools. Then, we analyze state of the art approaches related to microservices using this taxonomy to provide a holistic perspective of available solutions.",
isbn="978-3-319-74781-1"
}

@InProceedings{8582355,
  author    = {X. {Geng} and O. {Ma} and Y. {Pei} and Z. {Xu} and W. {Zeng} and J. {Zou}},
  title     = {Research on Early Warning System of Power Network Overloading Under Serverless Architecture},
  booktitle = {2018 2nd IEEE Conference on Energy Internet and Energy System Integration (EI2)},
  year      = {2018},
  pages     = {1-6},
  month     = {Oct},
  abstract  = {Under the background of massive computing resources in power grid, smart grid has become the direction and trend of power industry development. In order to ensure the safe and stable operation of smart grid, the monitoring of the grid load should be strengthened and a reasonable solution should be arranged in time when the grid load exceeded the standard. So early warning system of power network overloading under serverless architecture was proposed. The main advantages of the combination of smart grid and serverless architecture were analyzed in detail, and the technical feasibility and economic feasibility of the early warning system for grid overload under the serverless mode were analyzed by examples.},
  doi       = {10.1109/EI2.2018.8582355},
  keywords  = {alarm systems;electricity supply industry;power system measurement;power system stability;smart power grids;economic feasibility;grid load monitoring;power industry development;smart grid;power grid;massive computing resources;power network overloading;serverless mode;grid overload;early warning system;serverless architecture;Cloud computing;Smart grids;FAA;Computer architecture;Monitoring;Alarm systems;serverless architecture;overload warning;smart grid;cloud computing},
}

@Article{2018arXiv180801353G,
  author        = {{Gibert Renart}, Eduard and {Balouek-Thomert}, Daniel and {Parashar}, Manish},
  title         = {{Edge Based Data-Driven Pipelines (Technical Report)}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1808.01353},
  month         = {Aug},
  abstract      = {This research reports investigates an edge on-device stream processing platform, which extends the serverless com- puting model to the edge to help facilitate real-time data analytics across the cloud and edge in a uniform manner. We investigate associated use cases and architectural design. We deployed and tested our system on edge devices (Raspberry Pi and Android Phone), which proves that stream processing analytics can be performed at the edge of the network with single board computers in a real-time fashion.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180801353G},
  archiveprefix = {arXiv},
  eid           = {arXiv:1808.01353},
  eprint        = {1808.01353},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1808.01353.pdf},
}

@Article{GIMENEZALVENTOSA2019259,
  author   = {V. Giménez-Alventosa and Germán Moltó and Miguel Caballer},
  title    = {A framework and a performance assessment for serverless MapReduce on AWS Lambda},
  journal  = {Future Generation Computer Systems},
  year     = {2019},
  volume   = {97},
  pages    = {259 - 274},
  issn     = {0167-739X},
  abstract = {MapReduce is one of the most widely used programming models for analysing large-scale datasets, i.e. Big Data. In recent years, serverless computing and, in particular, Functions as a Service (FaaS) has surged as an execution model in which no explicit management of servers (e.g. virtual machines) is performed by the user. Instead, the Cloud provider dynamically allocates resources to the function invocations and fine-grained billing is introduced depending on the execution time and allocated memory, as exemplified by AWS Lambda. In this article, a high-performant serverless architecture has been created to execute MapReduce jobs on AWS Lambda using Amazon S3 as the storage backend. In addition, a thorough assessment has been carried out to study the suitability of AWS Lambda as a platform for the execution of High Throughput Computing jobs. The results indicate that AWS Lambda provides a convenient computing platform for general-purpose applications that fit within the constraints of the service (15 min of maximum execution time, 3008 MB of RAM and 512 MB of disk space) but it exhibits an inhomogeneous performance behaviour that may jeopardise adoption for tightly coupled computing jobs.},
  doi      = {https://doi.org/10.1016/j.future.2019.02.057},
  keywords = {MapReduce, Serverless, Cloud computing, Elasticity},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X18325172},
}

@InProceedings{Glikson:2017:DEC:3078468.3078497,
  author    = {Glikson, Alex and Nastic, Stefan and Dustdar, Schahram},
  title     = {Deviceless Edge Computing: Extending Serverless Computing to the Edge of the Network},
  booktitle = {Proceedings of the 10th ACM International Systems and Storage Conference},
  year      = {2017},
  series    = {SYSTOR '17},
  pages     = {28:1--28:1},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3078497},
  articleno = {28},
  doi       = {10.1145/3078468.3078497},
  isbn      = {978-1-4503-5035-8},
  keywords  = {edge computing, function as a service, serverless computing},
  location  = {Haifa, Israel},
  numpages  = {1},
  url       = {http://doi.acm.org/10.1145/3078468.3078497},
}

@Article{2018arXiv181102638G,
  author        = {{Gorlatova}, Maria and {Inaltekin}, Hazer and {Chiang}, Mung},
  title         = {{Characterizing Task Completion Latencies in Fog Computing}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1811.02638},
  month         = {Nov},
  abstract      = {Fog computing, which distributes computing resources to multiple locations between the Internet of Things (IoT) devices and the cloud, is attracting considerable attention from academia and industry. Yet, despite the excitement about the potential of fog computing, few comprehensive quantitative characteristics of the properties of fog computing architectures have been conducted. In this paper we examine the properties of task completion latencies in fog computing. First, we present the results of our empirical benchmarking-based study of task completion latencies. The study covered a range of settings, and uniquely considered both traditional and serverless fog computing execution points. It demonstrated the range of execution point characteristics in different locations and the relative stability of latency characteristics for a given location. It also highlighted properties of serverless execution that are not incorporated in existing fog computing algorithms. Second, we present a framework we developed for co-optimizing task completion quality and latency, which was inspired by the insights of our empirical study. We describe fog computing task assignment problems we formulated under this framework, and present the algorithms we developed for solving them.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181102638G},
  archiveprefix = {arXiv},
  eid           = {arXiv:1811.02638},
  eprint        = {1811.02638},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1811.02638.pdf},
}

@Article{2019arXiv190308857G,
  author        = {{Gupta}, Vipul and {Kadhe}, Swanand and {Courtade}, Thomas and {Mahoney}, Michael W. and {Ramchandran}, Kannan},
  title         = {{OverSketched Newton: Fast Convex Optimization for Serverless Systems}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1903.08857},
  month         = {Mar},
  abstract      = {Motivated by recent developments in serverless systems for large-scale machine learning as well as improvements in scalable randomized matrix algorithms, we develop OverSketched Newton, a randomized Hessian-based optimization algorithm to solve large-scale smooth and strongly-convex problems in serverless systems. OverSketched Newton leverages matrix sketching ideas from Randomized Numerical Linear Algebra to compute the Hessian approximately. These sketching methods lead to inbuilt resiliency against stragglers that are a characteristic of serverless architectures. We establish that OverSketched Newton has a linear-quadratic convergence rate, and we empirically validate our results by solving large-scale supervised learning problems on real-world datasets. Experiments demonstrate a reduction of ~50% in total running time on AWS Lambda, compared to state-of-the-art distributed optimization schemes. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190308857G},
  archiveprefix = {arXiv},
  eid           = {arXiv:1903.08857},
  eprint        = {1903.08857},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Information Theory, Computer Science - Machine Learning},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1903.08857.pdf},
}

@InProceedings{8622139,
  author    = {V. {Gupta} and S. {Wang} and T. {Courtade} and K. {Ramchandran}},
  title     = {OverSketch: Approximate Matrix Multiplication for the Cloud},
  booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
  year      = {2018},
  pages     = {298-304},
  month     = {Dec},
  abstract  = {We propose OverSketch, an approximate algorithm for distributed matrix multiplication in serverless computing. OverSketch leverages ideas from matrix sketching and high-performance computing to enable cost-efficient multiplication that is resilient to faults and straggling nodes pervasive in low-cost serverless architectures. We establish statistical guarantees on the accuracy of OverSketch and empirically validate our results by solving a large-scale linear program using interior-point methods and demonstrate a 34% reduction in compute time on AWS Lambda.},
  doi       = {10.1109/BigData.2018.8622139},
  keywords  = {approximation theory;cloud computing;computational complexity;linear programming;mathematics computing;matrix multiplication;matrix sketching;high-performance computing;cost-efficient multiplication;straggling nodes;low-cost serverless architectures;approximate matrix multiplication;approximate algorithm;distributed matrix multiplication;serverless computing;OverSketch;Cloud computing;Bandwidth;Distributed algorithms;Memory management;Partitioning algorithms;serverless computing;straggler mitigation;sketched matrix multiplication},
}

@InProceedings{Hafeez:2018:SAP:3284014.3284019,
  author    = {Hafeez, Faisal and Nasirifard, Pezhman and Jacobsen, Hans-Arno},
  title     = {A Serverless Approach to Publish/Subscribe Systems},
  booktitle = {Proceedings of the 19th International Middleware Conference (Posters)},
  year      = {2018},
  series    = {Middleware '18},
  pages     = {9--10},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3284019},
  doi       = {10.1145/3284014.3284019},
  isbn      = {978-1-4503-6109-5},
  keywords  = {Function as a service (FaaS), Serverless, content-based pub/sub, function-based pub/sub, topic-based pub/sub},
  location  = {Rennes, France},
  numpages  = {2},
  url       = {http://doi.acm.org/10.1145/3284014.3284019},
}

@InProceedings{Hall:2019:EMS:3302505.3310084,
  author    = {Hall, Adam and Ramachandran, Umakishore},
  title     = {An Execution Model for Serverless Functions at the Edge},
  booktitle = {Proceedings of the International Conference on Internet of Things Design and Implementation},
  year      = {2019},
  series    = {IoTDI '19},
  pages     = {225--236},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3310084},
  doi       = {10.1145/3302505.3310084},
  isbn      = {978-1-4503-6283-2},
  keywords  = {FaaS, edge computing, fog computing, function-as-a-service, serverless, webassembly},
  location  = {Montreal, Quebec, Canada},
  numpages  = {12},
  url       = {http://doi.acm.org/10.1145/3302505.3310084},
}

@InProceedings{8576921,
  author    = {E. {Handoyo} and M. {Arfan} and Y. A. A. {Soetrisno} and M. {Somantri} and A. {Sofwan} and E. W. {Sinuraya}},
  title     = {Ticketing Chatbot Service using Serverless NLP Technology},
  booktitle = {2018 5th International Conference on Information Technology, Computer, and Electrical Engineering (ICITACEE)},
  year      = {2018},
  pages     = {325-330},
  month     = {Sep.},
  abstract  = {Personal assistant using a human operator need some time to process single request such as ticket booking, ordering something, and get services. One request can contain many queries for some information provided on the internet. Business performance values time efficiency so must be considered an alternative way to take request. Chatbot can give 24 hours service which can become an advantage besides using a human personal assistant. Chatbot acts like routing agent that can classify user context in conversation. Chatbot helped with natural language processing (NLP) to analyze the request and extract some keyword information. One important process in NLP is morphological analysis and part of speech (POS) tagging. POS help to parse the meaning of chat text based on a set of rules. The rule base is specific to some language and designed to capture all the keyword relies on chat text. Keyword in booking conversation term is like departure and destination city and also the date of flight. There is a variation from a user determining city and date. NLP in booking confirmation has a task to analyze various pattern describing ordering requests like city and date. Messenger bot would be an example of assistance that can help user connected to many services some like ticketing service through conversation interaction. The contribution of this research is to conduct some scenario that happening in ordering tickets. This research conduct that chatbot can help acts as customer service, based on the conducted scenario and show an F-measure score of 89.65%.},
  doi       = {10.1109/ICITACEE.2018.8576921},
  keywords  = {customer services;Internet;natural language processing;query processing;reservation computer systems;text analysis;ticketing service;customer service;ticketing chatbot service;serverless NLP technology;human operator;ticket booking;human personal assistant;part of speech tagging;POS tagging;Internet;NLP;keyword information extraction;chat text;natural language processing;Facebook;Electrical engineering;Urban areas;Natural language processing;Programming;Task analysis;Testing;chatbot;routing agent;conversation;NLP;interaction;intent},
}

@Article{2018arXiv181203651H,
  author        = {{Hellerstein}, Joseph M. and {Faleiro}, Jose and {Gonzalez}, Joseph E. and {Schleier-Smith}, Johann and {Sreekanti}, Vikram and {Tumanov}, Alexey and {Wu}, Chenggang},
  title         = {{Serverless Computing: One Step Forward, Two Steps Back}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1812.03651},
  month         = {Dec},
  abstract      = {Serverless computing offers the potential to program the cloud in an autoscaling, pay-as-you go manner. In this paper we address critical gaps in first-generation serverless computing, which place its autoscaling potential at odds with dominant trends in modern computing: notably data-centric and distributed computing, but also open source and custom hardware. Put together, these gaps make current serverless offerings a bad fit for cloud innovation and particularly bad for data systems innovation. In addition to pinpointing some of the main shortfalls of current serverless architectures, we raise a set of challenges we believe must be met to unlock the radical potential that the cloud---with its exabytes of storage and millions of cores---should offer to innovative developers. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181203651H},
  archiveprefix = {arXiv},
  eid           = {arXiv:1812.03651},
  eprint        = {1812.03651},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Databases},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1812.03651.pdf},
}

@InProceedings{Hendrickson:2016:SCO:3027041.3027047,
  author    = {Hendrickson, Scott and Sturdevant, Stephen and Harter, Tyler and Venkataramani, Venkateshwaran and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
  title     = {Serverless Computation with openLambda},
  booktitle = {Proceedings of the 8th USENIX Conference on Hot Topics in Cloud Computing},
  year      = {2016},
  series    = {HotCloud'16},
  pages     = {33--39},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3027047},
  location  = {Denver, CO},
  numpages  = {7},
  url       = {http://dl.acm.org/citation.cfm?id=3027041.3027047},
}

@Article{8344774,
  author   = {L. F. {Herrera-Quintero} and J. C. {Vega-Alfonso} and K. B. A. {Banse} and E. {Carrillo Zambrano}},
  title    = {Smart ITS Sensor for the Transportation Planning Based on IoT Approaches Using Serverless and Microservices Architecture},
  journal  = {IEEE Intelligent Transportation Systems Magazine},
  year     = {2018},
  volume   = {10},
  number   = {2},
  pages    = {17-27},
  month    = {Summer},
  issn     = {1939-1390},
  abstract = {Currently, there are many challenges in the transportation scope that researchers are attempting to resolve, and one of them is transportation planning. The main contribution of this paper is the design and implementation of an ITS (Intelligent Transportation Systems) smart sensor prototype that incorporates and combines the Internet of Things (IoT) approaches using the Serverless and Microservice Architecture, to help the transportation planning for Bus Rapid Transit (BRT) systems. The ITS smart sensor prototype can detect several Bluetooth signals of several devices (e.g., from mobile phones) that people use while travelling by the BRT system (e.g., in Bogota city). From that information, the ITS smart-sensor prototype can create an O/D (origin/destiny) matrix for several BRT routes, and this information can be used by the Administrator Authorities (AA) to produce a suitable transportation planning for the BRT systems. In addition, this information can be used by the center of traffic management and the AA from ITS cloud services using the Serverless and Microservice architecture.},
  doi      = {10.1109/MITS.2018.2806620},
  keywords = {Bluetooth;intelligent sensors;intelligent transportation systems;Internet of Things;mobile radio;planning;rapid transit systems;road vehicles;traffic engineering computing;transportation;transportation planning;ITS smart sensor prototype;Intelligent Transportation Systems;BRT system;smart-sensor prototype;smart ITS sensor;IoT approaches;microservices architecture;bus rapid transit systems;Internet of Things;serverless architecture;Bluetooth signals;origin-destiny matrix;traffic management;Transportation;Planning;Cloud computing;Bluetooth;Urban areas;Computer architecture;Internet of Things;Intelligent sensors;Intelligent transportation systems},
}

@InProceedings{Hong:2018:GSS:3277180.3277191,
  author    = {Hong, Sanghyun and Srivastava, Abhinav and Shambrook, William and Dumitras, Tudor},
  title     = {Go Serverless: Securing Cloud via Serverless Design Patterns},
  booktitle = {Proceedings of the 10th USENIX Conference on Hot Topics in Cloud Computing},
  year      = {2018},
  series    = {HotCloud'18},
  pages     = {11--11},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3277191},
  location  = {Boston, MA, USA},
  numpages  = {1},
  url       = {http://dl.acm.org/citation.cfm?id=3277180.3277191},
}

@InProceedings{10.1007/978-3-030-13342-9_15,
author="Horovitz, Shay
and Amos, Roei
and Baruch, Ohad
and Cohen, Tomer
and Oyar, Tal
and Deri, Afik",
editor="Coppola, Massimo
and Carlini, Emanuele
and D'Agostino, Daniele
and Altmann, J{\"o}rn
and Ba{\~{n}}ares, Jos{\'e} {\'A}ngel",
title="FaaStest - Machine Learning Based Cost and Performance FaaS Optimization",
booktitle="Economics of Grids, Clouds, Systems, and Services",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="171--186",
abstract="With the emergence of Function-as-a-Service (FaaS) in the cloud, pay-per-use pricing models became available along with the traditional fixed price model for VMs and increased the complexity of selecting the optimal platform for a given service. We present FaaStest - an autonomous solution for cost and performance optimization of FaaS services by taking a hybrid approach - learning the behavioral patterns of the service and dynamically selecting the optimal platform. Moreover, we combine a prediction based solution for reducing cold starts of FaaS services. Experiments present a reduction of over 50{\%} in cost and over 90{\%} in response time for FaaS calls.",
isbn="978-3-030-13342-9"
}

@InProceedings{10.1007/978-3-319-69035-3_17,
author="HoseinyFarahabady, MohammadReza
and Lee, Young Choon
and Zomaya, Albert Y.
and Tari, Zahir",
editor="Maximilien, Michael
and Vallecillo, Antonio
and Wang, Jianmin
and Oriol, Marc",
title="A QoS-Aware Resource Allocation Controller for Function as a Service (FaaS) Platform",
booktitle="Service-Oriented Computing",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="241--255",
abstract="Function as a Service (FaaS) is a recent event-driven serverless paradigm that allows enterprises to build their applications in a fault tolerant distributed manner. Having been considered as an attractive replacement of traditional Service Oriented Architecture (SOA), the FaaS platform leverages the management of massive data sets or the handling of event streams. However, the realization of such leverage is largely dependent on the effective exploitation of FaaS elasticity/scalability.",
isbn="978-3-319-69035-3"
}

@InProceedings{8025307,
  author    = {M. {HoseinyFarahabady} and J. {Taheri} and Z. {Tari} and A. Y. {Zomaya}},
  title     = {A Dynamic Resource Controller for a Lambda Architecture},
  booktitle = {2017 46th International Conference on Parallel Processing (ICPP)},
  year      = {2017},
  pages     = {332-341},
  month     = {Aug},
  abstract  = {Lambda architecture is a novel event-driven serverless paradigm that allows companies to build scalable and reliable enterprise applications. As an attractive alternative to traditional service oriented architecture (SOA), Lambda architecture can be used in many use cases including BI tools, in-memory graph databases, OLAP, and streaming data processing. In practice, an important aim of Lambda's service providers is devising an efficient way to co-locate multiple Lambda functions with different attributes into a set of available computing resources. However, previous studies showed that consolidated workloads can compete fiercely for shared resources, resulting in severe performance variability/degradation. This paper proposes a resource allocation mechanism for a Lambda platform based on the model predictive control framework. Performance evaluation is carried out by comparing the proposed solution with multiple resource allocation heuristics, namely enhanced versions of spread and binpack, and best-effort approaches. Results confirm that the proposed controller increases the overall resource utilization by 37% on average and achieves a significant improvement in preventing QoS violation incidents compared to others.},
  doi       = {10.1109/ICPP.2017.42},
  issn      = {2332-5690},
  keywords  = {bin packing;business data processing;data handling;performance evaluation;predictive control;resource allocation;software architecture;performance variability;QoS violation prevention;resource utilization;spread approach;binpack approach;best-effort approach;performance evaluation;model predictive control;resource allocation;performance degradation;shared resources;computing resources;Lambda functions;Lambda service providers;enterprise applications;event-driven serverless paradigm;Lambda architecture;dynamic resource controller;Quality of service;Resource management;Sensitivity;Memory management;Interference;Servers;Dynamic Resource Allocation;Performance degradation;Lambda Platform Processing;Shared Resource Interference},
}

@Article{8126823,
  author   = {M. R. {HoseinyFarahabady} and A. Y. {Zomaya} and Z. {Tari}},
  title    = {A Model Predictive Controller for Managing QoS Enforcements and Microarchitecture-Level Interferences in a Lambda Platform},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  year     = {2018},
  volume   = {29},
  number   = {7},
  pages    = {1442-1455},
  month    = {July},
  issn     = {1045-9219},
  abstract = {Lambda paradigm, also known as Function as a Service (FaaS), is a novel event-driven concept that allows companies to build scalable and reliable enterprise applications in an off-premise computing data-center as a serverless solution. In practice, however, an important goal for the service provider of a Lambda platform is to devise an efficient way to consolidate multiple Lambda functions in a single host. While the majority of existing resource management solutions use only operating-system level metrics (e.g., average utilization of computing and I/O resources) to allocate the available resources among the submitted workloads in a balanced way, a resource allocation schema that is oblivious to the issue of shared-resource contention can result in a significant performance variability and degradation within the entire platform. This paper proposes a predictive controller scheme that dynamically allocates resources in a Lambda platform. This scheme uses a prediction tool to estimate the future rate of every event stream and takes into account the quality of service enforcements requested by the owner of each Lambda function. This is formulated as an optimization problem where a set of cost functions are introduced (i) to reduce the total QoS violation incidents; (ii) to keep the CPU utilization level within an accepted range; and (iii) to avoid the fierce contention among collocated applications for obtaining shared resources. Performance evaluation is carried out by comparing the proposed solution with an enhanced interference-aware version of three well-known heuristics, namely spread, binpack (the two native clustering solutions employed by Docker Swarm) and best-effort resource allocation schema. Experimental results show that the proposed controller improves the overall performance (in terms of reducing the end-to-end response time) by 14.9 percent on average compared to the best result of the other heuristics. The proposed solution also increases the overall CPU utilization by 18 percent on average (for lightweight workloads), while achieves an average 87 percent (maximum 146 percent) improvement in preventing QoS violation incidents.},
  doi      = {10.1109/TPDS.2017.2779502},
  keywords = {application program interfaces;cloud computing;computer centres;optimal control;optimisation;predictive control;quality of service;resource allocation;software engineering;virtualisation;model predictive controller;QoS enforcements;microarchitecture-level interferences;Lambda platform;Lambda paradigm;reliable enterprise applications;serverless solution;multiple Lambda functions;resource management solutions;operating-system level metrics;resource allocation schema;shared-resource contention;function as a service;quality of service;total QoS violation incident reduction;CPU utilization;enhanced interference-awareness;Docker Swarm;Resource management;Quality of service;Interference;Bandwidth;Measurement;Servers;Cloud computing;Serverless lambda platform;function as a service (FaaS);model predictive control;dynamic resource allocation/scheduling;performance degradation},
}

@InProceedings{8360337,
  author    = {V. {Ishakian} and V. {Muthusamy} and A. {Slominski}},
  title     = {Serving Deep Learning Models in a Serverless Platform},
  booktitle = {2018 IEEE International Conference on Cloud Engineering (IC2E)},
  year      = {2018},
  pages     = {257-262},
  month     = {April},
  abstract  = {Serverless computing has emerged as a compelling paradigm for the development and deployment of a wide range of event based cloud applications. At the same time, cloud providers and enterprise companies are heavily adopting machine learning and Artificial Intelligence to either differentiate themselves, or provide their customers with value added services. In this work we evaluate the suitability of a serverless computing environment for the inferencing of large neural network models. Our experimental evaluations are executed on the AWS Lambda environment using the MxNet deep learning framework. Our experimental results show that while the inferencing latency can be within an acceptable range, longer delays due to cold starts can skew the latency distribution and hence risk violating more stringent SLAs.},
  doi       = {10.1109/IC2E.2018.00052},
  keywords  = {cloud computing;learning (artificial intelligence);neural nets;serverless platform;event based cloud applications;cloud providers;enterprise companies;machine learning;value added services;serverless computing environment;neural network models;AWS Lambda environment;MxNet deep learning framework;artificial intelligence;latency distribution;SLA;Machine learning;Cloud computing;Neural networks;Containers;Computational modeling;Delays;Training;AWS Lambda;Cloud Computing;Serverless Computing;Deep Learningm},
}

@InProceedings{10.1007/978-3-030-03673-7_4,
author="Ivanov, Vitalii
and Smolander, Kari",
editor="Kuhrmann, Marco
and Schneider, Kurt
and Pfahl, Dietmar
and Amasaki, Sousuke
and Ciolkowski, Marcus
and Hebig, Regina
and Tell, Paolo
and Kl{\"u}nder, Jil
and K{\"u}pper, Steffen",
title="Implementation of a DevOps Pipeline for Serverless Applications",
booktitle="Product-Focused Software Process Improvement",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="48--64",
abstract="Context: The term ``serverless'' defines applications that use elements of Function as a Service or Backend as a Service cloud models in their architectures. Serverless promises infrastructure and operations cost reduction, faster software development, and automatic application scalability. Although many practitioners agree that Serverless simplifies operations part of DevOps, it still requires a new approach to automation practices because of the differences in its design and development workflow. Goal: The goal of this paper is to explore how Serverless affects DevOps practices and demonstrate a DevOps pipeline implementation for a Serverless case project. Method: As the method, we use the design science research, where the resulting artefact is a release and monitoring pipeline designed and implemented according to the requirements of the case organization. Results: The result of the study is an automated DevOps pipeline with an implementation of Continuous Integration, Continuous Delivery and Monitoring practices as required by the Serverless approach of the case project. Conclusions: The outcome shows how strongly the Serverless approach affects some automation practices such as test execution, deployment and monitoring of the application. In total, 18 out of 27 implemented practices were influenced by the Serverless-specific features of the project.",
isbn="978-3-030-03673-7"
}

@InProceedings{8605773,
  author    = {D. {Jackson} and G. {Clynch}},
  title     = {An Investigation of the Impact of Language Runtime on the Performance and Cost of Serverless Functions},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {154-160},
  month     = {Dec},
  abstract  = {Serverless, otherwise known as "Function-as-a-Service" (FaaS), is a compelling evolution of cloud computing that is highly scalable and event-driven. Serverless applications are composed of multiple independent functions, each of which can be implemented in a range of programming languages. This paper seeks to understand the impact of the choice of language runtime on the performance and subsequent cost of serverless function execution. It presents the design and implementation of a new serverless performance testing framework created to analyse performance and cost metrics for both AWS Lambda and Azure Functions. For optimum performance and cost management of serverless applications, Python is the clear choice on AWS Lambda. C# .NET is the top performer and most economical option for Azure Functions. NodeJS on Azure Functions and .NET Core 2 on AWS should be avoided or at the very least, used carefully in order to avoid their potentially slow and costly start-up times.},
  doi       = {10.1109/UCC-Companion.2018.00050},
  keywords  = {cloud computing;program testing;programming languages;serverless functions;event-driven system;NodeJS;Azure Functions;AWS Lambda;serverless performance testing framework;serverless function execution;programming languages;cloud computing;compelling evolution;Function-as-a-Service;language runtime;Runtime;Measurement;Testing;Containers;Java;C# languages;Cloud computing;serverless;FaaS;Lambda;aws;azure;functions;performance;cloud},
}

@InProceedings{8551035,
  author    = {B. {Jambunathan} and K. {Yoganathan}},
  title     = {Architecture Decision on using Microservices or Serverless Functions with Containers},
  booktitle = {2018 International Conference on Current Trends towards Converging Technologies (ICCTCT)},
  year      = {2018},
  pages     = {1-7},
  month     = {March},
  abstract  = {Cloud adoption is gaining lots of momentum across the globe and enterprise are focussing not only migration on to cloud but also on developing cloud native application. There are lots of focuses on reducing and optimizing resources and hence developing application in a serverless fashion is going to be the key in the industry. Many organizations are working on application modernization and developing distributed application and hence microservice is the key focus area in converting their monolithic into microservices and use containers for easy portability across the platform and makes it more platform neutral. There are high elements of focus on whether to go for serverless or Microservice mode and should we use containers for deployment is the key debate among the people who are working in this area and are still not clear which way to go forward in the given situation. In this article we would like to explore and discuss about each technology in details and analyse the advantage and challenge of these emerging areas and suggest the ideal way to move forward in cloud.},
  doi       = {10.1109/ICCTCT.2018.8551035},
  keywords  = {cloud computing;distributed processing;platform neutral;microservice;use containers;architecture decision;serverless functions;cloud adoption;cloud native application;reducing optimizing resources;serverless fashion;application modernization;distributed application;Containers;Servers;Cloud computing;Computer architecture;FAA;Buildings;Containers;dockers;Microservices;Serverless;Functions;Lambda;Kubernetes},
}

@Article{2019arXiv190205870J,
  author        = {{Jangda}, Abhinav and {Pinckney}, Donald and {Baxter}, Samuel and {Devore-McDonald}, Breanna and {Spitzer}, Joseph and {Brun}, Yuriy and {Guha}, Arjun},
  title         = {{Formal Foundations of Serverless Computing}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1902.05870},
  month         = {Feb},
  abstract      = {A robust, large-scale web service can be difficult to engineer. When demand spikes, it must configure new machines and manage load-balancing; when demand falls, it must shut down idle machines to reduce costs; and when a machine crashes, it must quickly work around the failure without losing data. In recent years, serverless computing, a new cloud computing abstraction, has emerged to help address these challenges. In serverless computing, programmers write serverless functions, and the cloud platform transparently manages the operating system, resource allocation, load-balancing, and fault tolerance. In 2014, Amazon Web Services introduced the first serverless platform, AWS Lambda, and similar abstractions are now available on all major clouds. Unfortunately, the serverless computing abstraction exposes several low-level operational details that make it hard for programmers to write and reason about their code. This paper sheds light on this problem by presenting λλ, an operational semantics of the essence of serverless computing. Despite being a small core calculus (less than one column), λλ models all the low-level details that serverless functions can observe. To show that λλ is useful, we present three applications. First, to make it easier for programmers to reason about their code, we present a simplified semantics of serverless execution and precisely characterize when the simplified semantics and λλ coincide. Second, we augment λλ with a key-value store, which allows us to reason about stateful serverless functions. Third, since a handful of serverless platforms support serverless function composition, we show how to extend λλ with a composition language. We have implemented this composition language and show that it outperforms prior work. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190205870J},
  archiveprefix = {arXiv},
  eid           = {arXiv:1902.05870},
  eprint        = {1902.05870},
  keywords      = {Computer Science - Programming Languages},
  primaryclass  = {cs.PL},
  url           = {https://arxiv.org/pdf/1902.05870.pdf},
}

@InProceedings{10.1007/978-3-319-69035-3_51,
author="Jiang, Qingye
and Lee, Young Choon
and Zomaya, Albert Y.",
editor="Maximilien, Michael
and Vallecillo, Antonio
and Wang, Jianmin
and Oriol, Marc",
title="Serverless Execution of Scientific Workflows",
booktitle="Service-Oriented Computing",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="706--721",
abstract="In this paper, we present a serverless workflow execution system (DEWE v3{\$}{\$}^{\{}1{\}}{\$}{\$}) with Function-as-a-Service (FaaS aka serverless computing) as the target execution environment. DEWE v3 is designed to address problems of (1) execution of large-scale scientific workflows and (2) resource underutilization. At its core is our novel hybrid (FaaS and dedicated/local clusters) job dispatching approach taking into account resource consumption patterns of different phases of workflow execution. In particular, the hybrid approach deals with the maximum execution duration limit, memory limit, and storage space limit. DEWE v3 significantly reduces the efforts needed to execute large-scale scientific workflow applications on public clouds. We have evaluated DEWE v3 on both AWS Lambda and Google Cloud Functions and demonstrate that FaaS offers an ideal solution for scientific workflows with complex precedence constraints. In our large-scale evaluations, the hybrid execution model surpasses the performance of the traditional cluster execution model with significantly less execution cost.",
isbn="978-3-319-69035-3"
}

@InProceedings{John:2018:MCE:3277180.3277192,
  author    = {John, Wolfgang and Hal{\'e}n, Joacim and Cai, Xuejun and Fu, Chunyan and Holmberg, Torgny and Katardjiev, Vladimir and Mecklin, Tomas and Sedaghat, Mina and Sk\"{o}ldstr\"{o}m, Pontus and Turull, Daniel and Yadhav, Vinay and Kempf, James},
  title     = {Making Cloud Easy: Design Considerations and First Components of a Distributed Operating System for Cloud},
  booktitle = {Proceedings of the 10th USENIX Conference on Hot Topics in Cloud Computing},
  year      = {2018},
  series    = {HotCloud'18},
  pages     = {12--12},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3277192},
  location  = {Boston, MA, USA},
  numpages  = {1},
  url       = {http://dl.acm.org/citation.cfm?id=3277180.3277192},
}

@InProceedings{Jonas:2017:OCD:3127479.3128601,
  author    = {Jonas, Eric and Pu, Qifan and Venkataraman, Shivaram and Stoica, Ion and Recht, Benjamin},
  title     = {Occupy the Cloud: Distributed Computing for the 99%},
  booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
  year      = {2017},
  series    = {SoCC '17},
  pages     = {445--451},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3128601},
  doi       = {10.1145/3127479.3128601},
  isbn      = {978-1-4503-5028-0},
  keywords  = {AWS lambda, PyWren, distributed computing, serverless},
  location  = {Santa Clara, California},
  numpages  = {7},
  url       = {http://doi.acm.org/10.1145/3127479.3128601},
}

@Article{2019arXiv190203383J,
  author        = {{Jonas}, Eric and {Schleier-Smith}, Johann and {Sreekanti}, Vikram and {Tsai}, Chia-Che and {Khandelwal}, Anurag and {Pu}, Qifan and {Shankar}, Vaishaal and {Carreira}, Joao and {Krauth}, Karl and {Yadwadkar}, Neeraja and {Gonzalez}, Joseph E. and {Popa}, Raluca Ada and {Stoica}, Ion and {Patterson}, David A.},
  title         = {{Cloud Programming Simplified: A Berkeley View on Serverless Computing}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1902.03383},
  month         = {Feb},
  abstract      = {Serverless cloud computing handles virtually all the system administration operations needed to make it easier for programmers to use the cloud. It provides an interface that greatly simplifies cloud programming, and represents an evolution that parallels the transition from assembly language to high-level programming languages. This paper gives a quick history of cloud computing, including an accounting of the predictions of the 2009 Berkeley View of Cloud Computing paper, explains the motivation for serverless computing, describes applications that stretch the current limits of serverless, and then lists obstacles and research opportunities required for serverless computing to fulfill its full potential. Just as the 2009 paper identified challenges for the cloud and predicted they would be addressed and that cloud use would accelerate, we predict these issues are solvable and that serverless computing will grow to dominate the future of cloud computing. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190203383J},
  archiveprefix = {arXiv},
  eid           = {arXiv:1902.03383},
  eprint        = {1902.03383},
  keywords      = {Computer Science - Operating Systems},
  primaryclass  = {cs.OS},
  url           = {https://arxiv.org/pdf/1902.03383.pdf},
}

@Article{doi:10.1002/cpe.5189,
  author   = {Kamburugamuve, Supun and Govindarajan, Kannan and Wickramasinghe, Pulasthi and Abeykoon, Vibhatha and Fox, Geoffrey},
  title    = {Twister2: Design of a big data toolkit},
  journal  = {Concurrency and Computation: Practice and Experience},
  volume   = {0},
  number   = {0},
  pages    = {e5189},
  note     = {e5189 cpe.5189},
  abstract = {Summary Data-driven applications are essential to handle the ever-increasing volume, velocity, and veracity of data generated by sources such as the Web and Internet of Things (IoT) devices. Simultaneously, an event-driven computational paradigm is emerging as the core of modern systems designed for database queries, data analytics, and on-demand applications. Modern big data processing runtimes and asynchronous many task (AMT) systems from high performance computing (HPC) community have adopted dataflow event-driven model. The services are increasingly moving to an event-driven model in the form of Function as a Service (FaaS) to compose services. An event-driven runtime designed for data processing consists of well-understood components such as communication, scheduling, and fault tolerance. Different design choices adopted by these components determine the type of applications a system can support efficiently. We find that modern systems are limited to specific sets of applications because they have been designed with fixed choices that cannot be changed easily. In this paper, we present a loosely coupled component-based design of a big data toolkit where each component can have different implementations to support various applications. Such a polymorphic design would allow services and data analytics to be integrated seamlessly and expand from edge to cloud to HPC environments.},
  doi      = {10.1002/cpe.5189},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.5189},
  keywords = {big data, dataflow, event-driven computing, high performance computing},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5189},
}

@InProceedings{Kanso:2017:SBC:3154847.3154854,
  author    = {Kanso, Ali and Youssef, Alaa},
  title     = {Serverless: Beyond the Cloud},
  booktitle = {Proceedings of the 2Nd International Workshop on Serverless Computing},
  year      = {2017},
  series    = {WoSC '17},
  pages     = {6--10},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3154854},
  doi       = {10.1145/3154847.3154854},
  isbn      = {978-1-4503-5434-9},
  keywords  = {cloud computing, function as a service, linux containers, serverless computing},
  location  = {Las Vegas, Nevada},
  numpages  = {5},
  url       = {http://doi.acm.org/10.1145/3154847.3154854},
}

@InProceedings{Karhula:2019:CMI:3301418.3313947,
  author    = {Karhula, Pekka and Janak, Jan and Schulzrinne, Henning},
  title     = {Checkpointing and Migration of IoT Edge Functions},
  booktitle = {Proceedings of the 2Nd International Workshop on Edge Systems, Analytics and Networking},
  year      = {2019},
  series    = {EdgeSys '19},
  pages     = {60--65},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3313947},
  doi       = {10.1145/3301418.3313947},
  isbn      = {978-1-4503-6275-7},
  keywords  = {Internet of Things, checkpointing, function as a service, light-weight virtualization, serverless},
  location  = {Dresden, Germany},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3301418.3313947},
}

@InProceedings{8436772,
  author    = {M. {Keltsch} and S. {Prokesch} and O. P. {Gordo} and J. {Serrano} and T. K. {Phan} and I. {Fritzsch}},
  title     = {Remote Production and Mobile Contribution Over 5G Networks: Scenarios, Requirements and Approaches for Broadcast Quality Media Streaming},
  booktitle = {2018 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)},
  year      = {2018},
  pages     = {1-7},
  month     = {June},
  abstract  = {Media applications are amongst the most demanding services requiring high amounts of network capacity as well as extremely low latency for synchronous audio-visual streaming in production quality. Recent technological advances in the 5G domain hold the promise to unlock the potential of the media industry by offering high quality media services through dynamic efficient resource allocation. Actual implementations are now required to validate whether advanced media applications can be realised benefiting from ultra-low latency, very-high bandwidth and flexible dynamic configuration offered by these new 5G networks. A truly integrated approach is needed that focuses on the media applications not only on the management of generic network functions and the orchestration of resources at the various radio, fronthaul/backhaul, edge and core network segments. The H2020 5G PPP Phase 2 project 5G-MEDIA [1] leverages new options for more flexible, ad-hoc and cost-effective production workflows by replacing dedicated lines and hardware equipment with software functions (VNFs) facilitating (semi-) automated smart production in remote locations. Highly scalable virtualized media services deployed on or close to the edge reduce complexity for the user, ensure operational reliability and increase the Quality of Experience (QoE). Virtual compression engines have the potential to replace dedicated encoder/decoder hardware while the network optimisation (Cognitive Network Optimizer) in combination with the Quality of Service (QoS) monitoring helps to overcome the current internet best-effort principle and ensures that the required performance needs are met at all times.},
  doi       = {10.1109/BMSB.2018.8436772},
  issn      = {2155-5052},
  keywords  = {5G mobile communication;decoding;Internet;media streaming;mobile radio;quality of service;resource allocation;telecommunication traffic;virtualisation;media industry;H2020 5G PPP Phase 2 project 5G-MEDIA;resource allocation;audio-visual streaming;network capacity;broadcast Quality media streaming;5G networks;Cognitive Network Optimizer;network optimisation;Production;Streaming media;Media;5G mobile communication;Bandwidth;Cameras;TV;Remote production;smart production;mobile contribution;broadcasting;SDN;NFV;edge computing;NFVI;VNF;FaaS},
}

@Article{2019arXiv190109842K,
  author        = {{Kesidis}, George},
  title         = {{Temporal Overbooking of Lambda Functions in the Cloud}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.09842},
  month         = {Jan},
  abstract      = {We consider the problem of scheduling "serverless computing" instances such as Amazon Lambda functions. Instead of a quota per tenant/customer, we assume demand for Lambda functions is modulated by token-bucket mechanisms per tenant. Based on an upper bound on the stationary number of active "Lambda servers" considering the execution-time distribution of Lambda functions, we describe an approach that the cloud could use to overbook Lambda functions for improved utilization of IT resources. An earlier bound for a single service tier is extended to the case of multiple service tiers.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190109842K},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.09842},
  eprint        = {1901.09842},
  keywords      = {Computer Science - Performance},
  primaryclass  = {cs.PF},
  url           = {https://arxiv.org/pdf/1901.09842.pdf},
}

@InProceedings{8457833,
  author    = {J. {Kijak} and P. {Martyna} and M. {Pawlik} and B. {Balis} and M. {Malawski}},
  title     = {Challenges for Scheduling Scientific Workflows on Cloud Functions},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {460-467},
  month     = {July},
  abstract  = {Serverless computing, also known as Function-as-a-Service (FaaS) or Cloud Functions, is a new method of running distributed applications by executing functions on the infrastructure of cloud providers. Although it frees the developers from managing servers, there are still decisions to be made regarding selection of function configurations based on the desired performance and cost. The billing model of this approach considers time of execution, measured in 100ms units, as well as the size of the memory allocated per function. In this paper, we look into the problem of scheduling scientific workflows, which are applications consisting of multiple tasks connected into a dependency graph. We discuss challenges related to workflow scheduling and propose the Serverless Deadline-Budget Workflow Scheduling (SDBWS) algorithm adapted to serverless platforms. We present preliminary experiments with a small-scale Montage workflow run on the AWS Lambda infrastructure.},
  doi       = {10.1109/CLOUD.2018.00065},
  issn      = {2159-6190},
  keywords  = {cloud computing;graph theory;scheduling;scientific information systems;workflow management software;Function-as-a-Service;cloud providers;Serverless Deadline-Budget Workflow Scheduling;small-scale Montage workflow;scientific workflow scheduling;serverless computing;cloud functions;dependency graph;AWS Lambda infrastructure;Task analysis;Cloud computing;FAA;Processor scheduling;Computational modeling;Adaptation models;Engines;Faas;serverless computing;cloud functions;scientific workflow;task scheduling},
}

@InProceedings{8374513,
  author    = {J. {Kim} and T. J. {Jun} and D. {Kang} and D. {Kim} and D. {Kim}},
  title     = {GPU Enabled Serverless Computing Framework},
  booktitle = {2018 26th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)},
  year      = {2018},
  pages     = {533-540},
  month     = {March},
  abstract  = {A new form of cloud computing, serverless computing, is drawing attention as a new way to design micro-services architectures. In a serverless computing environment, services are developed as service functional units. The function development environment of all serverless computing framework at present is CPU based. In this paper, we propose a GPU-supported serverless computing framework that can deploy services faster than existing serverless computing framework using CPU. Our core approach is to integrate the open source serverless computing framework with NVIDIA-Docker and deploy services based on the GPU support container. We have developed an API that connects the open source framework to the NVIDIA-Docker and commands that enable GPU programming. In our experiments, we measured the performance of the framework in various environments. As a result, developers who want to develop services through the framework can deploy high-performance micro services and developers who want to run deep learning programs without a GPU environment can run code on remote GPUs with little performance degradation.},
  doi       = {10.1109/PDP2018.2018.00090},
  issn      = {2377-5750},
  keywords  = {application program interfaces;cloud computing;graphics processing units;learning (artificial intelligence);public domain software;service-oriented architecture;GPU enabled serverless computing framework;cloud computing;microservices architectures;serverless computing environment;service functional units;function development environment;open source serverless computing framework;deploy services;open source framework;high-performance microservices;NVIDIA-Docker;GPU support container;GPU programming;deep learning programs;performance degradation;Graphics processing units;Servers;Cloud computing;Containers;Computer architecture;Libraries;Standards;Cloud Computing;Serverless Computing;Serverless Architecture;FaaS;GPGPU},
}

@InProceedings{8567385,
  author    = {Y. {Kim} and G. {Cha}},
  title     = {Design of the Cost Effective Execution Worker Scheduling Algorithm for FaaS Platform Using Two-Step Allocation and Dynamic Scaling},
  booktitle = {2018 IEEE 8th International Symposium on Cloud and Service Computing (SC2)},
  year      = {2018},
  pages     = {131-134},
  month     = {Nov},
  abstract  = {Function as a Service(FaaS) has been widely prevalent in the cloud computing area with the evolution of the cloud computing paradigm and the growing demand for event-based computing models. We have analyzed the preparation load required for the actual execution of a function, from assignment of a function execution walker to loading a function on the FaaS platform, by testing the execution of a dummy function on a simple FaaS prototype. According to the analysis results, we found that the cost of first worker allocation requires 1,850ms even though the lightweight container is used, and then the worker re-allocation cost require 470ms at the same node. The result shows that the function service is not enough to be used as a high efficiency processing calculation platform. We propose a new worker scheduling algorithm to appropriately distribute the worker's preparation load related to execution of functions so that FaaS platform is suitable for high efficiency computing environment. Proposed algorithm is to distribute the worker 's allocation tasks in two steps before the request occurs, and predict the number of workers required to be allocated in advance. When applying the proposed worker scheduling algorithm in FaaS platform under development, we estimate that worker allocation request can be processed with an allocation cost of less than 3% compared to the FaaS prototype. Therefore, it is expected that the functional service will become a high efficiency computing platform through the significant improvement of the worker allocation cost.},
  doi       = {10.1109/SC2.2018.00027},
  keywords  = {cloud computing;resource allocation;scheduling;cost effective execution worker scheduling algorithm;FaaS platform;two-step allocation;cloud computing area;event-based computing models;worker re-allocation cost;high efficiency processing calculation platform;high efficiency computing environment;worker allocation request;high efficiency computing platform;worker allocation cost;Function as a Service;dynamic scaling;Resource management;FAA;Cloud computing;Containers;Dynamic scheduling;Scheduling algorithms;Scheduling, Dynamic scaling, Function, Function as a Service},
}

@InProceedings{8457831,
  author    = {Y. {Kim} and J. {Lin}},
  title     = {Serverless Data Analytics with Flint},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {451-455},
  month     = {July},
  abstract  = {Serverless architectures organized around loosely-coupled function invocations represent an emerging design for many applications. Recent work mostly focuses on user-facing products and event-driven processing pipelines. In this paper, we explore a completely different part of the application space and examine the feasibility of analytical processing on big data using a serverless architecture. We present Flint, a prototype Spark execution engine that takes advantage of AWS Lambda to provide a pure pay-as-you-go cost model. With Flint, a developer uses PySpark exactly as before, but without needing an actual Spark cluster. We describe the design, implementation, and performance of Flint, along with the challenges associated with serverless analytics.},
  doi       = {10.1109/CLOUD.2018.00063},
  issn      = {2159-6190},
  keywords  = {Big Data;cloud computing;data analysis;serverless data analytics;Flint;serverless architecture;loosely-coupled function invocations;analytical processing;big data;prototype Spark execution engine;pure pay-as-you-go cost model;actual Spark cluster;serverless analytics;AWS Lambda;PySpark;Sparks;Task analysis;Cloud computing;Engines;Standards;Metadata;Data analysis;serverless computing, cloud computing, data analytics, data science},
}

@InProceedings{8514884,
  author    = {Y. K. {Kim} and M. R. {HoseinyFarahabady} and Y. C. {Lee} and A. Y. {Zomaya} and R. {Jurdak}},
  title     = {Dynamic Control of CPU Usage in a Lambda Platform},
  booktitle = {2018 IEEE International Conference on Cluster Computing (CLUSTER)},
  year      = {2018},
  pages     = {234-244},
  month     = {Sep.},
  abstract  = {Lambda platform is a new concept based on an event-driven server-less computation that empowers application developers to build scalable enterprise software in a virtualized environment without provisioning or managing any physical servers (a server-less solution). In reality, however, devising an effective consolidation method to host multiple Lambda functions into a single machine is challenging. The existing simple resource allocation algorithms, such as the round-robin policy used in many commercial server-less systems, suffer from lack of responsiveness to a sudden surge in the incoming workload. This will result in an unsatisfactory performance degradation that is directly experienced by the end-user of a Lambda application. In this paper, we address the problem of CPU cap management in a Lambda platform for ensuring different QoS enforcement levels in a platform with shared resources, in case of fluctuations and sudden surges in the incoming workload requests. To this end, we present a closed-loop (feedback-based) CPU cap controller, which fulfills the QoS levels enforced by the application owners. The controller adjusts the number of working threads per QoS class and dispatches the outstanding Lambda functions along with the associated events to the most appropriate working thread. The proposed solution reduces the QoS violations by an average of 6.36 times compared to the round-robin policy. It can also maintain the end-to-end response time of applications belonging to the highest priority QoS class close to the target set-point while decreasing the overall response time by up to 52%.},
  doi       = {10.1109/CLUSTER.2018.00041},
  issn      = {2168-9253},
  keywords  = {closed loop systems;microprocessor chips;quality of service;resource allocation;software engineering;virtualisation;simple resource allocation algorithms;QoS enforcement levels;highest priority QoS class close;end-to-end response time;outstanding Lambda functions;closed-loop CPU cap controller;CPU cap management;Lambda application;round-robin policy;multiple Lambda functions;scalable enterprise software;event-driven server-less computation;Lambda platform;CPU usage;dynamic control;Quality of service;Instruction sets;Delays;Time factors;Surges;Resource management;Server-less computing, FaaS (Function as a Service), QoS-aware resource manager, Dynamic CPU usage control},
}

@InProceedings{Klimovic:2018:UES:3277355.3277431,
  author    = {Klimovic, Ana and Wang, Yawen and Kozyrakis, Christos and Stuedi, Patrick and Pfefferle, Jonas and Trivedi, Animesh},
  title     = {Understanding Ephemeral Storage for Serverless Analytics},
  booktitle = {Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference},
  year      = {2018},
  series    = {USENIX ATC '18},
  pages     = {789--794},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3277431},
  isbn      = {978-1-931971-44-7},
  location  = {Boston, MA, USA},
  numpages  = {6},
  url       = {http://dl.acm.org/citation.cfm?id=3277355.3277431},
}

@InProceedings{Klimovic:2018:PEE:3291168.3291200,
  author    = {Klimovic, Ana and Wang, Yawen and Stuedi, Patrick and Trivedi, Animesh and Pfefferle, Jonas and Kozyrakis, Christos},
  title     = {Pocket: Elastic Ephemeral Storage for Serverless Analytics},
  booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
  year      = {2018},
  series    = {OSDI'18},
  pages     = {427--444},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3291200},
  isbn      = {978-1-931971-47-8},
  location  = {Carlsbad, CA, USA},
  numpages  = {18},
  url       = {http://dl.acm.org/citation.cfm?id=3291168.3291200},
}

@InProceedings{Koller:2017:SED:3102980.3103008,
  author    = {Koller, Ricardo and Williams, Dan},
  title     = {Will Serverless End the Dominance of Linux in the Cloud?},
  booktitle = {Proceedings of the 16th Workshop on Hot Topics in Operating Systems},
  year      = {2017},
  series    = {HotOS '17},
  pages     = {169--173},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3103008},
  doi       = {10.1145/3102980.3103008},
  isbn      = {978-1-4503-5068-6},
  location  = {Whistler, BC, Canada},
  numpages  = {5},
  url       = {http://doi.acm.org/10.1145/3102980.3103008},
}

@InProceedings{8515004,
  author    = {V. {Koumaras} and A. {Foteas} and A. {Foteas} and M. {Kapari} and C. {Sakkas} and H. {Koumaras}},
  title     = {5G Performance Testing of Mobile Chatbot Applications},
  booktitle = {2018 IEEE 23rd International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD)},
  year      = {2018},
  pages     = {1-6},
  month     = {Sep.},
  abstract  = {A Chatbot is an application that is designed to provide automated contextual communication. Today most chatbots are implemented on top of or as a gateway to popular messaging services, such as Facebook Messenger, Skype and Viber. Chatbots can be classified into many categories regarding their usage, such as conversational commerce, customer support, education, marketing and others. Due to their agile deployment ability on top of virtualized and serverless environments, chatbots are expected to play a pivotal role in the forthcoming 5G networks, which support virtualization capabilities at the edge of the network, making feasible the provision of diversified chatbot services customized to each user needs and requests. However, chatbot QoS might be affected under congested network conditions or in areas with poor signal reception quality. Currently, the performance of the chatbot has not been researched, while the users are experiencing only the results of the potential QoS degradation, such as loss or re-ordering of messages. This paper provides an experimental study of the chatbot apps performance/QoS under different network and reception conditions. The experiment was conducted using the 5G mobile network emulation testbed created and provided by the EU-funded TRIANGLE project.},
  doi       = {10.1109/CAMAD.2018.8515004},
  issn      = {2378-4873},
  keywords  = {5G mobile communication;electronic messaging;mobile computing;mobile radio;quality of service;social networking (online);mobile Chatbot applications;5G mobile network emulation;congested network conditions;chatbot QoS;diversified chatbot services;automated contextual communication;Testing;Quality of service;5G mobile communication;Keyboards;Servers;Quality of experience;Mobile handsets;Chatbot;QoS;QoE;5G;Benchmarking},
}

@Inbook{Kousalya2017,
author="Kousalya, G.
and Balakrishnan, P.
and Pethuru Raj, C.",
title="Demystifying the Traits of Software-Defined Cloud Environments (SDCEs)",
bookTitle="Automated Workflow Scheduling in Self-Adaptive Clouds: Concepts, Algorithms and Methods",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="23--53",
abstract="Definitely the cloud journey is on the fast track. The cloud idea got originated and started to thrive from the days of server virtualization. Server machines are being virtualized in order to have multiple virtual machines, which are provisioned dynamically and kept in ready and steady state to deliver sufficient resources (compute, storage, and network) for optimally running any software application. That is, a physical machine can be empowered to run multiple and different applications through the aspect of virtualization. Resultantly, the utilization of expensive compute machines is steadily going up.",
isbn="978-3-319-56982-6",
doi="10.1007/978-3-319-56982-6_2",
url="https://doi.org/10.1007/978-3-319-56982-6_2"
}

@InProceedings{8605774,
  author    = {K. {Kritikos} and P. {Skrzypek}},
  title     = {A Review of Serverless Frameworks},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {161-168},
  month     = {Dec},
  abstract  = {Serverless computing is a new computing paradigm that promises to revolutionize the way applications are built and provisioned. In this computing kind, small pieces of software called functions are deployed in the cloud with zero administration and minimal costs for the software developer. Further, this computing kind has various applications in areas like image processing and scientific computing. Due to the above advantages, the current uptake of serverless computing is being addressed by traditional big cloud providers like Amazon, who offer serverless platforms for serverless application deployment and provisioning. However, as in the case of cloud computing, such providers attempt to lock-in their customers with the supply of complementary services which provide added-value support to serverless applications. To this end, to resolve this issue, serverless frameworks have been recently developed. Such frameworks either abstract away from serverless platform specificities, or they enable the production of a mini serverless platform on top of existing clouds. However, these frameworks differ in various features that do have an impact on the serverless application lifecycle. To this end, to assist the developers in selecting the most suitable framework, this paper attempts to review these frameworks according to a certain set of criteria that directly map to the application lifecycle. Further, based on the review results, some remaining challenges are supplied, which when confronted will make serverless frameworks highly usable and suitable for the handling of both serverless as well as mixed application kinds.},
  doi       = {10.1109/UCC-Companion.2018.00051},
  keywords  = {cloud computing;formal specification;serverless computing;serverless application deployment;cloud computing;serverless platform specificities;mini serverless platform;functions;big cloud providers;Cloud computing;Software;Testing;Computer languages;Runtime;Monitoring;Production;serverless, function-as-a-service, abstraction, provisioning, framework, review},
}

@InProceedings{Krol:2017:NNF:3125719.3125727,
  author    = {Kr\'{o}l, Micha\l and Psaras, Ioannis},
  title     = {NFaaS: Named Function As a Service},
  booktitle = {Proceedings of the 4th ACM Conference on Information-Centric Networking},
  year      = {2017},
  series    = {ICN '17},
  pages     = {134--144},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3125727},
  doi       = {10.1145/3125719.3125727},
  isbn      = {978-1-4503-5122-5},
  keywords  = {function migration, information centric networking, mobile edge computing, network architectures, networks},
  location  = {Berlin, Germany},
  numpages  = {11},
  url       = {http://doi.acm.org/10.1145/3125719.3125727},
}

@InProceedings{10.1007/978-3-319-69035-3_48,
author="Kuhlenkamp, J{\"o}rn
and Klems, Markus",
editor="Maximilien, Michael
and Vallecillo, Antonio
and Wang, Jianmin
and Oriol, Marc",
title="Costradamus: A Cost-Tracing System for Cloud-Based Software Services",
booktitle="Service-Oriented Computing",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="657--672",
abstract="Cloud providers offer a range of fully managed infrastructure services that enable a ``serverless'' architecture and development paradigm. Following this paradigm, software services can be built on compositions of cloud infrastructure services that offer fine-granular pay-per-use pricing models. While this development and deployment approach simplifies service development and management, it remains an open challenge to make use of fine-granular pricing models for improving cost transparency and reducing cost of service operations. As a solution, we present Costradamus, a cost-tracing system that implements a generic cost model and three different tracing approaches. With Costradamus, we can derive cost and performance information per API operation. We evaluate our approach and system in a smart grid context and discuss unexpected performance and deployment cost tradeoffs.",
isbn="978-3-319-69035-3"
}

@InProceedings{8605778,
  author    = {J. {Kuhlenkamp} and S. {Werner}},
  title     = {Benchmarking FaaS Platforms: Call for Community Participation},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {189-194},
  month     = {Dec},
  abstract  = {The number of available FaaS platforms increases with the rising popularity of a "serverless" architecture and development paradigm. As a consequence, a high demand for benchmarking FaaS platforms exists. In response to this demand, new benchmarking approaches that focus on different objectives continuously emerge. In this paper, we call for community participation to conduct a collaborative systematic literature review with the goal to establish a community-driven knowledge base.},
  doi       = {10.1109/UCC-Companion.2018.00055},
  keywords  = {cloud computing;knowledge based systems;serverless architecture;development paradigm;community participation;FaaS platforms;collaborative systematic literature review;community-driven knowledge base;FAA;Task analysis;Benchmark testing;Data mining;Search problems;Computer architecture;Bibliographies;FaaS;serverless;benchmarking;secondary study;SLR},
}

@InProceedings{Kuntsevich:2018:DAB:3284014.3284016,
  author    = {Kuntsevich, Aleksandr and Nasirifard, Pezhman and Jacobsen, Hans-Arno},
  title     = {A Distributed Analysis and Benchmarking Framework for Apache OpenWhisk Serverless Platform},
  booktitle = {Proceedings of the 19th International Middleware Conference (Posters)},
  year      = {2018},
  series    = {Middleware '18},
  pages     = {3--4},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3284016},
  doi       = {10.1145/3284014.3284016},
  isbn      = {978-1-4503-6109-5},
  keywords  = {Apache OpenWhisk, Benchmarking, Function as a service (FaaS), Serverless},
  location  = {Rennes, France},
  numpages  = {2},
  url       = {http://doi.acm.org/10.1145/3284014.3284016},
}

@InProceedings{7774691,
  author    = {E. d. {Lara} and C. S. {Gomes} and S. {Langridge} and S. H. {Mortazavi} and M. {Roodi}},
  title     = {Poster Abstract: Hierarchical Serverless Computing for the Mobile Edge},
  booktitle = {2016 IEEE/ACM Symposium on Edge Computing (SEC)},
  year      = {2016},
  pages     = {109-110},
  month     = {Oct},
  abstract  = {EdgeScale is a new platform that leverages serveless cloud computing to enable storage and processing on a hierarchy of data centers positioned over the geographic span of a network between the end-user device and the traditional wide-area cloud datacenter. EdgeScale applications are structured as lightweight stateless handlers that can be rapidely instantiated on demand. EdgeScale provides a scalable and persistent storage service that automatically migrates application state across the data center hierarchy to optimize access latency and reduce bandwidth consumption.},
  doi       = {10.1109/SEC.2016.37},
  keywords  = {cloud computing;computer centres;mobile computing;storage management;hierarchical serverless cloud computing;mobile edge;data storage;data processing;geographic span;end-user device;EdgeScale applications;lightweight stateless handlers;storage service;data center hierarchy;access latency optimization;bandwidth consumption;Cloud computing;Bandwidth;Servers;Hardware;Mobile communication;Computer architecture;serverless computing;edge computing;cloudlets},
}

@Article{2019arXiv190103984L,
  author        = {{Lavoie}, Samuel and {Garant}, Anthony and {Petrillo}, Fabio},
  title         = {{Serverless architecture efficiency: an exploratory study}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.03984},
  month         = {Jan},
  abstract      = {Cloud service provider propose services to insensitive customers to use their platform. Different services can achieve the same result at different cost. In this paper, we study the efficiency of a serverless architecture for running highly parallelizable tasks to compare theses services in order to find the most efficient in term of performance and cost. More precisely, we look at the compute time and at the cost per task for a given task. The tasks studied is the count of the occurrence of a given word in a corpus. We compare the serverless architecture to the Apache Spark map reduce technique commonly used for this type of task. Using AWS Lambda for the serverless architecture and Amazon EMR for the Apache Spark map reduce, with similar compute power, we show that the serverless technique achieve comparable performance in term of compute time and cost. We observed that the lambda function is a great approach for real time computing, while EMR is preferable for task that require long compute time. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190103984L},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.03984},
  eprint        = {1901.03984},
  keywords      = {Computer Science - Software Engineering, Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/pdf/1901.03984.pdf},
}

@Inbook{Lebanon2018,
author="Lebanon, Guy
and El-Geish, Mohamed",
title="Thoughts on System Design for Big Data",
bookTitle="Computing with Data: An Introduction to the Data Industry",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="495--541",
abstract="In the context of computing with data, what exactly is a system? Generally speaking, a system is an aggregation of computing components (and the links between them) that collectively provide a solution to a problem. System design covers choices that system designers make regarding such components: hardware (e.g., servers, networks, sensors, etc.); software (e.g., operating systems, cluster managers, applications, etc.); data (e.g., collection, retention, processing, etc.); and other components that vary based on the nature of each solution. There's no free lunch in system design and no silver bullet; instead, there are patterns that can jumpstart a solution; and for the most part, there will always be tradeoffs. Skilled system designers learn how to deal with novel problems and ambiguity; one of the skills they practice is decomposing a complex problem into more manageable subproblems that look analogous to ones that can be solved using known patterns, then connect those components together to solve the complex problem. In this chapter, we put on our designer hats and explore various aspects of system design in practice by creating a hypothetical big-data solution: a productivity bot.",
isbn="978-3-319-98149-9",
doi="10.1007/978-3-319-98149-9_14",
url="https://doi.org/10.1007/978-3-319-98149-9_14"
}

@InProceedings{8457830,
  author    = {H. {Lee} and K. {Satyam} and G. {Fox}},
  title     = {Evaluation of Production Serverless Computing Environments},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {442-450},
  month     = {July},
  abstract  = {Serverless computing provides a small runtime container to execute lines of codes without infrastructure management which is similar to Platform as a Service (PaaS) but a functional level. Amazon started the event-driven compute named Lambda functions in 2014 with a 25 concurrent limitation, but it now supports at least a thousand of concurrent invocation to process event messages generated by resources like databases, storage and system logs. Other providers, i.e., Google, Microsoft, and IBM offer a dynamic scaling manager to handle parallel requests of stateless functions in which additional containers are provisioning on new compute nodes for distribution. However, while functions are often developed for microservices and lightweight workload, they are associated with distributed data processing using the concurrent invocations. We claim that the current serverless computing environments can support dynamic applications in parallel when a partitioned task is executable on a small function instance. We present results of throughput, network bandwidth, a file I/O and compute performance regarding the concurrent invocations. We deployed a series of functions for distributed data processing to address the elasticity and then demonstrated the differences between serverless computing and virtual machines for cost efficiency and resource utilization.},
  doi       = {10.1109/CLOUD.2018.00062},
  issn      = {2159-6190},
  keywords  = {cloud computing;virtual machines;distributed data processing;stateless functions;parallel requests;dynamic scaling manager;concurrent invocation;Lambda functions;event-driven compute;infrastructure management;production serverless computing environments;Google;Throughput;Cloud computing;Databases;Containers;Runtime;Data processing;FaaS, Serverless, Event-driven Computing, Amazon Lambda, Google Functions, Microsoft Azure Functions, IBM OpenWhisk},
}

@InProceedings{10.1007/978-3-319-74433-9_6,
author="Lehv{\"a}, Jyri
and M{\"a}kitalo, Niko
and Mikkonen, Tommi",
editor="Garrig{\'o}s, Irene
and Wimmer, Manuel",
title="Case Study: Building a Serverless Messenger Chatbot",
booktitle="Current Trends in Web Engineering",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="75--86",
abstract="Major chat platforms, such as Facebook Messenger, have recently added support for chatbots, thus making chatbots more accessible for the end users. This paper presents a case study on building and designing a Messenger chatbot for a media company. The chatbot uses a Serverless Microservice architecture which was implemented using Amazon Web Services (AWS) including API Gateway, Lambda, DynamoDB, SNS and CloudWatch. The paper presents the architecture and reports the findings regarding the design and the final implementation. These findings are also compared to other recent studies around the same emerging topic.",
isbn="978-3-319-74433-9"
}

@Article{LEITNER2019340,
  author   = {Philipp Leitner and Erik Wittern and Josef Spillner and Waldemar Hummer},
  title    = {A mixed-method empirical study of Function-as-a-Service software development in industrial practice},
  journal  = {Journal of Systems and Software},
  year     = {2019},
  volume   = {149},
  pages    = {340 - 359},
  issn     = {0164-1212},
  abstract = {Function-as-a-Service (FaaS) describes cloud computing services that make infrastructure components transparent to application developers, thus falling in the larger group of “serverless” computing models. When using FaaS offerings, such as AWS Lambda, developers provide atomic and short-running code for their functions, and FaaS providers execute and horizontally scale them on-demand. Currently, there is no systematic research on how developers use serverless, what types of applications lend themselves to this model, or what architectural styles and practices FaaS-based applications are based on. We present results from a mixed-method study, combining interviews with practitioners who develop applications and systems that use FaaS, a systematic analysis of grey literature, and a Web-based survey. We find that successfully adopting FaaS requires a different mental model, where systems are primarily constructed by composing pre-existing services, with FaaS often acting as the “glue” that brings these services together. Tooling availability and maturity, especially related to testing and deployment, remains a major difficulty. Further, we find that current FaaS systems lack systematic support for function reuse, and abstractions and programming models for building non-trivial FaaS applications are limited. We conclude with a discussion of implications for FaaS providers, software developers, and researchers.},
  doi      = {https://doi.org/10.1016/j.jss.2018.12.013},
  keywords = {Cloud computing, Serverless, Function-as-a-Service, Empirical research},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121218302735},
}

@Article{2019arXiv190312221L,
  author        = {{Lin}, Ping-Min and {Glikson}, Alex},
  title         = {{Mitigating Cold Starts in Serverless Platforms: A Pool-Based Approach}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1903.12221},
  month         = {Mar},
  abstract      = {Rapid adoption of the serverless (or Function-as-a-Service, FaaS) paradigm, pioneered by Amazon with AWS Lambda and followed by numerous commercial offerings and open source projects, introduces new challenges in designing the cloud infrastructure, balancing between performance and cost. While instant per-request elasticity that FaaS platforms typically offer application developers makes it possible to achieve high performance of bursty workloads without over-provisioning, such elasticity often involves extra latency associated with on-demand provisioning of individual runtime containers that serve the functions. This phenomenon is often called cold starts, as opposed to the situation when a function is served by a pre-provisioned "warm" container, ready to serve requests with close to zero overhead. Providers are constantly working on techniques aimed at reducing cold starts. A common approach to reduce cold starts is to maintain a pool of warm containers, in anticipation of future requests. In this report, we address the cold start problem in serverless architectures, specifically under the Knative Serving FaaS platform. We describe our implementation leveraging a pool of function instances, and evaluate the latency compared to the original implementation, resulting in a 85% reduction of P99 response time for a single instance pool. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190312221L},
  archiveprefix = {arXiv},
  eid           = {arXiv:1903.12221},
  eprint        = {1903.12221},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://ui.adsabs.harvard.edu/link_gateway/2019arXiv190312221L/EPRINT_PDF},
}

@InProceedings{8457807,
  author    = {W. {Lin} and C. {Krintz} and R. {Wolski}},
  title     = {Tracing Function Dependencies across Clouds},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {253-260},
  month     = {July},
  abstract  = {In this paper, we present Lowgo, a crosscloud tracing tool for capturing causal relationships in serverless applications. To do so, Lowgo records dependencies between functions, through cloud services, and across regions to facilitate debugging and reasoning about highly concurrent, multi-cloud applications. We empirically evaluate Lowgo using microbenchmarks and multi-function and multi-cloud applications. We find that Lowgo is able to capture causal dependencies with overhead that ranges from 2-12%, which is less than half that of the best-performing, cloud-specific approach.},
  doi       = {10.1109/CLOUD.2018.00039},
  issn      = {2159-6190},
  keywords  = {causality;cloud computing;program debugging;function dependencies;clouds;crosscloud tracing tool;causal relationships;serverless applications;Lowgo records dependencies;cloud services;multicloud applications;microbenchmarks;causal dependencies;cloud-specific approach;Cloud computing;Tools;Pipelines;Servers;Google;Debugging;Computational modeling;cloud computing;serverless computing;function as a service;faas;AWS Lambda;Azure Functions},
}

@InProceedings{8360312,
  author    = {W. {Lin} and C. {Krintz} and R. {Wolski} and M. {Zhang} and X. {Cai} and T. {Li} and W. {Xu}},
  title     = {Tracking Causal Order in AWS Lambda Applications},
  booktitle = {2018 IEEE International Conference on Cloud Engineering (IC2E)},
  year      = {2018},
  pages     = {50-60},
  month     = {April},
  abstract  = {Serverless computing is a new cloud programming and deployment paradigm that is receiving wide-spread uptake. Serverless offerings such as Amazon Web Services (AWS) Lambda, Google Functions, and Azure Functions automatically execute simple functions uploaded by developers, in response to cloud-based event triggers. The serverless abstraction greatly simplifies integration of concurrency and parallelism into cloud applications, and enables deployment of scalable distributed systems and services at very low cost. Although a significant first step, the serverless abstraction requires tools that software engineers can use to reason about, debug, and optimize their increasingly complex, asynchronous applications. Toward this end, we investigate the design and implementation of GammaRay, a cloud service that extracts causal dependencies across functions and through cloud services, without programmer intervention. We implement GammaRay for AWS Lambda and evaluate the overheads that it introduces for serverless micro-benchmarks and applications written in Python.},
  doi       = {10.1109/IC2E.2018.00027},
  keywords  = {cloud computing;Web services;cloud applications;scalable distributed systems;serverless abstraction;increasingly complex applications;asynchronous applications;cloud service;AWS Lambda applications;serverless computing;cloud programming;wide-spread uptake;serverless offerings;Amazon Web Services Lambda;Google Functions;Azure Functions;simple functions;cloud-based event triggers;concurrency;parallelism;causal dependencies extraction;causal order tracking;GammaRay;Cloud computing;Containers;X-ray imaging;Tools;Monitoring;Concurrent computing;serverless;faas;causal dependencies;AWS Lambda;profiling},
}

@InProceedings{8622861,
  author    = {W. {Ling} and C. {Tian} and L. {Ma} and Z. {Hu}},
  title     = {Lite-Service: A Framework to Build and Schedule Telecom Applications in Device, Edge and Cloud},
  booktitle = {2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
  year      = {2018},
  pages     = {708-717},
  month     = {June},
  abstract  = {In cloud, many different service frameworks with new concepts such as Function-as-a-Service (FaaS), Serverless, and Stateless emerge recently. They are essentially certain instantiations of Micro-Service architecture, in which, a software application is implemented as a set of "loosely-coupled services" [32]. From design perspective, these services are considered as the most fine-grained units implementing business logics. Existing frameworks mostly focus on deployment and management of Micro-Services in cloud. However, the internal management within Micro-Service at thread level and function level does not attract enough attention. These missing pieces are critical for telecom applications, in term of programming model, scalability and performance. In this paper, we present Lite-Service, a new framework addressing device, edge and cloud operation environments in telecom industry. It adopts FaaS, Serverless and Stateless programming models to separate telecom logics from runtimes in order to improve development efficiency and reduce operation cost. The Lite-Service framework features 2 additional runtimes: function-level runtime for complex telecom function scheduling; thread-level intra-service runtime for fast response to load change. An autonomous self-adaptive scheduler and decentralized service management schemes are proposed to enhance performance and service adaptability in different telecom deployment scenarios. Empirical results show that the Lite-Service framework is effective for building and scheduling telecom applications in device, edge and cloud.},
  doi       = {10.1109/HPCC/SmartCity/DSS.2018.00123},
  keywords  = {cloud computing;service-oriented architecture;telecommunication computing;software application;fine-grained units;internal management;telecom industry;telecom logics;function-level runtime;complex telecom function scheduling;thread-level intra-service runtime;service management schemes;Function-as-a-Service;lite-service framework;telecom applications;microservice architecture;self-adaptive scheduler;business logics;loosely-coupled services;FaaS;Telecommunications;Cloud computing;FAA;Runtime;Software;Programming;Computational modeling;Serverless, FaaS, Stateless, Autonomous},
}

@InProceedings{8360324,
  author    = {W. {Lloyd} and S. {Ramesh} and S. {Chinthalapati} and L. {Ly} and S. {Pallickara}},
  title     = {Serverless Computing: An Investigation of Factors Influencing Microservice Performance},
  booktitle = {2018 IEEE International Conference on Cloud Engineering (IC2E)},
  year      = {2018},
  pages     = {159-169},
  month     = {April},
  abstract  = {Serverless computing platforms provide function(s)-as-a-Service (FaaS) to end users while promising reduced hosting costs, high availability, fault tolerance, and dynamic elasticity for hosting individual functions known as microservices. Serverless Computing environments, unlike Infrastructure-as-a-Service (IaaS) cloud platforms, abstract infrastructure management including creation of virtual machines (VMs), operating system containers, and request load balancing from users. To conserve cloud server capacity and energy, cloud providers allow hosting infrastructure to go COLD, deprovisioning containers when service demand is low freeing infrastructure to be harnessed by others. In this paper, we present results from our comprehensive investigation into the factors which influence microservice performance afforded by serverless computing. We examine hosting implications related to infrastructure elasticity, load balancing, provisioning variation, infrastructure retention, and memory reservation size. We identify four states of serverless infrastructure including: provider cold, VM cold, container cold, and warm and demonstrate how microservice performance varies up to 15x based on these states.},
  doi       = {10.1109/IC2E.2018.00039},
  keywords  = {cloud computing;resource allocation;virtual machines;serverless computing platforms;hosting costs;fault tolerance;dynamic elasticity;individual functions;microservices;virtual machines;operating system containers;cloud server capacity;cloud providers;service demand;influence microservice performance;infrastructure elasticity;load balancing;infrastructure retention;serverless infrastructure including;infrastructure-as-a-service;infrastructure management;serverless computing environments;FaaS;IaaS;VM;Containers;Cloud computing;Load management;Elasticity;Servers;Operating systems;Fault tolerance;Resource Management and Performance;Serverless Computing;Function-as-a-Service;Provisioning Variation},
}

@InProceedings{8605779,
  author    = {W. {Lloyd} and M. {Vu} and B. {Zhang} and O. {David} and G. {Leavesley}},
  title     = {Improving Application Migration to Serverless Computing Platforms: Latency Mitigation with Keep-Alive Workloads},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {195-200},
  month     = {Dec},
  abstract  = {Serverless computing platforms provide Function(s)-as-a-Service (FaaS) to end users while promising reduced hosting costs, high availability, fault tolerance, and dynamic elasticity for hosting individual functions known as microservices. Serverless Computing environments abstract infrastructure management including creation of virtual machines (VMs), containers, and load balancing from users. To conserve cloud server capacity and energy, cloud providers allow serverless computing infrastructure to go COLD, deprovisioning hosting infrastructure when demand falls, freeing capacity to be harnessed by others. In this paper, we present on a case study migration of the Precipitation Runoff Modeling System (PRMS), a Java-based environmental modeling application to the AWS Lambda serverless platform. We investigate performance and cost implications of memory reservation size, and evaluate scaling performance for increasing concurrent workloads. We then investigate the use of Keep-Alive workloads to preserve serverless infrastructure to minimize cold starts and ensure fast performance after idle periods for up to 100 concurrent client requests. We show how Keep-Alive workloads can be generated using cloud-based scheduled event triggers, enabling minimization of costs, to provide VM-like performance for applications hosted on serverless platforms for a fraction of the cost.},
  doi       = {10.1109/UCC-Companion.2018.00056},
  keywords  = {cloud computing;Java;resource allocation;scheduling;serverless computing infrastructure;Java-based environmental modeling application;concurrent workloads;serverless infrastructure;serverless platforms;application migration;serverless computing platforms;load balancing;cloud server capacity;cloud providers;infrastructure management;function-as-a-service;serverless computing environments;keep-alive workloads;infrastructure management;precipitation runoff modeling system;AWS lambda serverless platform;cloud-based scheduled event triggers;Cloud computing;FAA;Containers;Computational modeling;Java;Load management;Load modeling;Resource Management and Performance;Serverless Computing;Function-as-a-Service;Application Migration},
}

@InProceedings{8605772,
  author    = {P. {García López} and M. {Sánchez-Artigas} and G. {París} and D. {Barcelona Pons} and Á. {Ruiz Ollobarren} and D. {Arroyo Pinto}},
  title     = {Comparison of FaaS Orchestration Systems},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {148-153},
  month     = {Dec},
  abstract  = {Since the appearance of Amazon Lambda in 2014, all major cloud providers have embraced the "Function as a Service" (FaaS) model, because of its enormous potential for a wide variety of applications. As expected (and also desired), the competition is fierce in the serverless world, and includes aspects such as the run-time support for the orchestration of serverless functions. In this regard, the three major production services are currently Amazon Step Functions (December 2016), Azure Durable Functions (June 2017), and IBM Composer (October 2017), still young and experimental projects with a long way ahead. In this article, we will compare and analyze these three serverless orchestration systems under a common evaluation framework. We will study their architectures, programming and billing models, and their effective support for parallel execution, among others. Through a series of experiments, we will also evaluate the run-time overhead of the different infrastructures for different types of workflows.},
  doi       = {10.1109/UCC-Companion.2018.00049},
  keywords  = {cloud computing;IBM Composer;serverless orchestration systems;billing models;FaaS orchestration systems;Amazon Lambda;cloud providers;Function as a Service model;serverless functions;Amazon Step Functions;Azure Durable Functions;Programming;Software;Packaging;Computer architecture;FAA;Measurement;DSL;Cloud computing;Serverless;Function Composition;Orchestration;Amazon Step Functions;Azure Durable Functions;IBM Composer},
}

@InProceedings{8241104,
  author    = {T. {Lynn} and P. {Rosati} and A. {Lejeune} and V. {Emeakaroha}},
  title     = {A Preliminary Review of Enterprise Serverless Cloud Computing (Function-as-a-Service) Platforms},
  booktitle = {2017 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)},
  year      = {2017},
  pages     = {162-169},
  month     = {Dec},
  abstract  = {In line with cloud computing emergence as the dominant enterprise computing paradigm, our conceptualization of the cloud computing reference architecture and service construction has also evolved. For example, to address the need for cost reduction and rapid provisioning, virtualization has moved beyond hardware to containers. More recently, serverless computing or Function-as-a-Service has been presented as a means to introduce further cost-efficiencies, reduce configuration and management overheads, and rapidly increase an application's ability to speed up, scale up and scale down in the cloud. The potential of this new computation model is reflected in the introduction of serverless computing platforms by the main hyperscale cloud service providers. This paper provides an overview and multi-level feature analysis of seven enterprise serverless computing platforms. It reviews extant research on these platforms and identifies the emergence of AWS Lambda as a de facto base platform for research on enterprise serverless cloud computing. The paper concludes with a summary of avenues for further research.},
  doi       = {10.1109/CloudCom.2017.15},
  issn      = {2330-2186},
  keywords  = {business data processing;cloud computing;service-oriented architecture;virtualisation;Function-as-a-Service;dominant enterprise computing paradigm;computation model;main hyperscale cloud service providers;enterprise serverless cloud computing platforms;cost-efficiencies;configuration overheads;management overheads;multilevel feature analysis;AWS Lambda;Cloud computing;Google;Servers;Real-time systems;Virtualization;Computational modeling;Serverless Computing;Function-as-a-Service;FAAS;AWS Lambda;Google Cloud Functions;Azure Functions;IBM OpenWhisk;Iron.io;Auth0 Webtask;Gestal Laser},
}

@InProceedings{Maas:2017:RRR:3102980.3103003,
  author    = {Maas, Martin and Asanovi\'{c}, Krste and Kubiatowicz, John},
  title     = {Return of the Runtimes: Rethinking the Language Runtime System for the Cloud 3.0 Era},
  booktitle = {Proceedings of the 16th Workshop on Hot Topics in Operating Systems},
  year      = {2017},
  series    = {HotOS '17},
  pages     = {138--143},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3103003},
  doi       = {10.1145/3102980.3103003},
  isbn      = {978-1-4503-5068-6},
  keywords  = {Cloud 3.0, Data Centers, FPGAs, Garbage Collection, JIT Compilation, Managed Language Runtime Systems, Platform-as-a-Service, Resource Disaggregation, Serverless Computing},
  location  = {Whistler, BC, Canada},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3102980.3103003},
}

@InProceedings{10.1007/978-3-319-75178-8_34,
author="Malawski, Maciej
and Figiela, Kamil
and Gajek, Adam
and Zima, Adam",
editor="Heras, Dora B.
and Boug{\'e}, Luc
and Mencagli, Gabriele
and Jeannot, Emmanuel
and Sakellariou, Rizos
and Badia, Rosa M.
and Barbosa, Jorge G.
and Ricci, Laura
and Scott, Stephen L.
and Lankes, Stefan
and Weidendorfer, Josef",
title="Benchmarking Heterogeneous Cloud Functions",
booktitle="Euro-Par 2017: Parallel Processing Workshops",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="415--426",
abstract="Cloud Functions, often called Function-as-a-Service (FaaS), pioneered by AWS Lambda, are an increasingly popular method of running distributed applications. As in other cloud offerings, cloud functions are heterogeneous, due to different underlying hardware, runtime systems, as well as resource management and billing models. In this paper, we focus on performance evaluation of cloud functions, taking into account heterogeneity aspects. We developed a cloud function benchmarking framework, consisting of one suite based on Serverless Framework, and one based on HyperFlow. We deployed the CPU-intensive benchmarks: Mersenne Twister and Linpack, and evaluated all the major cloud function providers: AWS Lambda, Azure Functions, Google Cloud Functions and IBM OpenWhisk. We make our results available online and continuously updated. We report on the initial results of the performance evaluation and we discuss the discovered insights on the resource allocation policies.",
isbn="978-3-319-75178-8"
}

@Article{MALAWSKI2017,
  author   = {Maciej Malawski and Adam Gajek and Adam Zima and Bartosz Balis and Kamil Figiela},
  title    = {Serverless execution of scientific workflows: Experiments with HyperFlow, AWS Lambda and Google Cloud Functions},
  journal  = {Future Generation Computer Systems},
  year     = {2017},
  issn     = {0167-739X},
  abstract = {Scientific workflows consisting of a high number of interdependent tasks represent an important class of complex scientific applications. Recently, a new type of serverless infrastructures has emerged, represented by such services as Google Cloud Functions and AWS Lambda, also referred to as the Function-as-a-Service model. In this paper we take a look at such serverless infrastructures, which are designed mainly for processing background tasks of Web and Internet of Things applications, or event-driven stream processing. We evaluate their applicability to more compute- and data-intensive scientific workflows and discuss possible ways to repurpose serverless architectures for execution of scientific workflows. We have developed prototype workflow executor functions using AWS Lambda and Google Cloud Functions, coupled with the HyperFlow workflow engine. These functions can run workflow tasks in AWS and Google infrastructures, and feature such capabilities as data staging to/from S3 or Google Cloud Storage and execution of custom application binaries. We have successfully deployed and executed the Montage astronomy workflow, often used as a benchmark, and we report on initial results of its performance evaluation. Our findings indicate that the simple mode of operation makes this approach easy to use, although there are costs involved in preparing portable application binaries for execution in a remote environment. While our solution is an early prototype, we find the presented approach highly promising. We also discuss possible future steps related to execution of scientific workflows in serverless infrastructures. Finally, we perform a cost analysis and discuss implications with regard to resource management for scientific applications in general.},
  doi      = {https://doi.org/10.1016/j.future.2017.10.029},
  keywords = {Scientific workflows, Cloud functions, Serverless architectures, FaaS},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X1730047X},
}

@InProceedings{8605777,
  author    = {J. {Manner} and M. {Endreß} and T. {Heckel} and G. {Wirtz}},
  title     = {Cold Start Influencing Factors in Function as a Service},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {181-188},
  month     = {Dec},
  abstract  = {Function as a Service (FaaS) is a young and rapidly evolving cloud paradigm. Due to its hardware abstraction, inherent virtualization problems come into play and need an assessment from the FaaS point of view. Especially avoidance of idling and scaling on demand cause a lot of container starts and as a consequence a lot of cold starts for FaaS users. The aim of this paper is to address the cold start problem in a benchmark and investigate influential factors on the duration of the perceived cold start. We conducted a benchmark on AWS Lambda and Microsoft Azure Functions with 49500 cloud function executions. Formulated as hypotheses, the influence of the chosen programming language, platform, memory size for the cloud function, and size of the deployed artifact are the dimensions of our benchmark. Cold starts on the platform as well as the cold starts for users were measured and compared to each other. Our results show that there is an enormous difference for the overhead the user perceives compared to the billed duration. In our benchmark, the average cold start overheads on the user's side ranged from 300ms to 24s for the chosen configurations.},
  doi       = {10.1109/UCC-Companion.2018.00054},
  keywords  = {cloud computing;virtualisation;container starts;cold start problem;Microsoft Azure Functions;virtualization problems;cloud function executions;Function as a Service;AWS Lambda;FAA;Containers;Benchmark testing;Cloud computing;Java;Pipelines;Serverless Computing, Function as a Service, FaaS, Cloud Functions, Cold Start, Benchmarking},
}

@Article{Manner2019,
author="Manner, Johannes
and Kolb, Stefan
and Wirtz, Guido",
title="Troubleshooting Serverless functions: a combined monitoring and debugging approach",
journal="SICS Software-Intensive Cyber-Physical Systems",
year="2019",
month="Feb",
day="06",
abstract="Today, Serverless computing gathers pace and attention in the cloud computing area. The abstraction of operational tasks combined with the auto-scaling property are convincing reasons to adapt this new cloud paradigm. Building applications in a Serverless style via cloud functions is challenging due to the fine-grained architecture and the tighter coupling to back end services. Increased complexity, loss of control over software layers and the large number of participating functions and back end services complicate the task of finding the cause of a faulty execution. A tedious but widespread strategy is the manual analysis of log data. In this paper, we present a semi-automated troubleshooting process to improve fault detection and resolution for Serverless functions. Log data is the vehicle to enable a posteriori analysis. The process steps of our concept enhance the log quality, detect failed executions automatically, and generate test skeletons based on the information provided in the log data. Ultimately, this leads to an increased test coverage, a better regression testing and more robust functions. Developers can trigger this process asynchronously and work with their accustomed tools. We also present a prototype SeMoDe to validate our approach for Serverless functions implemented in Java and deployed to AWS Lambda.",
issn="2524-8529",
doi="10.1007/s00450-019-00398-6",
url="https://doi.org/10.1007/s00450-019-00398-6"
}

@InProceedings{7979855,
  author    = {G. {McGrath} and P. R. {Brenner}},
  title     = {Serverless Computing: Design, Implementation, and Performance},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems Workshops (ICDCSW)},
  year      = {2017},
  pages     = {405-410},
  month     = {June},
  abstract  = {We present the design of a novel performance-oriented serverless computing platform implemented in. NET, deployed in Microsoft Azure, and utilizing Windows containers as function execution environments. Implementation challenges such as function scaling and container discovery, lifecycle, and reuse are discussed in detail. We propose metrics to evaluate the execution performance of serverless platforms and conduct tests on our prototype as well as AWS Lambda, Azure Functions, Google Cloud Functions, and IBM's deployment of Apache OpenWhisk. Our measurements show the prototype achieving greater throughput than other platforms at most concurrency levels, and we examine the scaling and instance expiration trends in the implementations. Additionally, we discuss the gaps and limitations in our current design, propose possible solutions, and highlight future research.},
  doi       = {10.1109/ICDCSW.2017.36},
  issn      = {2332-5666},
  keywords  = {cloud computing;concurrency (computers);Microsoft Windows (operating systems);performance evaluation;performance-oriented serverless computing;.NET;Microsoft Azure;Windows containers;function execution environments;execution performance evaluation;AWS Lambda;Azure Functions;Google Cloud Functions;IBM's deployment;Apache OpenWhisk;concurrency levels;Containers;Web services;Metadata;Resource management;Prototypes;Runtime;Google;serverless computing;serverless performance;FaaS;Function-as-a-Service;AWS Lambda;Azure Functions;Google Cloud Functions;Apache OpenWhisk;IBM OpenWhisk},
}

@InProceedings{7820297,
  author    = {G. {McGrath} and J. {Short} and S. {Ennis} and B. {Judson} and P. {Brenner}},
  title     = {Cloud Event Programming Paradigms: Applications and Analysis},
  booktitle = {2016 IEEE 9th International Conference on Cloud Computing (CLOUD)},
  year      = {2016},
  pages     = {400-406},
  month     = {June},
  abstract  = {Rapid expansion in cloud event technologies such as Amazon Web Service's Lambda, IBM Bluemix's OpenWhisk, Google Cloud Platform's Cloud Functions, and Microsoft Azure's Functions motivates study of software development in these services and their potential as a disruptive force in commercial cloud technologies. In addition to discussing the current state of cloud event services, this paper presents two real world applications utilizing these platforms: Lambdefy, a library designed to make traditional web application run effectively in AWS Lambda, and a performant media management service designed by Trek10, capable of resizing thousands of images per second. Furthermore, we discuss how cloud event technologies enable and/or limit these applications, motivate new software design paradigms in a cloud event environment, and highlight compelling use case scenarios and barriers to entry for cloud event services. AWS cloud technologies are exclusively used due to their maturity and the recent release of the other platforms, while Node.js and the Serverless Framework are utilized for deployment and application development.},
  doi       = {10.1109/CLOUD.2016.0060},
  issn      = {2159-6190},
  keywords  = {cloud computing;Web services;cloud event programming paradigm;Lambda Amazon Web service;OpenWhisk IBM Bluemix;Cloud Functions Google cloud platform;Microsoft Azure Functions;software development;cloud event services;Lambdefy platform;Web application;AWS Lambda;performant media management service;Trek10;AWS cloud technologies;Node.js;serverless framework;Cloud computing;Logic gates;Containers;Google;Hardware;Programming;cloud events;AWS;Lambda;Serverless;micro-services;containers;Google Cloud Platform;Cloud Functions;Microsoft Azure;IBM Bluemix;OpenWhisk},
}

@InProceedings{7980047,
  author    = {A. {Mehta} and R. {Baddour} and F. {Svensson} and H. {Gustafsson} and E. {Elmroth}},
  title     = {Calvin Constrained — A Framework for IoT Applications in Heterogeneous Environments},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)},
  year      = {2017},
  pages     = {1063-1073},
  month     = {June},
  abstract  = {Calvin is an IoT framework for application development, deployment and execution in heterogeneous environments, that includes clouds, edge resources, and embedded or constrained resources. Inside Calvin, all the distributed resources are viewed as one environment by the application. The framework provides multi-tenancy and simplifies development of IoT applications, which are represented using a dataflow of application components (named actors) and their communication. The idea behind Calvin poses similarity with the serverless architecture and can be seen as Actor as a Service instead of Function as a Service. This makes Calvin very powerful as it does not only scale actors quickly but also provides an easy actor migration capability. In this work, we propose Calvin Constrained, an extension to the Calvin framework to cover resource-constrained devices. Due to limited memory and processing power of embedded devices, the constrained side of the framework can only support a limited subset of the Calvin features. The current implementation of Calvin Constrained supports actors implemented in C as well as Python, where the support for Python actors is enabled by using MicroPython as a statically allocated library, by this we enable the automatic management of state variables and enhance code re-usability. As would be expected, Python-coded actors demand more resources over C-coded ones. We show that the extra resources needed are manageable on current off-the-shelve micro-controller-equipped devices when using the Calvin framework.},
  doi       = {10.1109/ICDCS.2017.181},
  issn      = {1063-6927},
  keywords  = {cloud computing;Internet of Things;IoT applications;heterogeneous environments;application development;distributed resources;application components;actor as a service;Calvin constrained;embedded devices;resource-constrained devices;MicroPython;statically allocated library;automatic management;state variables;code reusability enhancement;Python-coded actors;off-the-shelve microcontroller-equipped devices;Runtime;Temperature measurement;Frequency measurement;Sensors;Cloud computing;Actuators;Ports (Computers);IoT;Distributed Cloud;Serverless Architecture;Dataflow Application Development Model},
}

@InProceedings{MeiBner:2018:REP:3210284.3210285,
  author    = {Mei\ssner, Dominik and Erb, Benjamin and Kargl, Frank and Tichy, Matthias},
  title     = {Retro-{\$\Lambda\$}: An Event-sourced Platform for Serverless Applications with Retroactive Computing Support},
  booktitle = {Proceedings of the 12th ACM International Conference on Distributed and Event-based Systems},
  year      = {2018},
  series    = {DEBS '18},
  pages     = {76--87},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3210285},
  doi       = {10.1145/3210284.3210285},
  isbn      = {978-1-4503-5782-1},
  keywords  = {event processing, event sourcing, event-driven architecture, retroaction, retroactive computing, serverless computing},
  location  = {Hamilton, New Zealand},
  numpages  = {12},
  url       = {http://doi.acm.org/10.1145/3210284.3210285},
}

@InProceedings{10.1007/978-3-319-99253-2_32,
author="Merelo Guerv{\'o}s, Juan J.
and Garc{\'i}a-Valdez, J. Mario",
editor="Auger, Anne
and Fonseca, Carlos M.
and Louren{\c{c}}o, Nuno
and Machado, Penousal
and Paquete, Lu{\'i}s
and Whitley, Darrell",
title="Introducing an Event-Based Architecture for Concurrent and Distributed Evolutionary Algorithms",
booktitle="Parallel Problem Solving from Nature -- PPSN XV",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="399--410",
abstract="Cloud-native applications add a layer of abstraction to the underlying distributed computing system, defining a high-level, self-scaling and self-managed architecture of different microservices linked by a messaging bus. Creating new algorithms that tap these architectural patterns and at the same time employ distributed resources efficiently is a challenge we will be taking up in this paper. We introduce KafkEO, a cloud-native evolutionary algorithms framework that is prepared to work with different implementations of evolutionary algorithms and other population-based metaheuristics by using micro-populations and stateless services as the main building blocks; KafkEO is an attempt to map the traditional evolutionary algorithm to this new cloud-native format. As far as we know, this is the first architecture of this kind that has been published and tested, and is free software and vendor-independent, based on OpenWhisk and Kafka. This paper presents a proof of concept, examines its cost, and tests the impact on the algorithm of the design around cloud-native and asynchronous system by comparing it on the well known BBOB benchmarks with other pool-based architectures, with which it has a remarkable functional resemblance. KafkEO results are quite competitive with similar architectures.",
isbn="978-3-319-99253-2"
}

@InProceedings{Merelo:2018:MEA:3205651.3208317,
  author    = {Merelo, Juan J. and Garc\'{\i}a-Valdez, Jos{\'e}-Mario},
  title     = {Mapping Evolutionary Algorithms to a Reactive, Stateless Architecture: Using a Modern Concurrent Language},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  year      = {2018},
  series    = {GECCO '18},
  pages     = {1870--1877},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3208317},
  doi       = {10.1145/3205651.3208317},
  isbn      = {978-1-4503-5764-7},
  keywords  = {algorithm implementation, distributed computing, event-based systems, functions as a service, heterogeneous distributed systems, kappa architecture, microservices, performance evaluation, pool-based systems, serverless computing, stateless algorithms},
  location  = {Kyoto, Japan},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/3205651.3208317},
}

@InProceedings{8605775,
  author    = {P. {Moczurad} and M. {Malawski}},
  title     = {Visual-Textual Framework for Serverless Computation: A Luna Language Approach},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {169-174},
  month     = {Dec},
  abstract  = {As serverless technologies are emerging as a breakthrough in the cloud computing industry, the lack of proper tooling is becoming apparent. The model of computation that the serverless is imposing is as flexible as it is hard to manage and grasp. We present a novel approach towards serverless computing that tightly integrates it with the visual-textual, functional programming language: Luna. This way we are hoping to achieve the clarity and cognitive ease of visual solutions while retaining the flexibility and expressive power of textual programming languages. We created a proof of concept of the Luna Serverless Framework in which we extend the Luna standard library and we leverage the language features to create an intuitive API for serverless function calls using AWS Lambda and to call external functions implemented in JavaScript.},
  doi       = {10.1109/UCC-Companion.2018.00052},
  keywords  = {application program interfaces;cloud computing;data visualisation;functional programming;Java;visual programming;Luna language approach;serverless technologies;cloud computing industry;serverless computing;visual-textual programming language;functional programming language;textual programming languages;Luna standard library;serverless function;visual-textual framework;Luna serverless framework;AWS Lambda;JavaScript;Visualization;Cloud computing;Computer languages;Computational modeling;Market research;Programming;Semantics;serverless;functional programming;visual programming},
}

@InProceedings{8591002,
  author    = {S. K. {Mohanty} and G. {Premsankar} and M. {di Francesco}},
  title     = {An Evaluation of Open Source Serverless Computing Frameworks},
  booktitle = {2018 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)},
  year      = {2018},
  pages     = {115-120},
  month     = {Dec},
  abstract  = {Recent advancements in virtualization and software architecture have led to the new paradigm of serverless computing, which allows developers to deploy applications as stateless functions without worrying about the underlying infrastructure. Accordingly, a serverless platform handles the lifecycle, execution and scaling of the actual functions; these need to run only when invoked or triggered by an event. Thus, the major benefits of serverless computing are low operational concerns and efficient resource management and utilization. Serverless computing is currently offered by several public cloud service providers. However, there are certain limitations on the public cloud platforms, such as vendor lock-in and restrictions on the computation of the functions. Open source serverless frameworks are a promising solution to avoid these limitations and bring the power of serverless computing to on-premise deployments. However, these frameworks have not been evaluated before. Thus, we carry out a comprehensive feature comparison of popular open source serverless computing frameworks. We then evaluate the performance of selected frameworks: Fission, Kubeless and OpenFaaS. Specifically, we characterize the response time and ratio of successfully received responses under different loads and provide insights into the design choices of each framework.},
  doi       = {10.1109/CloudCom2018.2018.00033},
  issn      = {2330-2186},
  keywords  = {cloud computing;public domain software;software architecture;virtualisation;open source serverless computing frameworks;public cloud service providers;software architecture;resource management;virtualization;Fission;Kubeless;OpenFaaS;Time factors;Cloud computing;Containers;Concurrent computing;FAA;Python;Measurement;serverless computing;function-as-a-service;Kubeless;Fission;OpenFaas;performance evaluation},
}

@InProceedings{Mukhi:2017:USF:3154847.3154852,
  author    = {Mukhi, Nirmal K and Prabhu, Srijith and Slawson, Bruce},
  title     = {Using a Serverless Framework for Implementing a Cognitive Tutor: Experiences and Issues},
  booktitle = {Proceedings of the 2Nd International Workshop on Serverless Computing},
  year      = {2017},
  series    = {WoSC '17},
  pages     = {11--15},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3154852},
  doi       = {10.1145/3154847.3154852},
  isbn      = {978-1-4503-5434-9},
  keywords  = {microservice orchestration, real-time interaction, serverless computing},
  location  = {Las Vegas, Nevada},
  numpages  = {5},
  url       = {http://doi.acm.org/10.1145/3154847.3154852},
}

@InProceedings{Nadgowda:2017:LSA:3154847.3154850,
  author    = {Nadgowda, Shripad and Bila, Nilton and Isci, Canturk},
  title     = {The Less Server Architecture for Cloud Functions},
  booktitle = {Proceedings of the 2Nd International Workshop on Serverless Computing},
  year      = {2017},
  series    = {WoSC '17},
  pages     = {22--27},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3154850},
  doi       = {10.1145/3154847.3154850},
  isbn      = {978-1-4503-5434-9},
  keywords  = {cloud functions, data deduplication, serverless platform},
  location  = {Las Vegas, Nevada},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3154847.3154850},
}

@InProceedings{Nadgowda:2018:RRC:3277180.3277199,
  author    = {Nadgowda, Shripad and Suneja, Sahil and Isci, Canturk},
  title     = {RECap: Run-escape Capsule for On-demand Managed Service Delivery in the Cloud},
  booktitle = {Proceedings of the 10th USENIX Conference on Hot Topics in Cloud Computing},
  year      = {2018},
  series    = {HotCloud'18},
  pages     = {19--19},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3277199},
  location  = {Boston, MA, USA},
  numpages  = {1},
  url       = {http://dl.acm.org/citation.cfm?id=3277180.3277199},
}

@InProceedings{Nasirifard:2017:STC:3155016.3155024,
  author    = {Nasirifard, Pezhman and Slominski, Aleksander and Muthusamy, Vinod and Ishakian, Vatche and Jacobsen, Hans-Arno},
  title     = {A Serverless Topic-based and Content-based Pub/Sub Broker: Demo},
  booktitle = {Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference: Posters and Demos},
  year      = {2017},
  series    = {Middleware '17},
  pages     = {23--24},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3155024},
  doi       = {10.1145/3155016.3155024},
  isbn      = {978-1-4503-5201-7},
  keywords  = {content-based pub/sub, function as a service(FaaS), open-whisk, serverless, topic-based pub/sub},
  location  = {Las Vegas, Nevada},
  numpages  = {2},
  url       = {http://doi.acm.org/10.1145/3155016.3155024},
}
@Inbook{Nastic2018,
author="Nastic, Stefan
and Dustdar, Schahram",
editor="Gruhn, Volker
and Striemer, R{\"u}diger",
title="Towards Deviceless Edge Computing: Challenges, Design Aspects, and Models for Serverless Paradigm at the Edge",
bookTitle="The Essence of Software Engineering",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="121--136",
abstract="The serverless paradigm has been rapidly adopted by developers of cloud-native applications, mainly because it relieves them from the burden of provisioning, scaling, and operating the underlying infrastructure. In this chapter, we propose a novel computing paradigm---Deviceless Edge Computing--- that extends the serverless paradigm to the edge of the network, enabling IoT and Edge devices, such as gateways and micro clouds, to be seamlessly integrated as application execution infrastructure. We propose a reference architecture for the Deviceless Edge Computing. We also analyze the main requirements and challenges to realize this novel computing paradigm from two main points of view: (1) required support for application development, in terms of programming models, and (2) required runtime support for deviceless applications, in terms of main deviceless platform mechanisms. Finally, we show how our existing work in the area of Edge Computing and IoT serves as starting point and as one of main enablers for realizing the emerging Deviceless Edge Computing.",
isbn="978-3-319-73897-0",
doi="10.1007/978-3-319-73897-0_8",
url="https://doi.org/10.1007/978-3-319-73897-0_8"
}

@Article{7994559,
  author   = {S. {Nastic} and T. {Rausch} and O. {Scekic} and S. {Dustdar} and M. {Gusev} and B. {Koteska} and M. {Kostoska} and B. {Jakimovski} and S. {Ristov} and R. {Prodan}},
  title    = {A Serverless Real-Time Data Analytics Platform for Edge Computing},
  journal  = {IEEE Internet Computing},
  year     = {2017},
  volume   = {21},
  number   = {4},
  pages    = {64-71},
  issn     = {1089-7801},
  abstract = {Contemporary solutions for cloud-supported, edge-data analytics mostly apply analytics techniques in a rigid bottom-up approach, regardless of the data's origin. Typically, data are generated at the edge of the infrastructure and transmitted to the cloud, where traditional data analytics techniques are applied. Currently, developers are forced to resort to ad hoc solutions specifically tailored for the available infrastructure (for example, edge devices) when designing, developing, and operating the data analytics applications. Here, a novel approach implements cloud-supported, real-time data analytics in edge-computing applications. The authors introduce their serverless edge-data analytics platform and application model and discuss their main design requirements and challenges, based on real-life healthcare use case scenarios.},
  doi      = {10.1109/MIC.2017.2911430},
  keywords = {cloud computing;data analysis;serverless real-time data analytics;edge computing;cloud supported;edge data analytics;Cloud computing;Data analysis;Analytical models;Real-time systems;Quality of service;Data models;Computational modeling;Internet/Web technologies;real-time data analysis;cloud computing;IoT;edge computing;Internet of Things;security and privacy},
}

@InProceedings{7979853,
  author    = {E. {Oakes} and L. {Yang} and K. {Houck} and T. {Harter} and A. C. {Arpaci-Dusseau} and R. H. {Arpaci-Dusseau}},
  title     = {Pipsqueak: Lean Lambdas with Large Libraries},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems Workshops (ICDCSW)},
  year      = {2017},
  pages     = {395-400},
  month     = {June},
  abstract  = {Microservices are usually fast to deploy because each microservice is small, and thus each can be installed and started quickly. Unfortunately, lean microservices that depend on large libraries will start slowly and harm elasticity. In this paper, we explore the challenges of lean microservices that rely on large libraries in the context of Python packages and the OpenLambda serverless computing platform. We analyze the package types and compressibility of libraries distributed via the Python Package Index and propose PipBench, a new tool for evaluating package support. We also propose Pipsqueak, a package-aware compute platform based on OpenLambda.},
  doi       = {10.1109/ICDCSW.2017.32},
  issn      = {2332-5666},
  keywords  = {programming languages;software libraries;large libraries;Python packages;OpenLambda serverless computing platform;Python package index;cloud computing;Libraries;Containers;Virtual machining;Tools;Cloud computing;Memory management;Linux;serverless computing;cloud computing;distributed computing;distributed systems;python;distributed cache;software repository},
}

@InProceedings{Oakes:2018:SRT:3277355.3277362,
  author    = {Oakes, Edward and Yang, Leon and Zhou, Dennis and Houck, Kevin and Harter, Tyler and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
  title     = {SOCK: Rapid Task Provisioning with Serverless-optimized Containers},
  booktitle = {Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference},
  year      = {2018},
  series    = {USENIX ATC '18},
  pages     = {57--69},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3277362},
  isbn      = {978-1-931971-44-7},
  location  = {Boston, MA, USA},
  numpages  = {13},
  url       = {http://dl.acm.org/citation.cfm?id=3277355.3277362},
}

@InProceedings{Ortiz:2019:ASM:3287324.3287533,
  author    = {Ortiz, Ariel},
  title     = {Architecting Serverless Microservices on the Cloud with AWS},
  booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
  year      = {2019},
  series    = {SIGCSE '19},
  pages     = {1240--1240},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3287533},
  doi       = {10.1145/3287324.3287533},
  isbn      = {978-1-4503-5890-3},
  keywords  = {cloud computing, microservices, restful api, serverless computing},
  location  = {Minneapolis, MN, USA},
  numpages  = {1},
  url       = {http://doi.acm.org/10.1145/3287324.3287533},
}

@InProceedings{Palkar:2017:DHO:3152434.3152459,
  author    = {Palkar, Shoumik and Zaharia, Matei},
  title     = {DIY Hosting for Online Privacy},
  booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
  year      = {2017},
  series    = {HotNets-XVI},
  pages     = {1--7},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3152459},
  doi       = {10.1145/3152434.3152459},
  isbn      = {978-1-4503-5569-8},
  location  = {Palo Alto, CA, USA},
  numpages  = {7},
  url       = {http://doi.acm.org/10.1145/3152434.3152459},
}
@ARTICLE{2019arXiv190507228P,
author = {{Pellegrini}, Roland and {Ivkic}, Igor and {Tauber}, Markus},
title = "{Towards a Security-Aware Benchmarking Framework for Function-as-a-Service}",
journal = {arXiv e-prints},
keywords = {Computer Science - Software Engineering, Computer Science - Cryptography and Security},
year = "2019",
month = "May",
eid = {arXiv:1905.07228},
pages = {arXiv:1905.07228},
archivePrefix = {arXiv},
eprint = {1905.07228},
primaryClass = {cs.SE},
adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190507228P},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2019arXiv190511707P,
author = {{Pellegrini}, Roland and {Ivkic}, Igor and {Tauber}, Markus},
title = "{Function-as-a-Service Benchmarking Framework}",
journal = {arXiv e-prints},
keywords = {Computer Science - Performance, Computer Science - Distributed, Parallel, and Cluster Computing},
year = "2019",
month = "May",
eid = {arXiv:1905.11707},
pages = {arXiv:1905.11707},
archivePrefix = {arXiv},
eprint = {1905.11707},
primaryClass = {cs.PF},
adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190511707P},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@InProceedings{8544423,
  author    = {K. J. P. G. {Perera} and I. {Perera}},
  title     = {A Rule-based System for Automated Generation of Serverless-Microservices Architecture},
  booktitle = {2018 IEEE International Systems Engineering Symposium (ISSE)},
  year      = {2018},
  pages     = {1-8},
  month     = {Oct},
  abstract  = {Software being ubiquitous in today's systems and business operations, it's highly important to structure the high-level architecture of a software application accordingly to deliver the expected customer requirements while accounting for quality measures such as scalability, high availability and high performance. We propose The Architect, a rule-based system for serverless-microservices based high-level architecture generation. In the process of auto generating serverless-microservices high-level architecture, TheArchitect will preserve the highlighted quality measures. It will also provide a tool based support for the high-level architecture designing process of the software architect. Any software developer will be able to use TheArchitect to generate a proper architecture minimizing the involvement of a software architect. Furthermore, the positives of microservices and serverless technologies have made a significant impact on the software engineering community in terms of shifting from the era of building large monolith applications containing overly complex designs, to microservices and serverless based technologies. Hence The Architect focuses on generating best fitted microservices and serverless based high-level architecture for a given application.},
  doi       = {10.1109/SysEng.2018.8544423},
  keywords  = {knowledge based systems;software architecture;software prototyping;rule-based system;software architect;software developer;software engineering community;serverless based technologies;automated generation;business operations;software application;architect;customer requirements;serverless-microservices high-level architecture generation;quality measures;high-level architecture designing process;microservices based technologies;Computer architecture;Software;Knowledge based systems;Calculators;Software architecture;Architecture;Generators;Software Architecture;Microservices Architecture;Serverless Architecture;Rule-based Systems;Domain Specific Software Architecture},
}

@InProceedings{8466390,
  author    = {K. J. P. G. {Perera} and I. {Perera}},
  title     = {TheArchitect: A Serverless-Microservices Based High-level Architecture Generation Tool},
  booktitle = {2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS)},
  year      = {2018},
  pages     = {204-210},
  month     = {June},
  abstract  = {Software is ubiquitous in today's systems and business operations. Most importantly the architecture of a software system determines its quality and longevity, because the development work related to the software system will be carried out to be in line with its architecture design. Hence, it's highly important to structure the high-level software architecture accordingly to deliver the expected customer requirements while accounting for quality measures such as scalability, high availability and high performance. We propose TheArchitect, a serverless-microservices based high-level architecture generation tool, which will auto generate serverless-microservices based high-level architecture for a given business application, preserving the highlighted quality measures providing a tool based support for the software architect with respect to designing the high-level architecture. TheArchitect will provide any software developer to generate a proper architecture minimizing the involvement of an experienced software architect. Furthermore, the positives that microservices and serverless technologies has brought to the world of software engineering has made the software engineering community shift from the era of building large monolith applications containing overly complex designs, to microservices and serverless based technologies. Hence TheArchitect focuses on generating best fitted microservices and serverless based high-level architecture for a given application.},
  doi       = {10.1109/ICIS.2018.8466390},
  keywords  = {software architecture;business operations;software system;high-level software architecture;software developer;software engineering community shift;serverless based technologies;TheArchitect tool;serverless-microservices based high-level architecture generation tool;Computer architecture;Software;Calculators;Data models;Databases;Tools;Architecture;Software Architecture;Microservices Architecture;Serverless Architecture},
}

@Article{PEREZ201850,
  author   = {Alfonso Pérez and Germán Moltó and Miguel Caballer and Amanda Calatrava},
  title    = {Serverless computing for container-based architectures},
  journal  = {Future Generation Computer Systems},
  year     = {2018},
  volume   = {83},
  pages    = {50 - 59},
  issn     = {0167-739X},
  abstract = {New architectural patterns (e.g. microservices), the massive adoption of Linux containers (e.g. Docker containers), and improvements in key features of Cloud computing such as auto-scaling, have helped developers to decouple complex and monolithic systems into smaller stateless services. In turn, Cloud providers have introduced serverless computing, where applications can be defined as a workflow of event-triggered functions. However, serverless services, such as AWS Lambda, impose serious restrictions for these applications (e.g. using a predefined set of programming languages or difficulting the installation and deployment of external libraries). This paper addresses such issues by introducing a framework and a methodology to create Serverless Container-aware ARchitectures (SCAR). The SCAR framework can be used to create highly-parallel event-driven serverless applications that run on customized runtime environments defined as Docker images on top of AWS Lambda. This paper describes the architecture of SCAR together with the cache-based optimizations applied to minimize cost, exemplified on a massive image processing use case. The results show that, by means of SCAR, AWS Lambda becomes a convenient platform for High Throughput Computing, specially for highly-parallel bursty workloads of short stateless jobs.},
  doi      = {https://doi.org/10.1016/j.future.2018.01.022},
  keywords = {Cloud computing, Serverless, Docker, Elasticity, AWS lambda},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X17316485},
}

@inproceedings{Perez:2019:PMM:3297280.3297292,
 author = {P{\'e}rez, Alfonso and Molt\'{o}, Germ\'{a}n and Caballer, Miguel and Calatrava, Amanda},
 title = {A Programming Model and Middleware for High Throughput Serverless Computing Applications},
 booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
 series = {SAC '19},
 year = {2019},
 isbn = {978-1-4503-5933-7},
 location = {Limassol, Cyprus},
 pages = {106--113},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/3297280.3297292},
 doi = {10.1145/3297280.3297292},
 acmid = {3297292},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloud computing, containers, event-driven architectures, serverless},
}

@InProceedings{Persson:2017:KSI:3154847.3154853,
  author    = {Persson, Per and Angelsmark, Ola},
  title     = {Kappa: Serverless IoT Deployment},
  booktitle = {Proceedings of the 2Nd International Workshop on Serverless Computing},
  year      = {2017},
  series    = {WoSC '17},
  pages     = {16--21},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3154853},
  doi       = {10.1145/3154847.3154853},
  isbn      = {978-1-4503-5434-9},
  keywords  = {FaaS, IoT, actor model, cloud computing, data flow, serverless},
  location  = {Las Vegas, Nevada},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3154847.3154853},
}

@InProceedings{Pfretzschner:2017:IDA:3098954.3120928,
  author    = {Pfretzschner, Brian and ben Othmane, Lotfi},
  title     = {Identification of Dependency-based Attacks on Node.Js},
  booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security},
  year      = {2017},
  series    = {ARES '17},
  pages     = {68:1--68:6},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3120928},
  articleno = {68},
  doi       = {10.1145/3098954.3120928},
  isbn      = {978-1-4503-5257-4},
  keywords  = {Cloud computing, Dependency-based attack, Node.js, Software security},
  location  = {Reggio Calabria, Italy},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3098954.3120928},
}

@InProceedings{8588841,
  author    = {D. {Pinto} and J. P. {Dias} and H. {Sereno Ferreira}},
  title     = {Dynamic Allocation of Serverless Functions in IoT Environments},
  booktitle = {2018 IEEE 16th International Conference on Embedded and Ubiquitous Computing (EUC)},
  year      = {2018},
  pages     = {1-8},
  month     = {Oct},
  abstract  = {The IoT area has grown significantly in the last few years and is expected to reach a gigantic amount of 50 billion devices by 2020. The appearance of serverless architectures, specifically highlighting FaaS, raises the question of the suitability of using them in IoT environments. Combining IoT with a serverless architectural design can effective when trying to make use of local processing power that exists in a local network of IoT devices and creating a fog layer that leverages computational capabilities that are closer to the end-user. In this approach, which is placed between the device and the serverless function, when a device requests for the execution of a serverless function will decide based on previous metrics of execution if the serverless function should be executed locally, in the fog layer of a local network of IoT devices, or if it should be executed remotely, in one of the available cloud servers. Therefore, this approach allows dynamically allocating functions to the most suitable layer.},
  doi       = {10.1109/EUC.2018.00008},
  keywords  = {cloud computing;computer networks;Internet of Things;serverless function;device requests;fog layer;local network;IoT devices;IoT environments;IoT area;serverless architectural design;local processing power;cloud servers;Cloud computing;Servers;Edge computing;Runtime environment;Internet of Things;Market research;Estimation;Fog Computing;Internet of Things;Multi Armed Bandit;Ubiquitous Computing;Serverless},
}

@Article{doi:10.1002/smr.1930,
  author   = {Plaza, Andrea M. and D�az, Jessica and P�rez, Jennifer},
  title    = {Software architectures for health care cyber-physical systems: A systematic literature review},
  journal  = {Journal of Software: Evolution and Process},
  year     = {2018},
  volume   = {30},
  number   = {7},
  pages    = {e1930},
  note     = {e1930 JSME-16-0265.R3},
  abstract = {Abstract Cyber-physical systems (CPS) refer to the next generation of Information and Communication Technology systems that mainly integrate sensing, computing, and communication to monitor, control, and interact with a physical process to provide citizens and businesses with smart applications and services: health care, smart homes, smart cities, and so on. In recent years, health care has become one of the most important services due to the continuous increases in its costs. This has motivated extensive research on health care CPS, and some of that research has focused on describing the software architecture behind these systems. However, there is no secondary study to consolidate the research. This paper aims to identify and compare existing research on software architectures for health care CPS in order to determine successful solutions that could guide other architects and practitioners in their health care projects. We conducted a systematic literature review and compared the selected studies based on a characterization schema. The research synthesis results in a knowledge base of software architectures for health care CPS, describing their stakeholders, functional and non-functional features, quality attributes architectural views and styles, components, and implementation technologies. This systematic literature review also identifies research gaps, such as the lack of open common platforms, as well as directions for future research.},
  doi      = {10.1002/smr.1930},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.1930},
  keywords = {cyber-physical systems, health care, Internet of Things, software architecture, systematic literature review},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.1930},
}

@InProceedings{Pu:2019:SFS:3323234.3323251,
  author    = {Pu, Qifan and Venkataraman, Shivaram and Stoica, Ion},
  title     = {Shuffling, Fast and Slow: Scalable Analytics on Serverless Infrastructure},
  booktitle = {Proceedings of the 16th USENIX Conference on Networked Systems Design and Implementation},
  year      = {2019},
  series    = {NSDI'19},
  pages     = {193--206},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3323251},
  isbn      = {978-1-931971-49-2},
  location  = {Boston, MA, USA},
  numpages  = {14},
  url       = {http://dl.acm.org/citation.cfm?id=3323234.3323251},
}

@InProceedings{8116416,
  author    = {H. {Puripunpinyo} and M. H. {Samadzadeh}},
  title     = {Effect of optimizing Java deployment artifacts on AWS Lambda},
  booktitle = {2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)},
  year      = {2017},
  pages     = {438-443},
  month     = {May},
  abstract  = {AWS Lambda (Amazon Web Services) is the most popular serverless architecture provided by Amazon. It currently supports three platforms: JavaScript, Python, and Java Virtual Machine (JVM). The JVM could be the most complicate platform among the three as there are many languages that target the JVM platform besides Java. In addition, the complex hierarchy of dependencies, versioning, and the class loader are major issues that could cause conflict in a project. Deployment in the context of a serverless architecture means deployment as a function that represents a single service rather than as an application that is comprised of many services. AWS Lambda requires a deployment artifact to be self-contained which means all resources and dependencies must be packaged into a single jar file, and this file could be larger than AWS Lambda's allowable limit. Developers usually use build tool plugins to make self-contained artifacts, and those tools are generally unaware of what class and resource files a function needs. As a result, the artifact is not optimized. This paper demonstrates that optimization of an artifact can in general improve its resource usage and runtime performance. This paper also reports the result of an anecdotal experiment regarding the overhead of calling functions remotely in order to support design decisions in the development of AWS Lambda.},
  doi       = {10.1109/INFCOMW.2017.8116416},
  keywords  = {Java;operating systems (computers);virtual machines;Web services;optimizing Java deployment artifacts;Amazon Web Services;Java Virtual Machine;JVM;complicate platform;deployment artifact;single jar file;AWS Lambda's allowable limit;self-contained artifacts;resource files;Java;Libraries;Computer architecture;Tools;Servers;XML;Big Data;Serverless Architecture;Deployment;AWS Lambda;JVM;Java;Optimization;Performance},
}

@InProceedings{10.1007/978-3-030-01701-9_25,
author="Qiang, Weizhong
and Dong, Zezhao
and Jin, Hai",
editor="Beyah, Raheem
and Chang, Bing
and Li, Yingjiu
and Zhu, Sencun",
title="Se-Lambda: Securing Privacy-Sensitive Serverless Applications Using SGX Enclave",
booktitle="Security and Privacy in Communication Networks",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="451--470",
abstract="Serverless computing is an emerging trend in the cloud, which represents a new paradigm for deploying applications and services. In the serverless computing framework, cloud users can deploy arbitrary code and process data on the service runtime. However, as neither cloud users nor cloud providers are trustworthy, serverless computing platform suffers from trust issues caused by both sides. In this paper, we propose a new serverless computing framework called Se-Lambda, which protects the API gateway by using SGX enclave and the service runtime by leveraging a two-way sandbox that combines SGX enclave and WebAssembly sandboxed environment. In the proposed service runtime, users' untrusted code is confined by WebAssembly sandboxed environment, while SGX enclave prevents malicious cloud providers from stealing users' privacy-sensitive data. In addition, we implement a privilege monitoring mechanism in SGX enclave to manage the access control of function modules from users. We implement the prototype of Se-Lambda based on the open source project OpenLambda. The experimental results show that the Se-Lambda imposes a low performance penalty, while buying a significantly increased level of security.",
isbn="978-3-030-01701-9"
}

@Inbook{Ramachandran2018,
author="Ramachandran, Muthu",
editor="Mahmood, Zaigham",
title="SEF-SCC: Software Engineering Framework for Service and Cloud Computing",
bookTitle="Fog Computing: Concepts, Frameworks and Technologies",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="227--248",
abstract="Service computingService computingand cloud computing have emerged to address the need for more flexible and cost-efficient computing systems where software is delivered as a service. To make this more resilient and reliable, we need to adopt software engineering (SE) principles and best practices that have existed for the last 40 years or so. Therefore, this chapter proposes a Software EngineeringSoftware EngineeringFramework for Service and Cloud ComputingCloud Computing(SEF-SCC) to address the need for a systematic approach to design and develop robust, resilient, and reusable services. This chapter presents SEF-SCC methods, techniques, and a systematic engineering process supporting the development of service-oriented software systems and software as a service paradigms. SEF-SCC has been successfully validated for the past 10 years based on a large-scale case study on British Energy Power and Energy Trading (BEPETBEPET). Ideas and concepts suggested in this chapter are equally applicable to all distributed computing environments including Fog and Edge Computing paradigms.",
isbn="978-3-319-94890-4",
doi="10.1007/978-3-319-94890-4_11",
url="https://doi.org/10.1007/978-3-319-94890-4_11"
}

@InProceedings{8436935,
  author    = {S. {Rizou} and P. {Athanasoulis} and P. {Andriani} and F. {Iadanza} and G. {Carrozzo} and D. {Breitgand} and A. {Weit} and D. {Griffin} and D. {Jimenez} and U. {Acar} and O. P. {Gordo}},
  title     = {A Service Platform Architecture Enabling Programmable Edge-To-Cloud Virtualization for the 5G Media Industry},
  booktitle = {2018 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)},
  year      = {2018},
  pages     = {1-6},
  month     = {June},
  abstract  = {Media applications are amongst the most demanding services in terms of resources, requiring huge network capacity for high bandwidth audio-visual and other mobile sensory streams. The 5G-MEDIA project aims at innovating media-related applications by investigating how these applications and the underlying 5G network should be coupled and interwork to the benefit of both. The 5G-MEDIA approach aims at delivering an integrated programmable service platform for the development, design and operations of media applications in 5G networks by providing mechanisms to flexibly adapt service operations to dynamic conditions and react upon events (e.g. to transparently accommodate auto-scaling of resources, VNF replacement, etc.). In this paper we present the 5G-MEDIA service platform architecture, which has been specifically designed to enable the development and operation of services for the nascent 5G media industry. Our approach delivers an integrated programmable service platform for the development, design and operations of media applications in 5G networks.},
  doi       = {10.1109/BMSB.2018.8436935},
  issn      = {2155-5052},
  keywords  = {5G mobile communication;cloud computing;multimedia communication;telecommunication computing;virtualisation;Programmable Edge-To-Cloud Virtualization;media applications;5G-MEDIA project;media-related applications;integrated programmable service platform;5G-MEDIA service platform architecture;network capacity;5G network;5G media industry;service platform architecture;mobile sensory streams;high bandwidth audio-visual system;Media;5G mobile communication;Monitoring;FAA;Tools;Computer architecture;Optimization;Network Function Virtualization;edge-cloud;management and operation framework;5G networks for media applications},
}

@InProceedings{8457882,
  author    = {A. {Saha} and S. {Jindal}},
  title     = {EMARS: Efficient Management and Allocation of Resources in Serverless},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {827-830},
  month     = {July},
  abstract  = {We introduce EMARS, an efficient resource management system for serverless cloud computing frameworks with the goal to enhance resource (focus on memory) allocation among containers. We have built our prototype on top of an open-source serverless platform, OpenLambda. It is based upon application workloads and serverless functions' memory needs. As a background motivation we analyzed the latencies and memory requirements of functions running on AWS lambda. The memory limits also lead to variations in number of containers spawned on OpenLambda. We use memory limit settings to propose a model of predictive efficient memory management.},
  doi       = {10.1109/CLOUD.2018.00113},
  issn      = {2159-6190},
  keywords  = {cloud computing;resource allocation;storage management;serverless cloud;open-source serverless platform;OpenLambda;serverless functions;memory limit settings;predictive efficient memory management;resource management system;EMARS system;Containers;Memory management;Resource management;Cloud computing;Servers;Predictive models;Time factors;Serverless;cloud computing;memory limit;response time},
}

@inproceedings{Samea:2019:UPM:3316615.3316636,
 author = {Samea, Fatima and Azam, Farooque and Anwar, Muhammad Waseem and Khan, Mehreen and Rashid, Muhammad},
 title = {A UML Profile for Multi-Cloud Service Configuration (UMLPMSC) in Event-driven Serverless Applications},
 booktitle = {Proceedings of the 2019 8th International Conference on Software and Computer Applications},
 series = {ICSCA '19},
 year = {2019},
 isbn = {978-1-4503-6573-4},
 location = {Penang, Malaysia},
 pages = {431--435},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3316615.3316636},
 doi = {10.1145/3316615.3316636},
 acmid = {3316636},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MDA, Serverless computing, Service Configuration, UMLPMSC},
}

@InProceedings{Sampe:2017:DSF:3135974.3135980,
  author    = {Samp{\'e}, Josep and S\'{a}nchez-Artigas, Marc and Garc\'{\i}a-L\'{o}pez, Pedro and Par\'{\i}s, Gerard},
  title     = {Data-driven Serverless Functions for Object Storage},
  booktitle = {Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference},
  year      = {2017},
  series    = {Middleware '17},
  pages     = {121--133},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3135980},
  doi       = {10.1145/3135974.3135980},
  isbn      = {978-1-4503-4720-4},
  keywords  = {cloud computing, data flow interception, data management, object storage, openstack swift, programmability, serverless functions},
  location  = {Las Vegas, Nevada},
  numpages  = {13},
  url       = {http://doi.acm.org/10.1145/3135974.3135980},
}

@InProceedings{Sampe:2018:SDA:3284028.3284029,
  author    = {Samp{\'e}, Josep and Vernik, Gil and S\'{a}nchez-Artigas, Marc and Garc\'{\i}a-L\'{o}pez, Pedro},
  title     = {Serverless Data Analytics in the IBM Cloud},
  booktitle = {Proceedings of the 19th International Middleware Conference Industry},
  year      = {2018},
  series    = {Middleware '18},
  pages     = {1--8},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3284029},
  doi       = {10.1145/3284028.3284029},
  isbn      = {978-1-4503-6016-6},
  keywords  = {Distributed computing, IBM Cloud Functions, IBM Cloud Object Storage, PyWren, Serverless computing},
  location  = {Rennes, France},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/3284028.3284029},
}

@Article{Savage:2018:GS:3181977.3171583,
  author     = {Savage, Neil},
  title      = {Going Serverless},
  journal    = {Commun. ACM},
  year       = {2018},
  volume     = {61},
  number     = {2},
  pages      = {15--16},
  month      = jan,
  issn       = {0001-0782},
  acmid      = {3171583},
  address    = {New York, NY, USA},
  doi        = {10.1145/3171583},
  issue_date = {February 2018},
  numpages   = {2},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3171583},
}

@InProceedings{8529465,
  author    = {M. {Sewak} and S. {Singh}},
  title     = {Winning in the Era of Serverless Computing and Function as a Service},
  booktitle = {2018 3rd International Conference for Convergence in Technology (I2CT)},
  year      = {2018},
  pages     = {1-5},
  month     = {April},
  abstract  = {Serverless Computing and Function as a Service (FaaS) is gaining traction in cloud-based application architectures used by startups and matured organizations alike. Organizations that are keen to leverage modern technology to gain a disruptive edge, optimal efficiency, advanced agility and save cost are adopting these architectural styles rapidly. Cloud service provider offer and dynamically manages the allocation of machine resources in serverless computing. The serverless architectures allows the developers to focus on business logic exclusively without worrying about preparing the runtime, managing deployment and infrastructure related concerns. FaaS may be assumed as a subset of Serverless Computing, in which, instead of coding a full-fledged cloud based application, the developer just writes (often small) functions which are piece of code (in one of the multiple programming languages supported by the platform) dedicated to do a focused, often single task that are invoked by triggers. It offers dynamic allocation and scaling of the resources and innovative trigger based costing model. This paper introduces Serverless Computing, and Function as a Service (FaaS), explores its advantages and limitations, options available with popular cloud and Platform as a Service (PaaS) providers, and emerging use cases and success stories.},
  doi       = {10.1109/I2CT.2018.8529465},
  keywords  = {cloud computing;service-oriented architecture;serverless computing;FaaS;cloud-based application architectures;serverless architectures;full-fledged cloud based application;cloud service providers;Function as a Service;Cloud computing;FAA;Google;Computer architecture;Servers;Task analysis;Serverless Computing;Functions as a Service(FaaS);Platform as a Service (PaaS);Microservices;IBM Cloud Functions;Amazon AWS Lambda;Microsoft Azure Functions;Google Cloud Functions;Apache Open Whisk},
}

@Article{2018arXiv181009679S,
  author        = {{Shankar}, Vaishaal and {Krauth}, Karl and {Pu}, Qifan and {Jonas}, Eric and {Venkataraman}, Shivaram and {Stoica}, Ion and {Recht}, Benjamin and {Ragan-Kelley}, Jonathan},
  title         = {{numpywren: serverless linear algebra}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1810.09679},
  month         = {Oct},
  abstract      = {Linear algebra operations are widely used in scientific computing and machine learning applications. However, it is challenging for scientists and data analysts to run linear algebra at scales beyond a single machine. Traditional approaches either require access to supercomputing clusters, or impose configuration and cluster management challenges. In this paper we show how the disaggregation of storage and compute resources in so-called "serverless" environments, combined with compute-intensive workload characteristics, can be exploited to achieve elastic scalability and ease of management. We present numpywren, a system for linear algebra built on a serverless architecture. We also introduce LAmbdaPACK, a domain-specific language designed to implement highly parallel linear algebra algorithms in a serverless setting. We show that, for certain linear algebra algorithms such as matrix multiply, singular value decomposition, and Cholesky decomposition, numpywren's performance (completion time) is within 33% of ScaLAPACK, and its compute efficiency (total CPU-hours) is up to 240% better due to elasticity, while providing an easier to use interface and better fault tolerance. At the same time, we show that the inability of serverless runtimes to exploit locality across the cores in a machine fundamentally limits their network efficiency, which limits performance on other algorithms such as QR factorization. This highlights how cloud providers could better support these types of computations through small changes in their infrastructure.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181009679S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1810.09679},
  eprint        = {1810.09679},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1810.09679.pdf},
}

@InProceedings{Singhvi:2017:GCN:3152434.3152450,
  author    = {Singhvi, Arjun and Banerjee, Sujata and Harchol, Yotam and Akella, Aditya and Peek, Mark and Rydin, Pontus},
  title     = {Granular Computing and Network Intensive Applications: Friends or Foes?},
  booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
  year      = {2017},
  series    = {HotNets-XVI},
  pages     = {157--163},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3152450},
  doi       = {10.1145/3152434.3152450},
  isbn      = {978-1-4503-5569-8},
  location  = {Palo Alto, CA, USA},
  numpages  = {7},
  url       = {http://doi.acm.org/10.1145/3152434.3152450},
}

@Article{SOLTANI2018121,
  author   = {Boubaker Soltani and Afifa Ghenai and Nadia Zeghib},
  title    = {Towards Distributed Containerized Serverless Architecture in Multi Cloud Environment},
  journal  = {Procedia Computer Science},
  year     = {2018},
  volume   = {134},
  pages    = {121 - 128},
  issn     = {1877-0509},
  note     = {The 15th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2018) / The 13th International Conference on Future Networks and Communications (FNC-2018) / Affiliated Workshops},
  abstract = {Compared to the traditional Cloud solution which consists of hiring resources from a single Cloud provider, Multi Cloud is a rather more efficient solution; because it combines the diverse benefits from them, thus, the different platforms complement each other on behalf the client applications. Moreover, Serverless Function technology is a powerful Cloud tool that hides the unnecessary infrastructure management details, hence, allowing the developers to solely focus on their own functional code. Nevertheless, as far as we found, Serverless Functions are always limited to the provider offering them and they are not adopted in a Multi Cloud context. In this paper, we tackled this limit by suggesting a distributed architecture which extends the Serverless technology advantages to a wider scope, permitting the client to get at time the Multi Cloud and Serverless strengths.},
  doi      = {https://doi.org/10.1016/j.procs.2018.07.152},
  keywords = {Multi Cloud, Serverless architecture, Peer to Peer, container, container cluster manager},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050918311153},
}

@InProceedings{8469537,
  author    = {Y. {Song} and J. {Xie} and Q. {Huang} and M. {Wang} and J. {Yu}},
  title     = {Design and Implementation of Turtle Breeding System Based on Embedded Container Cloud},
  booktitle = {2018 2nd IEEE Advanced Information Management,Communicates,Electronic and Automation Control Conference (IMCEC)},
  year      = {2018},
  pages     = {2531-2534},
  month     = {May},
  abstract  = {With the rapid development of computer electronic equipment, embedded devices have been applied in various fields of daily life. Such as intelligent home, intelligent agriculture, intelligent farming and so on. This article describes the intelligent turtle breeding system is a typical application. Different from the combination of traditional embedded devices and servers, this system is based on the combination of the Raspberry Pi and the virtualized container. It is designed as a platform of automatic detection of temperature, humidity, co2concentration, and light intensity at turtle breeding bases. Breaking through the limitations of traditional server monitoring, we use Docker to build a cloud environment and Docker Swarm as a container management technology, and adopt openFaas architecture to complete the serverless integration of virtual container cloud and embedded devices. Making the turtle aquaculture system more secure, more stable, less costly, and more resource efficient.},
  doi       = {10.1109/IMCEC.2018.8469537},
  keywords  = {agriculture;aquaculture;cloud computing;embedded systems;knowledge based systems;virtualisation;embedded container cloud;computer electronic equipment;intelligent home;intelligent agriculture;intelligent farming;intelligent turtle breeding system;Raspberry Pi;virtualized container;cloud environment;container management technology;virtual container cloud;turtle aquaculture system;server monitoring;openFaas architecture;Docker Swarm;Automation;Raspberry pi;container;Turtle farming system},
}

@Article{2017arXiv170508169S,
  author        = {{Spillner}, Josef},
  title         = {{Transformation of Python Applications into Function-as-a-Service Deployments}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1705.08169},
  month         = {May},
  abstract      = {New cloud programming and deployment models pose challenges to software application engineers who are looking, often in vain, for tools to automate any necessary code adaptation and transformation. Function-as-a-Service interfaces are particular non-trivial targets when considering that most cloud applications are implemented in non-functional languages. Among the most widely used of these languages is Python. This starting position calls for an automated approach to transform monolithic Python code into modular FaaS units by partially automated decomposition. Hence, this paper introduces and evaluates Lambada, a Python module to dynamically decompose, convert and deploy unmodified Python code into AWS Lambda functions. Beyond the tooling in the form of a measured open source prototype implementation, the paper contributes a description of the algorithms and code rewriting rules as blueprints for transformations of other scripting languages. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170508169S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1705.08169},
  eprint        = {1705.08169},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Software Engineering, D.2.1, I.2.2, C.2.4},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1705.08169.pdf},
}

@Article{2017arXiv170307562S,
  author        = {{Spillner}, Josef},
  title         = {{Snafu: Function-as-a-Service (FaaS) Runtime Design and Implementation}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1703.07562},
  month         = {Mar},
  abstract      = {Snafu, or Snake Functions, is a modular system to host, execute and manage language-level functions offered as stateless (micro-)services to diverse external triggers. The system interfaces resemble those of commercial FaaS providers but its implementation provides distinct features which make it overall useful to research on FaaS and prototyping of FaaS-based applications. This paper argues about the system motivation in the presence of already existing alternatives, its design and architecture, the open source implementation and collected metrics which characterise the system. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170307562S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1703.07562},
  eprint        = {1703.07562},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, C.2.4, H.3.5, D.1.1},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1703.07562.pdf},
}
@ARTICLE{2019arXiv190504800S,
author = {{Spillner}, Josef},
title = "{Quantitative Analysis of Cloud Function Evolution in the AWS Serverless Application Repository}",
journal = {arXiv e-prints},
keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, D.2.8, C.2.4, K.6.3},
year = "2019",
month = "May",
eid = {arXiv:1905.04800},
pages = {arXiv:1905.04800},
archivePrefix = {arXiv},
eprint = {1905.04800},
primaryClass = {cs.DC},
adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190504800S},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@InProceedings{Spillner:2017:PTS:3147213.3149452,
  author    = {Spillner, Josef},
  title     = {Practical Tooling for Serverless Computing},
  booktitle = {Proceedings of the10th International Conference on Utility and Cloud Computing},
  year      = {2017},
  series    = {UCC '17},
  pages     = {185--186},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3149452},
  doi       = {10.1145/3147213.3149452},
  isbn      = {978-1-4503-5149-2},
  keywords  = {faas, hosted functions, microservices, serverless, tutorial},
  location  = {Austin, Texas, USA},
  numpages  = {2},
  url       = {http://doi.acm.org/10.1145/3147213.3149452},
}

@Article{2017arXiv170205510S,
  author        = {{Spillner}, Josef and {Dorodko}, Serhii},
  title         = {{Java Code Analysis and Transformation into AWS Lambda Functions}},
  journal       = {arXiv e-prints},
  year          = {2017},
  pages         = {arXiv:1702.05510},
  month         = {Feb},
  abstract      = {Software developers are faced with the issue of either adapting their programming model to the execution model (e.g. cloud platforms) or finding appropriate tools to adapt the model and code automatically. A recent execution model which would benefit from automated enablement is Function-as-a-Service. Automating this process requires a pipeline which includes steps for code analysis, transformation and deployment. In this paper, we outline the design and runtime characteristics of Podilizer, a tool which implements the pipeline specifically for Java source code as input and AWS Lambda as output. We contribute technical and economic metrics about this concrete 'FaaSification' process by observing the behaviour of Podilizer with two representative Java software projects.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170205510S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1702.05510},
  eprint        = {1702.05510},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Software Engineering, D.2.1, I.2.2, C.2.4},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1702.05510.pdf},
}

@InProceedings{10.1007/978-3-319-73353-1_11,
author="Spillner, Josef
and Mateos, Cristian
and Monge, David A.",
editor="Mocskos, Esteban
and Nesmachnow, Sergio",
title="FaaSter, Better, Cheaper: The Prospect of Serverless Scientific Computing and HPC",
booktitle="High Performance Computing",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="154--168",
abstract="The adoption of cloud computing facilities and programming models differs vastly between different application domains. Scalable web applications, low-latency mobile backends and on-demand provisioned databases are typical cases for which cloud services on the platform or infrastructure level exist and are convincing when considering technical and economical arguments. Applications with specific processing demands, including high-performance computing, high-throughput computing and certain flavours of scientific computing, have historically required special configurations such as compute- or memory-optimised virtual machine instances. With the rise of function-level compute instances through Function-as-a-Service (FaaS) models, the fitness of generic configurations needs to be re-evaluated for these applications. We analyse several demanding computing tasks with regards to how FaaS models compare against conventional monolithic algorithm execution. Beside the comparison, we contribute a refined FaaSification process for legacy software and provide a roadmap for future work.",
isbn="978-3-319-73353-1"
}

@Article{2019arXiv190103086S,
  author        = {{Stein}, Manuel},
  title         = {{Adaptive Event Dispatching in Serverless Computing Infrastructures}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.03086},
  month         = {Jan},
  abstract      = {Serverless computing is an emerging Cloud service model. It is currently gaining momentum as the next step in the evolution of hosted computing from capacitated machine virtualisation and microservices towards utility computing. The term "serverless" has become a synonym for the entirely resource-transparent deployment model of cloud-based event-driven distributed applications. This work investigates how adaptive event dispatching can improve serverless platform resource efficiency and contributes a novel approach that allows for better scaling and fitting of the platform's resource consumption to actual demand. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190103086S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.03086},
  eprint        = {1901.03086},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1901.03086.pdf},
}

@Article{2019arXiv190102680S,
  author        = {{Stein}, Manuel},
  title         = {{Interim Report on Adaptive Event Dispatching in Serverless Computing Infrastructures}},
  journal       = {arXiv e-prints},
  year          = {2019},
  pages         = {arXiv:1901.02680},
  month         = {Jan},
  abstract      = {Serverless computing is an emerging service model in distributed computing systems. The term captures cloud-based event-driven distributed application design and stems from its completely resource-transparent deployment model, i.e. serverless. This work thesisizes that adaptive event dispatching can improve current serverless platform resource efficiency by considering locality and dependencies. These design contemplations have also been formulated by Hendrickson et al., which identifies the requirement that "Serverless load balancers must make low-latency decisions while considering session, code and data locality". This interim report investigates the economical importance of the emerging trend and asserts that existing serverless platforms still do not optimize for data locality, whereas a variety of scheduling methods are available from distributed computing research which have proven to increase resource efficiency. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190102680S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1901.02680},
  eprint        = {1901.02680},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1901.02680.pdf},
}

@Article{2018arXiv180906100S,
  author        = {{Stein}, Manuel},
  title         = {{The Serverless Scheduling Problem and NOAH}},
  journal       = {arXiv e-prints},
  year          = {2018},
  pages         = {arXiv:1809.06100},
  month         = {Sep},
  abstract      = {The serverless scheduling problem poses a new challenge to Cloud service platform providers because it is rather a job scheduling problem than a traditional resource allocation or request load balancing problem. Traditionally, elastic cloud applications use managed virtual resource allocation and employ request load balancers to orchestrate the deployment. With serverless, the provider needs to solve both the load balancing and the allocation. This work reviews the current Apache OpenWhisk serverless event load balancing and a noncooperative game-theoretic load balancing approach for response time minimization in distributed systems. It is shown by simulation that neither performs well under high system utilization which inspired a noncooperative online allocation heuristic that allows tuning the trade-off between for response time and resource cost of each serverless function. },
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180906100S},
  archiveprefix = {arXiv},
  eid           = {arXiv:1809.06100},
  eprint        = {1809.06100},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/pdf/1809.06100.pdf},
}

@InProceedings{10.1007/978-3-319-72125-5_16,
author="Tai, Stefan",
editor="Lazovik, Alexander
and Schulte, Stefan",
title="Continuous, Trustless, and Fair: Changing Priorities in Services Computing",
booktitle="Advances in Service-Oriented and Cloud Computing",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="205--210",
abstract="Services computing research and practice traditionally has focused on the objectives of business alignment, software systems interoperability and on leveraging the Web as a compute platform. Corresponding technology solution stacks and architectural styles have been promoted. Today, and probably for the next decade to come, different objectives are replacing these original ones and, correspondingly, different solution stacks and architectural styles are emerging. Most notably, challenges such as frequent delivery of service systems, decentralization and business disintermediation, and ``socially aligned'' service systems lead us to continuous computing, trustless computing, and fair computing -- three major trends that we expect to become the driving force behind next-generation service systems. In this paper, we discuss these trends and identify major research directions to deliver on these changing priorities.",
isbn="978-3-319-72125-5"
}

@InProceedings{8301615,
  author    = {Y. {Tian} and R. {Babcock} and C. {Taylor} and Y. {Ji}},
  title     = {A new live video streaming approach based on Amazon S3 pricing model},
  booktitle = {2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC)},
  year      = {2018},
  pages     = {321-328},
  month     = {Jan},
  abstract  = {Video has become a mainstream media source on the web, and live video streaming is growing as a prominent player in the modern marketplace for both businesses and individuals. Due to elasticity and on-demand nature that cloud computing provides, more and more video service providers (VSP) utilize cloud infrastructures to provide cloud-based video applications. In this work, we propose Cloud Live Video Streaming (CLVS) - a new approach to efficiently stream live video that is based on Amazon S3 pricing model. In the design of CLVS, when a source video is being recorded by a mobile device, on which then it is segmented and encoded. Next, those video segments are pushed into a designated Amazon S3 bucket. On end-user devices, the client program of CLVS directly retrieves the most recent video segment from the S3 bucket, then performs decoding and video playing back. By using Amazon S3 service, our CLVS employs what is referred to as a “serverless” design by eliminating the needs for an intermediary and persistently running streaming server. Thus, VSPs do not have to pay for the idle time found on the traditional streaming servers, but still are able to continually provide live video streams to end viewers. We implement a prototype of CLVS and optimize its performance by using multithreaded prefetching and caching techniques. In experiments, we compare our CLVS with an existing video streaming software - Wowza Streaming Engine. Experiment results indicate that CLVS not only outperforms Wowza in many aspects of video streaming performance, but also financially costs less.},
  doi       = {10.1109/CCWC.2018.8301615},
  keywords  = {cloud computing;mobile handsets;multimedia communication;video signal processing;video streaming;live video streaming approach;Amazon S3 pricing model;mainstream media source;video service providers;cloud infrastructures;cloud-based video applications;Cloud Live Video Streaming;CLVS;source video;video segments;Amazon S3 service;traditional streaming servers;Wowza Streaming Engine;video streaming performance;video streaming software;Amazon S3 bucket;VSP;Streaming media;Cloud computing;Servers;Indexes;Pricing;Instruction sets;Cloud computing;Live Video Streaming;Cloud Storage Service},
}

@InProceedings{10.1007/978-3-030-06010-7_4,
author="Toepke, Samuel Lee",
editor="Ragia, Lemonia
and Laurini, Robert
and Rocha, Jorge Gustavo",
title="Implications of Data Density and Length of Collection Period for Population Estimations Using Social Media Data",
booktitle="Geographical Information Systems Theory, Applications and Management",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="57--69",
abstract="When programmatically utilizing public APIs provided by social media services, it is possible to attain a large volume of volunteered geographic information. Geospatially enabled data from Twitter, Instagram, Panaramio, etc. can be used to create high-resolution estimations of human movements over time, with volume of the data being of critical importance. This investigation extends previous work, showing the effects of artificial data removal, and generated error; though using over twice as much collected data, attained using an enterprise cloud solution, over a span of thirteen months instead of five.",
isbn="978-3-030-06010-7"
}

@InProceedings{8666520,
  author    = {R. {Tonelli} and M. I. {Lunesu} and A. {Pinna} and D. {Taibi} and M. {Marchesi}},
  title     = {Implementing a Microservices System with Blockchain Smart Contracts},
  booktitle = {2019 IEEE International Workshop on Blockchain Oriented Software Engineering (IWBOSE)},
  year      = {2019},
  pages     = {22-31},
  month     = {Feb},
  abstract  = {Blockchain technologies and smart contracts are becoming mainstream research fields in computer science and researchers are continuously investigating new frontiers for new applications. Likewise, microservices are getting more and more popular in the latest years thanks to their properties, that allow teams to slice existing information systems into small and independent services that can be developed independently by different teams. A symmetric paradigm applies to smart contracts as well, which represent well defined, usually isolated, executable programs, typically implementing simple and autonomous tasks with a well defined purpose, which can be assumed as services provided by the Contract. In this work we analyze a concrete case study where the microservices architecture environment is replicated and implemented through an equivalent set of smart contracts, showing for the first time the feasibility of implementing a microservices-based system with smart contracts and how the two innovative paradigms match together. Results show that it is possible to implement a simple microservices-based system with smart contracts maintaining the same set of functionalities and results. The result could be highly beneficial in contexts such as smart voting, where not only the data integrity is fundamental but also the source code executed must be trustable.},
  doi       = {10.1109/IWBOSE.2019.8666520},
  keywords  = {contracts;cryptography;distributed databases;innovation management;software architecture;microservices system;blockchain smart contracts;information systems;microservices architecture environment;innovative paradigms;Smart contracts;Blockchain;Computer architecture;Computer languages;Logic gates;Microservice;Cloud Native;Blockchain;Smart contract;Serverless},
}

@Article{TOPPER20185,
  author   = {Jon Topper},
  title    = {Compliance is not security},
  journal  = {Computer Fraud \& Security},
  year     = {2018},
  volume   = {2018},
  number   = {3},
  pages    = {5 - 8},
  issn     = {1361-3723},
  abstract = {Modern IT infrastructure practice is evolving at an incredible pace, with the ongoing proliferation of cloud computing, container orchestration platforms and, most recently, a trend that we're calling ‘serverless’. Is our security practice evolving along with it? IT infrastructure practice is evolving at an incredible pace, with the ongoing proliferation of cloud computing, container orchestration platforms and, most recently, the ‘serverless’ trend. But is our security practice evolving along with it? It's no longer sufficient, if indeed it ever was, to uncritically import a bunch of ‘best practices’ into your organisation once and then call that your security policy. Policies must evolve over time, with people at all levels of the business involved in their implementation, explains Jon Topper of The Scale Factory.},
  doi      = {https://doi.org/10.1016/S1361-3723(18)30022-8},
  url      = {http://www.sciencedirect.com/science/article/pii/S1361372318300228},
}

@inproceedings{Trach:2019:CTS:3319647.3325835,
 author = {Trach, Bohdan and Oleksenko, Oleksii and Gregor, Franz and Bhatotia, Pramod and Fetzer, Christof},
 title = {Clemmys: Towards Secure Remote Execution in FaaS},
 booktitle = {Proceedings of the 12th ACM International Conference on Systems and Storage},
 series = {SYSTOR '19},
 year = {2019},
 isbn = {978-1-4503-6749-3},
 location = {Haifa, Israel},
 pages = {44--54},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3319647.3325835},
 doi = {10.1145/3319647.3325835},
 acmid = {3325835},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@InProceedings{8539109,
  author    = {H. {Truong}},
  title     = {Integrated Analytics for IIoT Predictive Maintenance Using IoT Big Data Cloud Systems},
  booktitle = {2018 IEEE International Conference on Industrial Internet (ICII)},
  year      = {2018},
  pages     = {109-118},
  month     = {Oct},
  abstract  = {For predictive maintenance of equipment with Industrial Internet of Things (IIoT) technologies, existing IoT Cloud systems provide strong monitoring and data analysis capabilities for detecting and predicting status of equipment. However, we need to support complex interactions among different software components and human activities to provide an integrated analytics, as software algorithms alone cannot deal with the complexity and scale of data collection and analysis and the diversity of equipment, due to the difficulties of capturing and modeling uncertainties and domain knowledge in predictive maintenance. In this paper, we describe how we design and augment complex IoT big data cloud systems for integrated analytics of IIoT predictive maintenance. Our approach is to identify various complex interactions for solving system incidents together with relevant critical analytics results about equipment. We incorporate humans into various parts of complex IoT Cloud systems to enable situational data collection, services management, and data analytics. We leverage serverless functions, cloud services, and domain knowledge to support dynamic interactions between human and software for maintaining equipment. We use a real-world maintenance of Base Transceiver Stations to illustrate our engineering approach which we have prototyped with state-of-the art cloud and IoT technologies, such as Apache Nifi, Hadoop, Spark and Google Cloud Functions.},
  doi       = {10.1109/ICII.2018.00020},
  keywords  = {Big Data;cloud computing;data analysis;Internet of Things;production engineering computing;software maintenance;system monitoring;Web services;integrated analytics;IIoT predictive maintenance;software algorithms;system incidents;data analytics;cloud services;Google Cloud Functions;system monitoring;data analysis;software components;IoT Cloud systems;Industrial Internet of Things technologies;IoT big data cloud systems;base transceiver stations;Predictive maintenance;Cloud computing;Task analysis;Software;Data analysis;Sensors;IIOT;predictive maintenance;big data;cloud computing;analytics;services computing},
}

@Article{VAQUERO201920,
  author   = {Luis M. Vaquero and Felix Cuadrado and Yehia Elkhatib and Jorge Bernal-Bernabe and Satish N. Srirama and Mohamed Faten Zhani},
  title    = {Research challenges in nextgen service orchestration},
  journal  = {Future Generation Computer Systems},
  year     = {2019},
  volume   = {90},
  pages    = {20 - 38},
  issn     = {0167-739X},
  abstract = {Fog/edge computing, function as a service, and programmable infrastructures, like software-defined networking or network function virtualisation, are becoming ubiquitously used in modern Information Technology infrastructures. These technologies change the characteristics and capabilities of the underlying computational substrate where services run (e.g. higher volatility, scarcer computational power, or programmability). As a consequence, the nature of the services that can be run on them changes too (smaller codebases, more fragmented state, etc.). These changes bring new requirements for service orchestrators, which need to evolve so as to support new scenarios where a close interaction between service and infrastructure becomes essential to deliver a seamless user experience. Here, we present the challenges brought forward by this new breed of technologies and where current orchestration techniques stand with regards to the new challenges. We also present a set of promising technologies that can help tame this brave new world.},
  doi      = {https://doi.org/10.1016/j.future.2018.07.039},
  keywords = {NVM, SDN, NFV, Orchestration, Large scale, Serverless, FaaS, Churn, Edge, Fog},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X18303157},
}

@Article{VARGHESE2018849,
  author   = {Blesson Varghese and Rajkumar Buyya},
  title    = {Next generation cloud computing: New trends and research directions},
  journal  = {Future Generation Computer Systems},
  year     = {2018},
  volume   = {79},
  pages    = {849 - 861},
  issn     = {0167-739X},
  abstract = {The landscape of cloud computing has significantly changed over the last decade. Not only have more providers and service offerings crowded the space, but also cloud infrastructure that was traditionally limited to single provider data centers is now evolving. In this paper, we firstly discuss the changing cloud infrastructure and consider the use of infrastructure from multiple providers and the benefit of decentralising computing away from data centers. These trends have resulted in the need for a variety of new computing architectures that will be offered by future cloud infrastructure. These architectures are anticipated to impact areas, such as connecting people and devices, data-intensive computing, the service space and self-learning systems. Finally, we lay out a roadmap of challenges that will need to be addressed for realising the potential of next generation cloud systems.},
  doi      = {https://doi.org/10.1016/j.future.2017.09.020},
  keywords = {Cloud computing, Fog computing, Cloudlet, Multi-cloud, Serverless computing, Cloud security},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X17302224},
}

@Article{8625892,
  author   = {J. L. {Vázquez-Poletti} and I. M. {Llorente} and K. {Hinsen} and M. {Turk}},
  title    = {Serverless Computing: From Planet Mars to the Cloud},
  journal  = {Computing in Science Engineering},
  year     = {2018},
  volume   = {20},
  number   = {6},
  pages    = {73-79},
  month    = {Nov},
  issn     = {1521-9615},
  abstract = {Serverless computing is a new way of managing computations in the cloud. We show how it can be put to work for scientific data analysis. For this, we detail our serverless architecture for an application analyzing data from one of the instruments onboard the ESA Mars Express orbiter, and then, we compare it with a traditional server solution.},
  doi      = {10.1109/MCSE.2018.2875315},
  keywords = {astronomy computing;data analysis;Mars;space vehicles;planet Mars;serverless computing;scientific data analysis;serverless architecture;ESA Mars Express orbiter;cloud;Cloud computing;Containers;Mars;Servers;Computational modeling;Artificial intelligence;Programming},
}

@Article{Villamizar2017,
author="Villamizar, Mario
and Garc{\'e}s, Oscar
and Ochoa, Lina
and Castro, Harold
and Salamanca, Lorena
and Verano, Mauricio
and Casallas, Rubby
and Gil, Santiago
and Valencia, Carlos
and Zambrano, Angee
and Lang, Mery",
title="Cost comparison of running web applications in the cloud using monolithic, microservice, and AWS Lambda architectures",
journal="Service Oriented Computing and Applications",
year="2017",
month="Jun",
day="01",
volume="11",
number="2",
pages="233--247",
abstract="Large Internet companies like Amazon, Netflix, and LinkedIn are using the microservice architecture pattern to deploy large applications in the cloud as a set of small services that can be independently developed, tested, deployed, scaled, operated, and upgraded. However, aside from gaining agility, independent development, and scalability, how microservices affect the infrastructure costs is a major evaluation topic for companies adopting this pattern. This paper presents a cost comparison of a web application developed and deployed using the same scalable scenarios with three different approaches: 1) a monolithic architecture, 2) a microservice architecture operated by the cloud customer, and 3) a microservice architecture operated by the cloud provider. Test results show that microservices can help reduce infrastructure costs in comparison with standard monolithic architectures. Moreover, the use of services specifically designed to deploy and scale microservices, such as AWS Lambda, reduces infrastructure costs by 70{\%} or more, and unlike microservices operated by cloud customers, these specialized services help to guarantee the same performance and response times as the number of users increases. Lastly, we also describe the challenges we faced while implementing and deploying microservice applications, and include a discussion on how to replicate the results on other cloud providers.",
issn="1863-2394",
doi="10.1007/s11761-017-0208-y",
url="https://doi.org/10.1007/s11761-017-0208-y"
}
@InProceedings{10.1007/978-3-030-04284-4_10,
author="Vogelgesang, Christian
and Spieldenner, Torsten
and Schubotz, Ren{\'e}",
editor="Ichise, Ryutaro
and Lecue, Freddy
and Kawamura, Takahiro
and Zhao, Dongyan
and Muggleton, Stephen
and Kozaki, Kouji",
title="SPARQ{\$}{\$}{\backslash}lambda {\$}{\$}: A Functional Perspective on Linked Data Services",
booktitle="Semantic Technology",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="136--152",
abstract="With more and more applications providing semantic data to improve interoperability, the amount of available RDF datasets is constantly increasing. The SPARQL query language is a W3C recommendation to provide query capabilities on such RDF datasets. Data integration from different RDF sources is up to now mostly task of RDF consuming clients. However, from a functional perspective, data integration boils down to a function application that consumes input data as parameters, and based on these, produces a new set of data as output. Following this notion, we introduce SPARQ{\$}{\$}{\backslash}lambda {\$}{\$}, an extension to the SPARQL 1.1 query language. SPARQ{\$}{\$}{\backslash}lambda {\$}{\$}enables dynamic injection of RDF datasets during evaluation of the query, and by this lifts SPARQL to a tool to write templates for RDF producing functions, an important step to reduce the effort to write SPARQL queries that work on data from various sources. SPARQ{\$}{\$}{\backslash}lambda {\$}{\$}is moreover suitable to directly translate to an RDF described Web service interface, which allows to lift integration of data and re-provisioning of integrated results from clients to cloud environments, and by this solving the bottleneck of RDF data integration on client side.",
isbn="978-3-030-04284-4"
}

@InProceedings{7573771,
  author    = {B. {Wagner} and A. {Sood}},
  title     = {Economics of Resilient Cloud Services},
  booktitle = {2016 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
  year      = {2016},
  pages     = {368-374},
  month     = {Aug},
  abstract  = {Today's computer systems must meet and maintain service availability, performance, and security requirements. Each of these demands requires redundancy and some form of isolation. When service requirements are implemented separately, the system architecture cannot easily share common components of redundancy and isolation. We will present these service traits collectively as cyber resilience with a system called Self-Cleansing Intrusion Tolerance (SCIT). Further, we will demonstrate that SCIT provides an effective resilient cloud implementation making cost effective utilization of cloud's excess capacity and economies of scale. Lastly, we will introduce the notion of serverless applications utilizing AWS Lambda and how a stateless architecture can drastically reduce operational costs by utilizing cloud function services.},
  doi       = {10.1109/QRS-C.2016.56},
  keywords  = {cloud computing;economics;security of data;economics;resilient cloud services;security requirements;system architecture;self-cleansing intrusion tolerance;SCIT;resilient cloud implementation;cloud excess capacity;AWS Lambda;stateless architecture;cloud function services;computer systems;service availability;service performance;Servers;Cloud computing;Redundancy;Security;Programming;Resilience;Cyber Resilience;Security Economics;SCIT;AWS Spot;GCP Preemptive},
}

@InProceedings{Wang:2019:REO:3302424.3303978,
  author    = {Wang, Kai-Ting Amy and Ho, Rayson and Wu, Peng},
  title     = {Replayable Execution Optimized for Page Sharing for a Managed Runtime Environment},
  booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
  year      = {2019},
  series    = {EuroSys '19},
  pages     = {39:1--39:16},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3303978},
  articleno = {39},
  doi       = {10.1145/3302424.3303978},
  isbn      = {978-1-4503-6281-8},
  keywords  = {Cloud Computing, Operating Systems, Programming Languages and Runtimes},
  location  = {Dresden, Germany},
  numpages  = {16},
  url       = {http://doi.acm.org/10.1145/3302424.3303978},
}

@InProceedings{Wang:2018:PBC:3277355.3277369,
  author    = {Wang, Liang and Li, Mengyuan and Zhang, Yinqian and Ristenpart, Thomas and Swift, Michael},
  title     = {Peeking Behind the Curtains of Serverless Platforms},
  booktitle = {Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference},
  year      = {2018},
  series    = {USENIX ATC '18},
  pages     = {133--145},
  address   = {Berkeley, CA, USA},
  publisher = {USENIX Association},
  acmid     = {3277369},
  isbn      = {978-1-931971-44-7},
  location  = {Boston, MA, USA},
  numpages  = {13},
  url       = {http://dl.acm.org/citation.cfm?id=3277355.3277369},
}

@InProceedings{8121899,
  author    = {T. {Wei} and Y. {Coady} and J. {MacDonald} and K. {Booth} and J. {Salter} and C. {Girling}},
  title     = {Dumb pipes for smart systems: How tomorrow's applications can salvage yesterday's plumbing},
  booktitle = {2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)},
  year      = {2017},
  pages     = {1-5},
  month     = {Aug},
  abstract  = {Tomorrow's immersive applications will leverage Mixed Reality interfaces accessing a multitude of services from distributed clouds. They will face extreme latency constraints, massive datasets, spontaneous collaboration, and constant service churn. This paper outlines our experience evolving an application designed to support collaborative work in Urban Design (UD) practices. The application, UD Co-Spaces, recently weathered significant churn as a core service was discontinued and replaced by a service with a subtly different API. A “dumb pipes” approach, where services communicate through a simple message queue, facilitated this evolution with relatively little disruption to the rest of the system. We show how this strategy can be used to reintroduce new features to the system, and is sustainable as the system's interfaces evolve to use Virtual, Augmented and Mixed Reality environments.},
  doi       = {10.1109/PACRIM.2017.8121899},
  keywords  = {application program interfaces;augmented reality;cloud computing;groupware;human computer interaction;dumb pipes;smart systems;distributed clouds;extreme latency constraints;massive datasets;spontaneous collaboration;constant service churn;collaborative work;Urban Design practices;UD Co-Spaces;simple message queue;mixed reality environments;augmented environments;virtual reality environments;API;Three-dimensional displays;Google;Earth;Servers;Solid modeling;Computer architecture;Buildings;serverless systems;software evolution;virtual reality;computer supported collaborative work},
}

@InProceedings{8622362,
  author    = {S. {Werner} and J. {Kuhlenkamp} and M. {Klems} and J. {Müller} and S. {Tai}},
  title     = {Serverless Big Data Processing using Matrix Multiplication as Example},
  booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
  year      = {2018},
  pages     = {358-365},
  month     = {Dec},
  abstract  = {Serverless computing, or Function-as-a-Service (FaaS), is emerging as a popular alternative model to on-demand cloud computing. Function services are executed by a FaaS provider; a client no longer uses cloud infrastructure directly as in traditional cloud consumption. Is serverless computing a feasible and beneficial approach to big data processing, regarding performance, scalability, and cost effectiveness? In this paper, we explore this research question using matrix multiplication as example. We define requirements for the design of serverless big data applications, present a prototype for matrix multiplication using FaaS, and discuss and synthesize insights from results of extensive experimentation. We show that serverless big data processing can lower operational and infrastructure costs without compromising system qualities; serverless computing can even outperform cluster-based distributed compute frameworks regarding performance and scalability.},
  doi       = {10.1109/BigData.2018.8622362},
  keywords  = {Big Data;cloud computing;matrix multiplication;pattern clustering;service-oriented architecture;serverless big data processing;matrix multiplication;serverless computing;popular alternative model;on-demand cloud computing;FaaS provider;cloud infrastructure;traditional cloud consumption;serverless big data applications;compute frameworks;function-as-a-service;Cloud computing;Scalability;FAA;Prototypes;Big Data applications;Task analysis;serverless;big data;cloud;matrix multiplication},
}

@InProceedings{8514413,
  author    = {M. {Westerlund} and N. {Kratzke}},
  title     = {Towards Distributed Clouds: A Review About the Evolution of Centralized Cloud Computing, Distributed Ledger Technologies, and A Foresight on Unifying Opportunities and Security Implications},
  booktitle = {2018 International Conference on High Performance Computing Simulation (HPCS)},
  year      = {2018},
  pages     = {655-663},
  month     = {July},
  abstract  = {This review focuses on the evolution of cloud computing and distributed ledger technologies (blockchains) over the last decade. Cloud computing relies mainly on a conceptually centralized service provisioning model, while blockchain technologies originate from a peer-to-peer and a completely distributed approach. Still, noteworthy commonalities between both approaches are often overlooked by researchers. Therefore, to the best of the authors knowledge, this paper reviews both domains in parallel for the first time. We conclude that both approaches have advantages and disadvantages. The advantages of centralized service provisioning approaches are often the disadvantages of distributed ledger approaches and vice versa. It is obviously an interesting question whether both approaches could be combined in a way that the advantages can be added while the disadvantages could be avoided. We derive a software stack that could build the foundation unifying the best of these two worlds and that would avoid existing shortcomings like vendor lock-in, some security problems, and inherent platform dependencies.},
  doi       = {10.1109/HPCS.2018.00108},
  keywords  = {cloud computing;security of data;centralized cloud computing;distributed ledger technologies;security implications;conceptually centralized service provisioning model;blockchain technologies;completely distributed approach;centralized service provisioning approaches;distributed ledger approaches;peer-to-peer approach;Cloud computing;Computer architecture;FAA;Servers;Containers},
}

@InProceedings{8599581,
  author    = {M. {Wurster} and U. {Breitenbücher} and K. {Képes} and F. {Leymann} and V. {Yussupov}},
  title     = {Modeling and Automated Deployment of Serverless Applications Using TOSCA},
  booktitle = {2018 IEEE 11th Conference on Service-Oriented Computing and Applications (SOCA)},
  year      = {2018},
  pages     = {73-80},
  month     = {Nov},
  abstract  = {The serverless computing paradigm brings multiple benefits to application developers who are interested in consuming computing resources as services without the need to manage physical capacities or limits. There are several deployment technologies and languages available suitable for deploying applications to a single cloud provider. However, for multi-cloud application deployments, multiple technologies have to be used and orchestrated. In addition, the event-driven nature of serverless computing imposes further requirements on modeling such application structures in order to automate their deployment. In this paper, we tackle these issues by introducing an event-driven deployment modeling approach using the standard Topology and Orchestration Specification for Cloud Applications (TOSCA) that fully employs the suggested standard lifecycle to provision and manage multi-cloud serverless applications. To show the feasibility of our approach, we extended the existing TOSCA-based ecosystem OpenTOSCA.},
  doi       = {10.1109/SOCA.2018.00017},
  issn      = {2163-2871},
  keywords  = {cloud computing;computing resources;physical capacities;deployment technologies;single cloud provider;multicloud application deployments;multiple technologies;event-driven nature;application structures;event-driven deployment modeling approach;multicloud serverless applications;serverless computing paradigm;application developers;cloud applications;TOSCA-based ecosystem;OpenTOSCA;Cloud computing;Computational modeling;Topology;Standards;Computer architecture;FAA;Serverless;Multi-Cloud;Modeling;Automated Deployment;TOSCA},
}

@InProceedings{Yan:2016:BCS:3007203.3007217,
  author    = {Yan, Mengting and Castro, Paul and Cheng, Perry and Ishakian, Vatche},
  title     = {Building a Chatbot with Serverless Computing},
  booktitle = {Proceedings of the 1st International Workshop on Mashups of Things and APIs},
  year      = {2016},
  series    = {MOTA '16},
  pages     = {5:1--5:4},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3007217},
  articleno = {5},
  doi       = {10.1145/3007203.3007217},
  isbn      = {978-1-4503-4669-6},
  keywords  = {FaaS, Serverless, bots, cloud computing},
  location  = {Trento, Italy},
  numpages  = {4},
  url       = {http://doi.acm.org/10.1145/3007203.3007217},
}

@Article{8327546,
  author   = {M. {Yousif}},
  title    = {The State of the Cloud},
  journal  = {IEEE Cloud Computing},
  year     = {2018},
  volume   = {5},
  number   = {1},
  pages    = {4-5},
  month    = {Jan},
  issn     = {2325-6095},
  abstract = {In the State of the Cloud article in January 2017, IEEE Cloud Computing Editor in Chief Mazin Yousif said that the cloud would become the de-facto hosting platform for all applications and social innovations. He also mentioned that it is becoming the new normal. And that does seem to be the case. Cloud computing is enabling companies to innovate at their own speed, allowing them to spin up and down images, launch applications and analytics as fast as they need. The cloud also lets consumers make choices from a wide variety of services.},
  doi      = {10.1109/MCC.2018.011791706},
  keywords = {cloud computing;consumers;digitalization;serverless computing},
}

@InProceedings{Zheng:2018:DCD:3234200.3234232,
  author    = {Zheng, Shengbao and Yang, Xiaowei},
  title     = {DynaShield: A Cost-Effective DDoS Defense Architecture},
  booktitle = {Proceedings of the ACM SIGCOMM 2018 Conference on Posters and Demos},
  year      = {2018},
  series    = {SIGCOMM '18},
  pages     = {15--17},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3234232},
  doi       = {10.1145/3234200.3234232},
  isbn      = {978-1-4503-5915-3},
  keywords  = {Cryptocurrency Mining, DDoS, On-demand, Serverless},
  location  = {Budapest, Hungary},
  numpages  = {3},
  url       = {http://doi.acm.org/10.1145/3234200.3234232},
}

@InProceedings{10.1007/978-3-319-94295-7_18,
author="Zhou, Huan
and Hu, Yang
and Su, Jinshu
and de Laat, Cees
and Zhao, Zhiming",
editor="Luo, Min
and Zhang, Liang-Jie",
title="CloudsStorm: An Application-Driven Framework to Enhance the Programmability and Controllability of Cloud Virtual Infrastructures",
booktitle="Cloud Computing -- CLOUD 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="265--280",
abstract="Most current IaaS (Infrastructure-as-a-Service) clouds provide dedicated virtual infrastructure resources to cloud applications with only limited programmability and controllability, which enlarges the management gap between infrastructures and applications. Traditional DevOps (development and operations) approaches are not suitable in today's cloud environments, because of the slow, manual and error-prone collaboration between developers and operations personnel. It is essential to involve the operation into the cloud application development phase, which needs to make the infrastructure able to be controlled by the application directly. Moreover, each of these cloud providers offers their own set of APIs to access the resources. It causes the vendor lock-in problem for the application when managing its infrastructure across federated clouds or multiple data centers. To mitigate this gap, we have designed CloudsStorm, an application-driven DevOps framework that allows the application directly program and control its infrastructure. In particular, it provides multi-level programmability and controllability according to the applications' specifications. We evaluate it by comparing its functionality to other proposed solutions. Moreover, we implement an extensible TSV-Engine, which is the core component of CloudsStorm for managing infrastructures. It is the first to be able to provision a networked infrastructure among public clouds. At last, we conduct a set of experiments on actual clouds and compare with other related DevOps tools. The experimental results demonstrate our solution is efficient and outperforms others.",
isbn="978-3-319-94295-7"
}

@InProceedings{10.1007/978-3-319-77028-4_45,
author="Zieba, Daniela
and Jenkins, Wren
and Galloway, Michael
and Houle, Jean-Luc",
editor="Latifi, Shahram",
title="Taking a Steppe Towards Optimizing Note-Taking Software by Creation of a Note Classification Algorithm",
booktitle="Information Technology - New Generations",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="333--341",
abstract="Note-taking software often far surpasses its paper-and-pencil counterpart when measured in metrics such as availability and reliability. However, there is ample opportunity relating to the analysis and organization of notes in structures often called folders, notebooks, or projects within various software. ShovelWare is a project designed for an ongoing field research project analyzing the Bronze and Iron Ages of Mongolia. Accessible through a web interface and cross-platform mobile application, it is a replacement for manual data collection on paper and excessive, error-ridden input into digital spreadsheets. We propose a machine learning algorithm that classifies notes using a variety of metrics, sorting them into graph structures to provide initial insights into the similarity of field notes. As a result, ShovelWare will allow archaeologists to more quickly and cleanly view and share their data. The algorithm, as well as the note-taking structure, are planned with hopes of scalability and applicability into more disciplines.",
isbn="978-3-319-77028-4"
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveOrderConfig:specified;author;false;abstract;false;abstract;false;}
